import sys
import argparse
import threading
from pathlib import Path
from PIL import Image
from prettytable import PrettyTable
import time
try:
    from tqdm.rich import tqdm
except:
    from tqdm import tqdm


ROOT = Path.cwd().resolve()
FILE = Path(__file__).resolve()  # å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
if str(ROOT) not in sys.path:  # è§£å†³VSCodeæ²¡æœ‰ROOTçš„é—®é¢˜
    sys.path.append(str(ROOT))
ROOT = ROOT.relative_to(Path.cwd())

from utils.general import (
    IMAGE_TYPE, RECORDER, TranslationDict,
    get_logger, colorstr, listdir, second_confirm, verify_image, exif_size, read_json, fix_illegal_coordinates, 
    fix_reverse_coordinates, XMLWriter, split_list_equally, calc_cost_time, check_dataset, dict2table, statistics)
        

def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument("--image-path", type=str, default="Datasets/coco128/train/images", help="å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--label-path", type=str, default="Datasets/coco128/train/xmls", help="xmlæ ‡ç­¾è·¯å¾„")
    parser.add_argument("--target-path", type=str, default="Datasets/coco128/train/jsons", help="ç›®æ ‡æ ‡ç­¾ä¿å­˜è·¯å¾„")
    parser.add_argument("--override", action='store_true', default=False, help="å¦‚æœå¯¹åº”çš„targetæ–‡ä»¶å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–å®ƒ")
    parser.add_argument("--num-threading", type=int, default=4, help="ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼Œä¸ä½¿ç”¨å¤šçº¿ç¨‹åˆ™è®¾ç½®ä¸º1")
    parser.add_argument("--ndigit", type=int, default=None, help="åæ ‡ä¿ç•™çš„å°æ•°ä½ï¼Œé»˜è®¤ä¸ºNone")
    
    return parser.parse_known_args()[0] if known else parser.parse_args()


def process(args: argparse, images: list) -> None:
    for image in images:  # image: PosixPath
        image = Path(image)  # ä¸ºäº†æ–¹ä¾¿IDEç»™å‡ºä»£ç æç¤º

        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºä¿¡æ¯
        pbar.set_description(f"Processing {colorstr(image.name):<30s}")

        RECORDER["touch"] += 1
        
        # è¯»å–å›¾ç‰‡å°ºå¯¸
        im = Image.open(image)

        # éªŒè¯å›¾ç‰‡æ˜¯å¦ç ´æŸ
        if not verify_image(image):  # éªŒè¯å›¾ç‰‡æ˜¯å¦ç ´æŸ
            pbar.clear()
            LOGGER.error(f"âŒ [Corrupt image] Found corrupt image! -> {str(image)}")
            RECORDER["corrupt"] += 1
            pbar.update()
            continue
        
        # è·å–å›¾ç‰‡å°ºå¯¸
        img_width, img_height = exif_size(im)

        # è·å–å›¾ç‰‡é€šé“æ•°
        img_channel = 3 if im.mode == "RGB" else 1 if im.mode == "L" else 4 if im.mode == "RGBA" else "Unknown"
        
        # ç¡®å®šlabelä½ç½®
        label = label_dir.joinpath(image.stem + '.json')
        
        # åˆ¤æ–­labelæ˜¯å¦å­˜åœ¨ï¼šä¸å­˜åœ¨ -> è´Ÿæ ·æœ¬
        if not label.exists():
            pbar.clear()
            LOGGER.info(f"âš ï¸ [Negative sample] {str(image)}")
            RECORDER["missing"] += 1
            pbar.update()
            continue
            
        # è¯»å–labelä¿¡æ¯å¹¶è·å–å­—å…¸
        objects = read_json(label)['shapes']

        # å¦‚æœæ²¡æœ‰object -> å®šä¹‰ä¸ºè´Ÿæ ·æœ¬
        if not objects:
            RECORDER["background"] += 1
            pbar.update()
            continue
        
        # å¦‚æœtargetæ–‡ä»¶å­˜åœ¨
        target = target_dir.joinpath(image.stem + '.xml')
        if target.exists() and target.read_text():  # å¦‚æœæ–‡ä»¶å­˜åœ¨ä¸”æ–‡ä»¶å†…å®¹ä¸ä¸ºç©º
            if args.override:  # è¦†ç›–æ‰ä¹‹å‰çš„å†…å®¹
                pbar.clear()
                LOGGER.warning(f"âš ï¸ [Override] The target file has existed, but its content will be overrode! -> {str(target)}")
            else:
                pbar.clear()
                LOGGER.info(f"[Skip] The target file has existed, and it will not be overrode. -> {str(target)}")
                RECORDER['skip'] += 1
                pbar.update()
                continue
        
        # åˆ›å»ºå†™å…¥å™¨
        xml_writer = XMLWriter(image, img_width, img_height, img_channel)

        # å¤„ç†objectsæ–‡ä»¶
        for index, object_info in enumerate(objects):
            # [[1, 75], [64, 244]] -> [1, 75, 64, 244]
            object_info['points'] = [coordinate for pair in object_info['points'] for coordinate in pair]

            # æ£€æŸ¥ï¼šåæ ‡ç‚¹çš„ä¸ªæ•°æ˜¯å¦ä¸º4
            num_pts = len(object_info["points"])
            if num_pts != 4:
                pbar.clear()
                LOGGER.error(f"âŒ [Illegal points] The No.{index} object has illegal points({num_pts} != 4)! -> {str(label)}")
                RECORDER["illegal_pts"] += 1
                pbar.update()
                continue
            
            # è·å–æ¯ä¸ªobjectçš„boxä¿¡æ¯
            x1 = round(float(object_info["points"][0]), args.ndigit)
            y1 = round(float(object_info["points"][1]), args.ndigit)
            x2 = round(float(object_info["points"][2]), args.ndigit)
            y2 = round(float(object_info["points"][3]), args.ndigit)
            
            # æ£€æŸ¥ï¼šä¿®å¤ä¸åˆè§„çš„åæ ‡ï¼šè´Ÿæ•°å’Œè¶Šç•Œ
            x1, y1, x2, y2, msg = fix_illegal_coordinates(
                x1, y1, x2, y2, img_width, img_height
            )
            if msg:
                msg = [f"[{i}] {content}" for i, content in enumerate(msg)]
                msg = ", ".join(msg)
                pbar.clear()
                LOGGER.warning(f"âš ï¸ [Out of boundary] The No.{index} object has illegal coordinates: {msg}! -> {str(label)}")
                RECORDER["out_of_boundary"] += 1
            
            # æ£€æŸ¥ï¼šä¿®å¤ç›¸åçš„åæ ‡ï¼šx2y2x1y1 -> x1y1x2y2
            x1, y1, x2, y2, msg = fix_reverse_coordinates(x1, y1, x2, y2)
            if msg:
                msg = [f"[{i}] {content}" for i, content in enumerate(msg)]
                msg = ", ".join(msg)
                pbar.clear()
                LOGGER.warning(f"âš ï¸ [Reversed coordinates] The No.{index} object of has illegal coordinates: {msg}! -> {str(label)}")
                RECORDER["reversed"] += 1
            
            # è·å–å¯¹åº”çš„ç±»åˆ«
            class_name = object_info["label"]

            # å°†è·å–åˆ°çš„ç±»åˆ«æ·»åŠ åˆ°é›†åˆä¸­
            classes_dict.add(class_name)

            # æ·»åŠ objectï¼Œä¿æŒ6ä½å°æ•°
            xml_writer.add_object(class_name, x1, y1, x2, y2)
            RECORDER['objects'] += 1  # è®°å½•å¯¹è±¡+1

        # è®°å½•å®Œæ‰€æœ‰çš„objectsï¼Œä¿å­˜æ–‡ä»¶
        xml_writer.save(target)

        RECORDER["found"] += 1
        pbar.update()
    

if __name__ == "__main__":
    t1 = time.time()
    LOGGER = get_logger(FILE)  # global
    
    # è§£æå‚æ•°
    args = parse_opt(known=False)  # å¦‚æœå‘ç°ä¸è®¤è¯†çš„å‚æ•°åˆ™æŠ¥é”™

    # è®°å½•
    RECORDER['image path'] = args.image_path
    RECORDER['label path'] = args.label_path
    RECORDER['target path'] = args.target_path
    
    # è¯»å–æ‰€æœ‰çš„å›¾ç‰‡å’Œæ ‡ç­¾
    total_images = listdir(args.image_path, extension=IMAGE_TYPE)
    total_labels = listdir(args.label_path, extension='.json')
    RECORDER['images'] = len(total_images)
    RECORDER['labels'] = len(total_labels)
    RECORDER['ndigit'] = args.ndigit
    
    # åˆ›å»ºç±»åˆ«å­—å…¸
    classes_dict = set()  # ç©ºé›†åˆ

    # æ ¹æ®çº¿ç¨‹æ•°ï¼Œå¾—åˆ°æ¯ä¸ªçº¿ç¨‹éœ€è¦å¤„ç†çš„å›¾ç‰‡list
    total_image_lists = split_list_equally(total_images, args.num_threading)

    # è®°å½•çº¿ç¨‹ç›¸å…³
    RECORDER['threadings'] = args.num_threading
    RECORDER['data num of every threading'] = len(total_image_lists[0])
    RECORDER['script'] = str(FILE.name)
    
    # è¾“å‡ºå¼€å§‹æ‰§è¡Œè„šæœ¬å‰çš„ç»Ÿè®¡ä¿¡æ¯
    LOGGER.info(dict2table(RECORDER, align='l', replace_keys=TranslationDict))
    
    # æ ¹æ®å›¾ç‰‡å’Œæ ‡ç­¾æ•°é‡å‘å‡ºå¯¹åº”çš„å‘Šè­¦
    check_dataset(num_images=RECORDER['images'], num_labels=RECORDER['labels'])
    
    # 2FA
    second_confirm(script=FILE, LOGGER=LOGGER)
    
    # åˆ›å»ºPathå¯¹è±¡
    label_dir = Path(args.label_path)
    target_dir = Path(args.target_path)
    
    # åˆ›å»ºæ ‡ç­¾æ–‡ä»¶å¤¹
    target_dir.mkdir(exist_ok=True)
    
    threads = []  # ä¿å­˜çº¿ç¨‹çš„list
    pbar = tqdm(total=RECORDER['images'], dynamic_ncols=True)  # for every image file
    for images in total_image_lists:
        t = threading.Thread(
            target=process, 
            args=(
                args, 
                images,
            )
        )
        threads.append(t)
        t.start()

    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹éƒ½æ‰§è¡Œå®Œæ¯•
    for t in threads:
        t.join()

    # æ‰€æœ‰è¿›ç¨‹ç»“æŸåå†å…³é—­è¿›åº¦æ¡
    pbar.close()

    # ç»Ÿè®¡è·å–åˆ°çš„ç±»åˆ«æƒ…å†µ
    RECORDER['nc'] = len(classes_dict)
    RECORDER['classes_dict'] = {class_index: class_name for class_index, class_name in enumerate(classes_dict)}

    # ç»Ÿè®¡æ­£æ ·æœ¬æƒ…å†µ
    RECORDER = statistics(RECORDER)
    
    # å†æ¬¡è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    LOGGER.info(dict2table(RECORDER, align='l', replace_keys=TranslationDict))
    
    if RECORDER["found"] + RECORDER["background"] + RECORDER['skip'] + RECORDER['missing'] == RECORDER["images"]:
        LOGGER.info(colorstr('green', 'bold', 'âœ… All conversion has done correctly!'))
        if RECORDER['missing'] != 0:
            LOGGER.warning(colorstr('yellow', 'bold', f"âš ï¸ There are {RECORDER['missing']} images without label, "
                                    f"and they have be regarded as negative samples!"))
    else:
        LOGGER.warning(colorstr('red', 'bold', "âš ï¸ Some question have occurred, please check dataset!"))

    if RECORDER['skip'] == RECORDER['images']:
        LOGGER.warning(f"âš ï¸ All target file have been skipped, please check dataset!")

    LOGGER.info(f"â³ The cost time of {str(FILE.name)} is {colorstr(calc_cost_time(t1, time.time()))}")
    LOGGER.info(
        f"â— Please check the {colorstr('blue', 'bold', 'num classes')} and {colorstr('blue', 'bold', 'class name')} in the table, "
        f"which are obtained from the {colorstr('yellow', 'bold', 'JSON')}s ({colorstr('green', 'the order of classes is not important')})."
    )
    LOGGER.info(
        f"ğŸ‘€ The detailed information has been saved to {colorstr(LOGGER.handlers[0].baseFilename)}. \n"
        f"    This script is formatted with {colorstr('ANSI')} color codes, so it is recommended to {colorstr('use a terminal or a compatible tool')} "
        f"that supports color display for viewing."
    )
