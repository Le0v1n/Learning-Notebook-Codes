# 0. å‰è¨€

## 0.1 YOLO-Worldä»‹ç»

YOLO-World æ˜¯ä¸€ç§åŸºäº YOLOç³»åˆ—ç›®æ ‡æ£€æµ‹å™¨çš„åˆ›æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è§†è§‰è¯­è¨€å»ºæ¨¡å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„é¢„è®­ç»ƒï¼Œå¢å¼ºäº† YOLO çš„å¼€æ”¾è¯æ±‡æ£€æµ‹èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒYOLO-World å¼•å…¥äº†ä¸€ç§æ–°çš„å¯é‡å‚æ•°åŒ–çš„è§†è§‰-è¯­è¨€è·¯å¾„èšåˆç½‘ç»œï¼ˆRepVL-PANï¼‰å’ŒåŒºåŸŸ-æ–‡æœ¬å¯¹æ¯”æŸå¤±ï¼Œä»¥ä¿ƒè¿›è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ä¹‹é—´çš„äº¤äº’ã€‚è¿™ç§æ–¹æ³•åœ¨é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰æƒ…å†µä¸‹é«˜æ•ˆåœ°æ£€æµ‹å„ç§ç‰©ä½“ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ LVIS æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡å’Œé«˜é€Ÿåº¦çš„æ£€æµ‹ã€‚

YOLO-World çš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
- å®æ—¶è§£å†³æ–¹æ¡ˆï¼šåˆ©ç”¨ CNN çš„è®¡ç®—é€Ÿåº¦ï¼Œæä¾›å¿«é€Ÿçš„å¼€æ”¾è¯æ±‡æ£€æµ‹è§£å†³æ–¹æ¡ˆã€‚
- æ•ˆç‡å’Œæ€§èƒ½ï¼šåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹é™ä½è®¡ç®—å’Œèµ„æºéœ€æ±‚ï¼Œæ”¯æŒå®æ—¶åº”ç”¨ã€‚
- åˆ©ç”¨ç¦»çº¿è¯æ±‡è¿›è¡Œæ¨ç†ï¼šå¼•å…¥äº† "å…ˆæç¤ºåæ£€æµ‹" çš„ç­–ç•¥ï¼Œä½¿ç”¨é¢„å…ˆè®¡ç®—çš„è‡ªå®šä¹‰æç¤ºæ¥æé«˜æ•ˆç‡ã€‚
- å“è¶Šçš„åŸºå‡†æµ‹è¯•ï¼šåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒYOLO-World çš„é€Ÿåº¦å’Œæ•ˆç‡è¶…è¿‡äº†ç°æœ‰çš„å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨ã€‚
- åº”ç”¨å¹¿æ³›ï¼šYOLO-World çš„åˆ›æ–°æ–¹æ³•ä¸ºä¼—å¤šè§†è§‰ä»»åŠ¡å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚

## 0.2 å‰ç½®çŸ¥è¯†

å‰ç½®çŸ¥è¯†åŒ…æ‹¬ï¼š

| åç§°         | æ–‡å†…é“¾æ¥         |
| :----------- | :--------------- |
| é›¶æ ·æœ¬       | [æ–‡å†…é“¾æ¥](#6.1) |
| CLIP         | [æ–‡å†…é“¾æ¥](#6.2) |
| å¼€é›†ç›®æ ‡æ£€æµ‹ | [æ–‡å†…é“¾æ¥](#6.3) |
| LVISæ•°æ®é›†   | [æ–‡å†…é“¾æ¥](#6.4) |

è¯·ç‚¹å‡»å¯¹åº”é“¾æ¥è·³è½¬åˆ°æœ¬æ–‡çš„å¯¹åº”ä½ç½®ã€‚

# 1. å®‰è£…

ç›®å‰YOLO-Worldæœ‰ä¸¤ä¸ªä»“åº“ï¼š

1. [å®˜æ–¹åŸºäºMMOpenLabå®ç°](https://github.com/AILab-CVC/YOLO-World?tab=readme-ov-file)
2. [Ultralyticså®ç°](https://github.com/ultralytics/ultralytics)

å®˜æ–¹å®ç°ç›¸æ¯”Ultralyticså®ç°æœ‰æ›´å¤šçš„ç»†èŠ‚ï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å®˜æ–¹åŸºäºMMOpenLabå®ç°ï¼Œå…·ä½“å®‰è£…è¯·è§[installation.md](https://github.com/AILab-CVC/YOLO-World/blob/master/docs/installation.md)ã€‚ä¸‹é¢æ˜¯æˆ‘è‡ªå·±çš„å®‰è£…è¿‡ç¨‹ï¼š

```bash
# å®‰è£…è™šæ‹Ÿç¯å¢ƒ
conda create -n yolo-world python=3.9
conda activate yolo-world

# æ ¹æ®cudaç‰ˆæœ¬å®‰è£…PyTorchï¼ˆå¦‚æœå®‰è£…æ…¢ï¼Œåˆ™å¯ä»¥åœ¨åé¢æ·»åŠ  -i https://pypi.tuna.tsinghua.edu.cn/simpleï¼‰
pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install requests==2.28.2 tqdm==4.65.0 rich==13.4.2 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -U openmim
mim install mmcv=2.1.0
mim install mmdet=3.3.0
mim install mmcv=2.1.0
mim install mmcv-lite=2.0.1
mim install mmengine=0.10.4
mim install mmyolo=0.6.0
```

<font color='red'><b>å¦‚æœMMYOLOå®‰è£…å¤±è´¥</b></font>ï¼ˆå®‰è£…æˆåŠŸäº†åˆ™ä¸éœ€è¦äº†ï¼‰ï¼Œé‚£ä¹ˆä»MMYOLOå®˜æ–¹ä»“åº“ä¸‹è½½é¡¹ç›®å‹ç¼©åŒ…ï¼Œä¹‹åå†ï¼š

```bash
# è§£å‹
7z x mmyolo-main.zip -o.

# æŠŠæ–‡ä»¶å¤¹åå­—ä»mmyolo-mainä¿®æ”¹ä¸ºmmyolo
mv mmyolo-main mmyolo

# å®‰è£…mmyolo
pip install -e mmyolo
```

å®‰è£…å®Œæˆåå†å®‰è£…å…¶ä»–ä¾èµ–ï¼ˆå…¶ä»–MMOpenlabçš„åº“å¦‚æœå®‰è£…å¤±è´¥ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥ä½¿ç”¨è¿™æ ·çš„æ–¹å¼æ¥è¿›è¡Œï¼‰ï¼š

```bash
# å®‰è£…å…¶ä»–ä¾èµ–åŒ…
pip install opencv-python --upgrade
pip install opencv-python-headless --upgrade
pip install timm==0.6.13 transformers==4.36.2 albumentations==1.4.4
pip install gradio==4.16.0 supervision
pip install onnx onnxruntime onnxsim
```

åœ¨å®‰è£…YOLO-Worldé¡¹ç›®ä¹‹å‰ï¼Œéœ€è¦æ‰“å¼€`pyproject.toml`æ–‡ä»¶ï¼Œå°†`dependencies`ä¿®æ”¹ä¸ºå¦‚ä¸‹å†…å®¹ï¼š

```toml
dependencies = [
    "wheel",
    "torch==2.1.2",
    "torchvision==0.16.2",
    "transformers",
    "tokenizers",
    "numpy",
    "opencv-python",
    "supervision==0.19.0",
    "openmim",
    "mmcv-lite==2.0.1",
    "mmdet==3.3.0",
    "mmengine==0.10.4",
    "mmcv==2.1.0",
#    'mmyolo @ git+https://github.com/onuralpszr/mmyolo.git',

]
```

<kbd>Ctrl + S</kbd>ä¿å­˜åï¼Œå®‰è£…YOLO-Worldé¡¹ç›®ï¼š

```bash
# å®‰è£…yolo-worldé¡¹ç›®
pip install -e .
```

æ£€æŸ¥`third_party`æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºï¼Œé‚£ä¹ˆå°†`mm-yolo`è¿™ä¸ªæ–‡ä»¶å¤¹æ”¾åˆ°`third_party`ä¸­ã€‚

# 2. æ•°æ®å‡†å¤‡ <a id=Title_2></a>

## 2.1 æ¦‚å†µ

YOLO-Worldçš„é¢„è®­ç»ƒæ¨¡å‹é‡‡ç”¨äº†ä¸‹è¡¨åˆ—å‡ºçš„å‡ ä¸ªæ•°æ®é›†ï¼š

| Data         | Samples |    Type    | Boxes  | Description                                                         |
| :----------- | :-----: | :--------: | :----: | :------------------------------------------------------------------ |
| Objects365v1 |  609k   | detection  | 9,621k | ä¸€ä¸ªå¤§è§„æ¨¡çš„å¯¹è±¡æ£€æµ‹æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡60ä¸‡å¼ å›¾åƒå’Œè¿‘1åƒä¸‡ä¸ªè¾¹ç•Œæ¡†     |
| GQA          |  621k   | grounding  | 3,681k | åŒ…å«è¶…è¿‡62ä¸‡å¼ å›¾åƒå’Œè¶…è¿‡368ä¸‡å¯¹é—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œç”¨äºè§†è§‰é—®ç­”ä»»åŠ¡     |
| Flickr       |  149k   | grounding  |  641k  | ä¸€ä¸ªåŒ…å«çº¦14ä¸‡å¼ å›¾åƒå’Œ641ké—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œç”¨äºè§†è§‰é—®ç­”ä»»åŠ¡          |
| CC3M-Lite    |  245k   | image-text |  821k  | ä¸€ä¸ªåŒ…å«24.5ä¸‡å›¾åƒ-æ ‡é¢˜å¯¹çš„æ•°æ®é›†ï¼Œä¸“æ³¨äºè·¨æ¨¡æ€åŒ¹é…ï¼Œå…±æœ‰821kä¸ªå®ä¾‹ |

å…¶ä¸­ï¼š
- **detection**ï¼šæŒ‡çš„æ˜¯å¯¹è±¡æ£€æµ‹ä»»åŠ¡ï¼Œç®—æ³•éœ€è¦è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡å¹¶ä¸ºå®ƒä»¬ç»˜åˆ¶è¾¹ç•Œæ¡†ã€‚
- **grounding**ï¼šå°†è‡ªç„¶è¯­è¨€æè¿°ä¸å›¾åƒä¸­çš„å…·ä½“ç‰©ä½“å»ºç«‹è”ç³»çš„è¿‡ç¨‹ã€‚
- **image-text**ï¼šæ¶‰åŠå°†å›¾åƒå†…å®¹ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è¿›è¡ŒåŒ¹é…çš„ä»»åŠ¡ï¼Œå¯èƒ½åŒ…æ‹¬å›¾åƒæ ‡æ³¨ã€å›¾åƒæè¿°ç”Ÿæˆç­‰ã€‚

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šgroundingå’Œimage-textæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šåœ¨è§†è§‰é¢†åŸŸï¼Œ"grounding"å’Œ"image-text"æ˜¯ä¸¤ä¸ªç›¸å…³ä½†æœ‰æ‰€åŒºåˆ«çš„æ¦‚å¿µï¼š

1. **Grounding**ï¼š
   - åœ¨è§†è§‰æ¥åœ°ï¼ˆVisual Groundingï¼‰ä»»åŠ¡ä¸­ï¼Œ"grounding"æŒ‡çš„æ˜¯å°†æ–‡æœ¬æè¿°ä¸­çš„è¯æ±‡æˆ–çŸ­è¯­ä¸å›¾åƒä¸­çš„å…·ä½“ç‰©ä½“æˆ–åœºæ™¯ç›¸åŒ¹é…çš„è¿‡ç¨‹ã€‚è¿™é€šå¸¸æ¶‰åŠåˆ°ç†è§£å’Œå…³è”è¯­è¨€æè¿°ä¸è§†è§‰ä¿¡æ¯ï¼Œä»¥è¯†åˆ«å›¾åƒä¸­ä¸æ–‡æœ¬æè¿°ç›¸å¯¹åº”çš„ç‰©ä½“æˆ–åŒºåŸŸã€‚
   - Groundingä»»åŠ¡å¯ä»¥è§†ä¸ºä¸€ç§è·¨æ¨¡æ€çš„æ˜ å°„ï¼Œ<font color='blue'><b>å®ƒè¦æ±‚æ¨¡å‹ä¸ä»…è¦ç†è§£æ–‡æœ¬çš„å«ä¹‰ï¼Œè¿˜è¦å°†è¿™äº›æ–‡æœ¬ä¸å›¾åƒä¸­çš„å…·ä½“è§†è§‰å®ä½“å…³è”èµ·æ¥</b></font>ã€‚

2. **Image-Text**ï¼š
   - "Image-Text"é€šå¸¸æŒ‡çš„æ˜¯å›¾åƒå’Œæ–‡æœ¬å¯¹ï¼Œè¿™ç§æ•°æ®å¯¹å¯ä»¥ç”¨äºå¤šç§ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å›¾åƒæ£€ç´¢ç­‰ã€‚åœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œå›¾åƒå’Œæ–‡æœ¬å¹¶ä¸æ˜¯ç›´æ¥ç›¸äº’æ˜ å°„ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ç§å¤šæ¨¡æ€æ•°æ®å­˜åœ¨ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹å¯¹è§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„è”åˆç†è§£ã€‚
   - Image-Textä»»åŠ¡æ›´ä¾§é‡äºå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³è”ï¼Œå¯èƒ½<font color='blue'><b>ä¸è¦æ±‚æ¨¡å‹åœ¨å›¾åƒä¸­ç²¾ç¡®åœ°å®šä½ä¸æ–‡æœ¬æè¿°ç›´æ¥å¯¹åº”çš„ç‰©ä½“æˆ–åŒºåŸŸï¼Œè€Œæ˜¯æ›´å…³æ³¨æ•´ä½“çš„è¯­ä¹‰ä¸€è‡´æ€§</b></font>ã€‚

æ€»çš„æ¥è¯´ï¼Œ"grounding"æ›´ä¾§é‡äºæ–‡æœ¬æè¿°ä¸å›¾åƒä¸­å…·ä½“ç‰©ä½“çš„ç²¾ç¡®åŒ¹é…å’Œå®šä½ï¼Œè€Œ"image-text"åˆ™æ˜¯æ›´å¹¿æ³›çš„æ¦‚å¿µï¼Œæ¶µç›–äº†å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å„ç§è¯­ä¹‰å…³è”ä»»åŠ¡ï¼Œä¸ä¸€å®šè¦æ±‚ç²¾ç¡®çš„ç‰©ä½“å®šä½ã€‚

> åœ¨YOLO-Worldè¿™æ ·çš„æ¨¡å‹ä¸­ï¼Œ"grounding"èƒ½åŠ›ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“ï¼Œè€Œ"image-text"æ•°æ®åˆ™å¯èƒ½ç”¨äºæ¨¡å‹çš„é¢„è®­ç»ƒï¼Œä»¥æé«˜å¯¹è§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„è”åˆç†è§£èƒ½åŠ›ã€‚

## 2.2 æ•°æ®é›†ç›®å½•ç»“æ„

YOLO-Worldé¡¹ç›®çš„æ•°æ®é›†éƒ½æ”¾å…¥ `data` ç›®å½•ä¸­ï¼Œå¦‚ï¼š

```
â”œâ”€â”€ coco
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ lvis
â”‚   â”œâ”€â”€ train2017
â”‚   â”œâ”€â”€ val2017
â”œâ”€â”€ flickr
â”‚   â”œâ”€â”€ annotations
â”‚   â””â”€â”€ images
â”œâ”€â”€ mixed_grounding
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ images
â”œâ”€â”€ mixed_grounding
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ images
â”œâ”€â”€ objects365v1
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ train
â”‚   â”œâ”€â”€ val
```

## 2.3 æ•°æ®é›†ä¸‹è½½ä»‹ç»ä¸ä¸‹è½½

| æ•°æ®é›†åç§°   | é¢†åŸŸ      |    ç±»åˆ«æ•°    | å›¾ç‰‡æ•°é‡ | è¯´æ˜                                                    |
| :----------- | :-------- | :----------: | :------: | :------------------------------------------------------ |
| Objects365v1 | detection |     365      |    2M    | å’Œä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸€æ ·                              |
| GQA          | grounding | æ— æ˜ç¡®çš„ç±»åˆ« |   148k   | æ˜¯ä¸€ä¸ªç”¨äºé—®ç­”çš„æ•°æ®é›†                                  |
| Flickr30k    | grounding | æ— æ˜ç¡®çš„ç±»åˆ« |   31k    | æ¯å¼ å›¾ç‰‡éƒ½æœ‰5ä¸ªcaptionså’Œä¸€ç³»åˆ—çš„bboxï¼ˆå®ä½“ç‰ˆæ‰æœ‰bboxï¼‰ |
| LVIS         | grounding |     1203     |   160k   | æ¯ä¸ªç±»åˆ«éƒ½ä¼šæœ‰ä¸€ä¸ªæè¿°è¯­å¥                              |

### 2.3.1 Objects365v1

- è®ºæ–‡é“¾æ¥ï¼š[Objects365: A Large-scale, High-quality Dataset for Object Detection](http://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf)

- è¯´æ˜ï¼šObjects365å°±æ˜¯ä¸€ä¸ªä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼Œå›¾ç‰‡/ç±»åˆ«æ²¡æœ‰å¯¹åº”çš„æè¿°æ€§æ–‡æœ¬ï¼Œä½†å•ç‹¬çš„ç±»åˆ«å¯ä»¥æœ‰è¿‘ä¹‰è¯ã€‚

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-14-16-49-56.png
    width=75%></br><center>Objects365v1æ•°æ®é›†ç¤ºä¾‹</center>
</div></br>

### 2.3.2 GQA

- è®ºæ–‡é“¾æ¥ï¼š[GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](https://arxiv.org/abs/1902.09506v3)
- è¯´æ˜ï¼šGQAæ•°æ®é›†å…¶å®ä¸æ˜¯ç”¨äºç›®æ ‡æ£€æµ‹çš„ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé—®ç­”æ•°æ®é›†ï¼Œå°±æ˜¯è¯´å®ƒçš„æ¯ä¸€å¼ å›¾ç‰‡éƒ½æœ‰ä¸€äº›åˆ—é—®é¢˜ä»¥åŠå¯¹åº”çš„ç­”æ¡ˆã€‚

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-14-16-56-16.png
    width=60%></br><center>GQAæ•°æ®é›†ç¤ºä¾‹</center>
</div></br>

### 2.3.3 Flickr30k

- è®ºæ–‡é“¾æ¥ï¼š[Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models](https://arxiv.org/abs/1505.04870)

- è¯´æ˜ï¼šåŸå§‹çš„Flickr30kæ•°æ®é›†åŒ…å«äº†31,783å¼ å›¾ç‰‡ï¼Œæ¯å¼ å›¾ç‰‡é…æœ‰5ä¸ªç”±äººç±»æ ‡æ³¨è€…æä¾›çš„å‚è€ƒå¥å­ï¼ˆcaptionsï¼‰ï¼Œå…±è®¡158,915ä¸ªå¥å­ã€‚éšåï¼Œæœ‰ç ”ç©¶è€…ä¸ºäº†è¿›ä¸€æ­¥æå‡æ•°æ®é›†çš„å¤šæ¨¡æ€ç ”ç©¶ä»·å€¼ï¼Œåœ¨Flickr30kæ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ‰©å±•ï¼Œåˆ›å»ºäº†Flickr30k Entitiesæ•°æ®é›†ã€‚Flickr30k Entitiesæ•°æ®é›†åœ¨åŸæœ‰çš„å›¾ç‰‡å’Œå¥å­çš„åŸºç¡€ä¸Šå¢åŠ äº†244,000ä¸ªå…±æŒ‡é“¾ï¼ˆcoreference chainsï¼‰å’Œ276,000ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„è¾¹ç•Œæ¡†ï¼ˆbounding boxesï¼‰ã€‚è¿™äº›è¾¹ç•Œæ¡†ä¸å›¾ç‰‡ä¸­æåŠçš„å®ä½“ç›¸å¯¹åº”ï¼Œæå¤§åœ°ä¸°å¯Œäº†æ•°æ®é›†çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¸ºå›¾åƒæè¿°ã€è§†è§‰é—®ç­”ç­‰ä»»åŠ¡æä¾›äº†æ›´ä¸°å¯Œçš„æ ‡æ³¨èµ„æºã€‚

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-14-16-32-48.png
    width=80%></br><center>Flickr30k entitiesæ•°æ®é›†ç¤ºä¾‹</center>
</div></br>

### 2.3.4 LVIS

- è®ºæ–‡é“¾æ¥ï¼š[LVIS: A Dataset for Large Vocabulary Instance Segmentation](https://arxiv.org/abs/1908.03195)
- è¯´æ˜ï¼šLVISå’Œä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸åŒçš„æ˜¯ï¼š
  - LVISæ¯ä¸ªobjectçš„åæ ‡æ˜¯å¤šè¾¹å½¢
  - LVISä¸ºæ¯ä¸ªç±»åˆ«éƒ½å®šä¹‰äº†ä¸€æ®µæ–‡å­—ç”¨æ¥æè¿°è¯¥ç±»åˆ«
  - å•ç‹¬çš„ç±»åˆ«å¯ä»¥æœ‰è¿‘ä¹‰è¯

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-14-17-05-06.png
    width=40%></br><center></center>
</div></br>

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-14-17-02-30.png
    width=70%></br><center></center>
</div></br>

### 2.3.5 æ•°æ®é›†ä¸‹è½½åœ°å€

| æ•°æ®é›†åç§° | å›¾ç‰‡ä¸‹è½½åœ°å€ | æ ‡ç­¾ä¸‹è½½åœ°å€ |
| :--- | :------| :-------------- |
| Objects365v1 | [Objects365 train](https://opendatalab.com/OpenDataLab/Objects365_v1) | [objects365_train.json](https://opendatalab.com/OpenDataLab/Objects365_v1) |
| MixedGrounding | [GQA](https://nlp.stanford.edu/data/gqa/images.zip) | [final_mixed_train_no_coco.json](https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_mixed_train_no_coco.json) |
| Flickr30k | [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/) |[final_flickr_separateGT_train.json](https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_flickr_separateGT_train.json) |
| LVIS-minival | [COCO val2017](https://cocodataset.org/) | [lvis_v1_minival_inserted_image_name.json](https://huggingface.co/GLIPModel/GLIP/blob/main/lvis_v1_minival_inserted_image_name.json) |


## 2.4 æ•°æ®é›†ç±»åˆ«

> å¯¹äºåœ¨Close-setï¼ˆä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼‰ç›®æ ‡æ£€æµ‹ä¸Šå¾®è°ƒYOLO-Worldï¼Œå»ºè®®ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®é›†ã€‚

### 2.4.1 è®¾ç½®ç±»åˆ«

å¦‚æœæˆ‘ä»¬ä½¿ç”¨ `COCO-format` è‡ªå®šä¹‰æ•°æ®é›†ï¼Œåˆ™â€œä¸éœ€è¦â€ä¸ºè‡ªå®šä¹‰è¯æ±‡è¡¨/ç±»åˆ«å®šä¹‰æ•°æ®é›†ç±»ã€‚é€šè¿‡ `metainfo=dict(classes=your_classes)`, åœ¨é…ç½®æ–‡ä»¶ä¸­æ˜¾å¼è®¾ç½® `CLASSES` å¾ˆç®€å•ï¼š

```python
coco_train_dataset = dict(
    _delete_=True,
    type='MultiModalDataset',
    dataset=dict(
        type='YOLOv5CocoDataset',
        metainfo=dict(classes=your_classes),  # è¿™é‡Œéœ€è¦å¡«å†™å…·ä½“çš„ç±»åˆ«æ•°
        data_root='data/your_data',  # æ•°æ®é›†ROOT
        ann_file='annotations/your_annotation.json',  # æ ‡ç­¾æ–‡ä»¶ï¼ˆç›¸å¯¹ROOTçš„è·¯å¾„ï¼‰
        data_prefix=dict(img='images/'),  # å›¾ç‰‡æ‰€åœ¨æ–‡ä»¶åç§°ï¼ˆç›¸å¯¹ROOTçš„è·¯å¾„ï¼‰
        filter_cfg=dict(filter_empty_gt=False, min_size=32)),
    class_text_path='data/texts/your_class_texts.json',  # ç±»åˆ«å¯¹åº”çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    pipeline=train_pipeline)
```

ä¸ºäº†è®­ç»ƒYOLO-Worldï¼Œæˆ‘ä»¬ä¸»è¦é‡‡ç”¨ä¸¤ç§æ•°æ®é›†ç±»ï¼š

1. `MultiModalDataset`ï¼šæ•°æ®é›†è¿˜æ˜¯ä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼ˆè½¬æ¢ä¸ºCOCOæ ¼å¼å³å¯ï¼‰ï¼Œåªä¸è¿‡éœ€è¦ä¸ºæ¯ä¸ªç±»åˆ«æ·»åŠ ä¸€ä¸ªæè¿°ï¼Œå³å¤šäº†ä¸€ä¸ª`class_text`æ–‡ä»¶
2. `YOLOv5MixedGroundingDataset`ï¼šæ•°æ®é›†æ ¼å¼è¿˜æ˜¯COCOæ ¼å¼ï¼Œä½†<font color='red'><b></b></font>æ¯ä¸ªå›¾ç‰‡éƒ½ä¼šæœ‰ä¸€ä¸ªæ–‡å­—æè¿°ï¼ˆcaptionï¼‰ã€‚

ä¸‹é¢æˆ‘ä»¬è¯¦ç»†è¯´æ˜ä¸€ä¸‹è¿™äºŒè€…ã€‚

### 2.4.2 å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆMultiModalDatasetï¼‰

`MultiModalDataset` æ˜¯é¢„å®šä¹‰æ•°æ®é›†ç±»çš„ç®€å•åŒ…è£…å™¨ï¼Œä¾‹å¦‚ Objects365æˆ–COCO ï¼Œå®ƒå°†æ–‡æœ¬ï¼ˆç±»åˆ«æ–‡æœ¬ï¼‰æ·»åŠ åˆ°æ•°æ®é›†å®ä¾‹ä¸­ä»¥è¿›è¡Œæ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬ã€‚

`.json` æ–‡ä»¶æ ¼å¼å¦‚ä¸‹ï¼š

```json
[
    ["A_1","A_2"],
    ["B"],
    ["C_1", "C_2", "C_3"],
    ["..."]
]
```

å…¶ä¸­ï¼š
- `"A_1"`å’Œ`"A_2"`æ˜¯ä¸€ä¸ªç±»åˆ«ï¼ŒäºŒè€…ä¸ºè¿‘ä¹‰è¯ï¼Œå¯ä»¥è¡¨ç¤ºè¿™ä¸€ä¸ªç±»åˆ«ã€‚
- `"B"`ï¼šæ˜¯ä¸€ä¸ªç±»åˆ«ï¼Œå®ƒæ²¡æœ‰è¿‘ä¹‰è¯ã€‚
- `"C_1", "C_2", "C_3"`æ˜¯ä¸€ä¸ªç±»åˆ«ï¼Œä¸‰è€…ä¸ºè¿‘ä¹‰è¯ï¼Œå‡è¡¨ç¤ºè¿™ä¸ªç±»åˆ«ã€‚

#### 1. LVISçš„jsonæ–‡ä»¶æ ¼å¼ç¤ºæ„ï¼ˆæœ‰è¿‘ä¹‰è¯ï¼‰ï¼š

```json
[
    [
        "aerosol can",
        "spray can"
    ],
    [
        "air conditioner"
    ],
    [
        "airplane",
        "aeroplane"
    ],
    [
        "alarm clock"
    ],
    [
        "alcohol",
        "alcoholic beverage"
    ],
    [
        "...",
        "æœ‰è¿‘ä¹‰è¯ï¼ï¼ï¼ï¼ï¼ï¼"
    ],
    [
        "yogurt",
        "yoghurt",
        "yoghourt"
    ],
    [
        "yoke",
        "yoke animal equipment"
    ],
    [
        "zebra"
    ],
    [
        "zucchini",
        "courgette"
    ]
]
```

#### 2. COCOçš„jsonæ–‡ä»¶æ ¼å¼ç¤ºæ„ï¼ˆæ²¡æœ‰è¿‘ä¹‰è¯ï¼‰ï¼š

```json
[
    [
        "person"
    ],
    [
        "bicycle"
    ],
    [
        "car"
    ],
    [
        "motorcycle"
    ],
    [
        "airplane"
    ],
    [
        "æ²¡æœ‰è¿‘ä¹‰è¯ï¼ï¼ï¼ï¼ï¼"
    ],
    [
        "teddy bear"
    ],
    [
        "hair drier"
    ],
    [
        "toothbrush"
    ]
]
```

#### 3. Object365V1çš„jsonæ–‡ä»¶æ ¼å¼ç¤ºæ„ï¼ˆæœ‰è¿‘ä¹‰è¯ï¼‰ï¼š

```json
[
    [
        "person"
    ],
    [
        "sneakers"
    ],
    [
        "chair"
    ],
    [
        "hat"
    ],
    [
        "lamp"
    ],
    [
        "bottle"
    ],
    [
        "cabinet",
        "shelf"
    ],
    [
        "...",
        "æœ‰è¿‘ä¹‰è¯ï¼ï¼ï¼ï¼ï¼"
    ],
    [
        "iron"
    ],
    [
        "flashlight"
    ]
]
```

### 2.4.3 æ··åˆæ–‡æœ¬å¯¹é½æ•°æ®é›†ï¼ˆYOLOv5MixedGroundingDatasetï¼‰

`YOLOv5MixedGroundingDataset` é€šè¿‡æ”¯æŒä» `json` æ–‡ä»¶åŠ è½½æ–‡æœ¬/æ ‡é¢˜æ¥æ‰©å±• `COCO` æ•°æ®é›†ã€‚å®ƒæ˜¯ä¸º `MixedGrounding`æˆ–`Flickr30K` è®¾è®¡çš„ï¼Œ<font color='red'><b>æ¯ä¸ªå¯¹è±¡éƒ½æœ‰æ–‡æœ¬æ ‡è®°</b></font>ã€‚

### 2.4.4 è‡ªå®šä¹‰æ•°æ®é›†

å¯¹äºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå»ºè®®æ ¹æ®ç”¨é€”è½¬æ¢æ³¨é‡Šæ–‡ä»¶ã€‚

> ğŸ’¡ <font color='red'><b>è¯·æ³¨æ„ï¼ŒåŸºæœ¬ä¸Šéƒ½éœ€è¦å°†æ ‡ç­¾æ–‡ä»¶è½¬æ¢ä¸ºæ ‡å‡† COCO æ ¼å¼</b></font>ã€‚

1. è‡ªå®šä¹‰è¯æ±‡è¡¨ï¼ˆå·²ä¿®å¤ï¼‰ï¼šå¯ä»¥é‡‡ç”¨ `MultiModalDataset` åŒ…è£…å™¨ï¼ˆwrapperï¼‰ä½œä¸º `Objects365` å¹¶ä¸ºè‡ªå®šä¹‰ç±»åˆ«åˆ›å»º`æ–‡æœ¬json`ã€‚
2. å¤§è¯æ±‡é‡ã€åŸºç¡€ã€å‚è€ƒï¼šè¯·éµå¾ª `MixedGrounding` æ•°æ®é›†çš„æ³¨é‡Šæ ¼å¼ï¼Œå…¶ä¸­æ·»åŠ  `caption`å’Œ`tokens_positive` æ¥<font color='red'><b>ä¸ºæ¯ä¸ªå¯¹è±¡åˆ†é…æ–‡æœ¬</b></font>ã€‚<font color='blue'><b>æ–‡æœ¬å¯ä»¥æ˜¯ç±»åˆ«æˆ–åè¯çŸ­è¯­</b></font>ã€‚-

### 2.4.5 CC3M ä¼ªæ³¨é‡Šï¼ˆPseudo Annotationsï¼‰

ä»¥ä¸‹æ³¨é‡Šæ˜¯æ ¹æ®è®ºæ–‡ä¸­çš„è‡ªåŠ¨æ ‡è®°è¿‡ç¨‹ç”Ÿæˆçš„ï¼Œç„¶åæ ¹æ®è¿™äº›æ³¨é‡ŠæŠ¥å‘Šç»“æœã€‚è¦ä½¿ç”¨CC3Mæ³¨é‡Šï¼Œéœ€è¦å…ˆå‡†å¤‡ CC3M å›¾åƒã€‚

| Data | Images | Boxes | File |
| :--: | :----: | :---: | :---: |
| CC3M-246K | 246,363 | 820,629 | [Download ğŸ¤—](https://huggingface.co/wondervictor/YOLO-World/blob/main/cc3m_pseudo_annotations.json) |
| CC3M-500K | 536,405 | 1,784,405| [Download ğŸ¤—](https://huggingface.co/wondervictor/YOLO-World/blob/main/cc3m_pseudo_500k_annotations.json) |
| CC3M-750K | 750,000 | 4,504,805 | [Download ğŸ¤—](https://huggingface.co/wondervictor/YOLO-World/blob/main/cc3m_pseudo_750k_annotations.json) |

CC3Mçš„æ ‡æ³¨ç¤ºä¾‹å¦‚ä¸‹ï¼š

<details><summary>ğŸª å±•å¼€æŸ¥çœ‹CC3Mçš„æ ‡ç­¾å†…å®¹</summary>

```json
{
    "categories": {
        {"name": "appropriate attire", "id": 1},
        {"name": "golfers", "id": 2},
        {"name": "the exhibit", "id": 3},
        {"name": "art museum", "id": 4},
        {"name": "homes", "id": 5},
        {"name": "the finish line", "id": 6},
        {"name": "a runner", "id": 7},
        {"name": "actor", "id": 8},
        {"name": "psych folk artist performs", "id": 9},
        {"name": "the opening night", "id": 10},
    },
    "images": {
        {"id": 0, "file_name": "2916026_474308128", "height": 811, "width": 1024, "caption": "actor , who recently played his first round of golf , feels rules on appropriate attire for golfers are a good idea ."},
        {"id": 1, "file_name": "426852_4023546009", "height": 400, "width": 600, "caption": "the exhibit displays homes commissioned by art museum ."},
        {"id": 2, "file_name": "429649_2882962956", "height": 706, "width": 1024, "caption": "a runner crosses the finish line during recurring competition ."},
        {"id": 3, "file_name": "2906408_1262177123", "height": 447, "width": 640, "caption": "pop artist , film director and actor ."},
        {"id": 4, "file_name": "2115477_1264179008", "height": 612, "width": 408, "caption": "psych folk artist performs at festival"},
        {"id": 5, "file_name": "1665090_1070062237", "height": 681, "width": 1024, "caption": "musical artist performs on stage supporting artist on the opening night of their tour ."},
        {"id": 6, "file_name": "440715_513854996", "height": 900, "width": 600, "caption": "this dress just reminds me so much of one of my bridesmaids !"},
        {"id": 7, "file_name": "443433_3084953150", "height": 438, "width": 640, "caption": "rhythm and blues artist performs as part of the event"},
        {"id": 8, "file_name": "2078134_958326625", "height": 447, "width": 640, "caption": "the village of person in the island"},
        {"id": 9, "file_name": "1263157_4047346620", "height": 408, "width": 612, "caption": "actor arrives at the premiere"},
        {"id": 10, "file_name": "2929378_677160606", "height": 488, "width": 640, "caption": "politician on a state visit visited country and met with religious leader"},
    }, 
    "annotations": {
        {"image_id": 0, "id": 0, "category_id": 1, "area": 422653.1869565472, "bbox": [0.0, 379.05767822265625, 976.2655029296875, 432.92852783203125], "iscrowd": 0, "score": 0.3347477614879608, "tokens": "appropriate attire"},
        {"image_id": 0, "id": 1, "category_id": 2, "area": 790882.0195608903, "bbox": [0.0, 11.648781776428223, 989.0121459960938, 799.668662071228], "iscrowd": 0, "score": 0.3464314341545105, "tokens": "golfers"},
        {"image_id": 1, "id": 2, "category_id": 3, "area": 130493.92645893525, "bbox": [129.6766357421875, 4.1426849365234375, 332.00677490234375, 393.0459747314453], "iscrowd": 0, "score": 0.3408966362476349, "tokens": "the exhibit"},
        {"image_id": 1, "id": 3, "category_id": 3, "area": 18324.43645722326, "bbox": [0.7921218872070312, 125.02227783203125, 109.76447296142578, 166.9432373046875], "iscrowd": 0, "score": 0.3054649829864502, "tokens": "the exhibit"},
        {"image_id": 1, "id": 4, "category_id": 4, "area": 238566.77633925015, "bbox": [0.240753173828125, 0.5902252197265625, 599.0253601074219, 398.2582244873047], "iscrowd": 0, "score": 0.3036207854747772, "tokens": "art museum"},
        {"image_id": 1, "id": 5, "category_id": 5, "area": 238566.77633925015, "bbox": [0.240753173828125, 0.5902252197265625, 599.0253601074219, 398.2582244873047], "iscrowd": 0, "score": 0.3074909448623657, "tokens": "homes"},
        {"image_id": 2, "id": 6, "category_id": 6, "area": 328616.3230983538, "bbox": [255.7866668701172, 18.506240844726562, 502.07691955566406, 654.5139007568359], "iscrowd": 0, "score": 0.3392699360847473, "tokens": "the finish line"},
        {"image_id": 2, "id": 7, "category_id": 7, "area": 65720.81522059813, "bbox": [247.374755859375, 270.8210754394531, 162.9920654296875, 403.2148132324219], "iscrowd": 0, "score": 0.34287354350090027, "tokens": "a runner"},
        {"image_id": 2, "id": 8, "category_id": 7, "area": 313720.4859452478, "bbox": [281.114990234375, 16.223989486694336, 478.8563232421875, 655.1453342437744], "iscrowd": 0, "score": 0.30990859866142273, "tokens": "a runner"},
        {"image_id": 2, "id": 9, "category_id": 7, "area": 96920.24000985548, "bbox": [429.16864013671875, 87.87799072265625, 176.37725830078125, 549.5053100585938], "iscrowd": 0, "score": 0.3093494474887848, "tokens": "a runner"},
        {"image_id": 3, "id": 10, "category_id": 8, "area": 47870.292117924895, "bbox": [226.57835388183594, 104.74246215820312, 147.6943817138672, 324.1172180175781], "iscrowd": 0, "score": 0.3774438202381134, "tokens": "actor"},
        {"image_id": 3, "id": 11, "category_id": 8, "area": 49782.2714155633, "bbox": [342.3399353027344, 93.310546875, 148.91860961914062, 334.29180908203125], "iscrowd": 0, "score": 0.3582227826118469, "tokens": "actor"},
        {"image_id": 3, "id": 12, "category_id": 8, "area": 42798.63825566019, "bbox": [113.33319091796875, 110.71199035644531, 134.9569549560547, 317.12806701660156], "iscrowd": 0, "score": 0.34423449635505676, "tokens": "actor"},
        {"image_id": 4, "id": 13, "category_id": 9, "area": 158787.37313683218, "bbox": [28.66830825805664, 80.38202667236328, 298.2539939880371, 532.389762878418], "iscrowd": 0, "score": 0.370086133480072, "tokens": "psych folk artist performs"},
        {"image_id": 5, "id": 14, "category_id": 10, "area": 690727.2885092197, "bbox": [3.4731175899505615, 0.566997766494751, 1020.6946680545807, 676.722736120224], "iscrowd": 0, "score": 0.30705296993255615, "tokens": "the opening night"},
        {"image_id": 5, "id": 15, "category_id": 11, "area": 377094.0525847074, "bbox": [287.8883972167969, 16.25448989868164, 569.5247497558594, 662.1205711364746], "iscrowd": 0, "score": 0.33824190497398376, "tokens": "musical artist performs"},
        {"image_id": 5, "id": 16, "category_id": 12, "area": 377094.0525847074, "bbox": [287.8883972167969, 16.25448989868164, 569.5247497558594, 662.1205711364746], "iscrowd": 0, "score": 0.31286439299583435, "tokens": "artist"},
        {"image_id": 6, "id": 17, "category_id": 13, "area": 361252.4822749605, "bbox": [56.020660400390625, 67.7717514038086, 473.9666442871094, 762.1896743774414], "iscrowd": 0, "score": 0.39791786670684814, "tokens": "bridesmaids"},
        {"image_id": 6, "id": 18, "category_id": 14, "area": 111691.20079200249, "bbox": [190.32395935058594, 241.3621826171875, 186.20994567871094, 599.8132934570312], "iscrowd": 0, "score": 0.3924933671951294, "tokens": "this dress"},
        {"image_id": 6, "id": 19, "category_id": 14, "area": 262045.76735822484, "bbox": [87.693359375, 79.24378967285156, 344.794189453125, 760.0063323974609], "iscrowd": 0, "score": 0.31617414951324463, "tokens": "this dress"},
        {"image_id": 7, "id": 20, "category_id": 15, "area": 176702.29679879732, "bbox": [159.9798583984375, 53.910037994384766, 481.47216796875, 367.00417709350586], "iscrowd": 0, "score": 0.34347665309906006, "tokens": "performs"},
        {"image_id": 7, "id": 21, "category_id": 16, "area": 26940.760472631548, "bbox": [90.44544982910156, 253.2623291015625, 231.88111877441406, 116.18350219726562], "iscrowd": 0, "score": 0.30747899413108826, "tokens": "part"},
        {"image_id": 7, "id": 22, "category_id": 16, "area": 4047.9430108852684, "bbox": [572.8274536132812, 314.28662109375, 68.55877685546875, 59.04339599609375], "iscrowd": 0, "score": 0.3061348795890808, "tokens": "part"},
        {"image_id": 8, "id": 23, "category_id": 17, "area": 498.5000101849437, "bbox": [618.4949340820312, 280.6375732421875, 19.456787109375, 25.620880126953125], "iscrowd": 0, "score": 0.31343746185302734, "tokens": "person"},
        {"image_id": 8, "id": 24, "category_id": 18, "area": 84702.77219054895, "bbox": [88.44400024414062, 160.23072814941406, 549.9750061035156, 154.0120391845703], "iscrowd": 0, "score": 0.3755825161933899, "tokens": "the village"},
        {"image_id": 8, "id": 25, "category_id": 19, "area": 136346.31888543777, "bbox": [159.03176879882812, 1.7530081272125244, 477.5589904785156, 285.5067574977875], "iscrowd": 0, "score": 0.31633585691452026, "tokens": "the island"},
        {"image_id": 9, "id": 26, "category_id": 8, "area": 190156.06273768027, "bbox": [96.15538787841797, 6.901155471801758, 474.92542266845703, 400.3914165496826], "iscrowd": 0, "score": 0.34347039461135864, "tokens": "actor"},
        {"image_id": 9, "id": 27, "category_id": 20, "area": 231256.54143596182, "bbox": [20.39750862121582, 6.0372772216796875, 577.8034801483154, 400.23390197753906], "iscrowd": 0, "score": 0.35184529423713684, "tokens": "the premiere"},
        {"image_id": 10, "id": 28, "category_id": 21, "area": 275776.2488502312, "bbox": [21.93243980407715, 12.839357376098633, 610.1771793365479, 451.96093559265137], "iscrowd": 0, "score": 0.3917006850242615, "tokens": "a state visit"},
        {"image_id": 10, "id": 29, "category_id": 22, "area": 95793.32920437756, "bbox": [85.48125457763672, 24.463964462280273, 218.02680206298828, 439.36492347717285], "iscrowd": 0, "score": 0.3641899526119232, "tokens": "politician"},
        {"image_id": 10, "id": 30, "category_id": 23, "area": 103885.73604834673, "bbox": [367.5455017089844, 50.02199172973633, 258.4236145019531, 401.9978446960449], "iscrowd": 0, "score": 0.4383631646633148, "tokens": "religious leader"},
    }
}
```

</details>

é¦–å…ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒCC3Mçš„æ ‡ç­¾æ ¼å¼å’ŒCOCOæ˜¯ä¸€æ ·çš„ï¼Œä½†æœ‰ä¸€ç‚¹ç»†å¾®çš„å·®åˆ«ï¼š
- å¯¹äº`"categories"`ï¼Œç›¸æ¯”åŸæœ¬çš„COCOå°‘äº†ä¸€ä¸ª`"supercategory"`é”®å€¼å¯¹ï¼Œè¿™ä¸ªå½±å“ä¸å¤§ã€‚
- å¯¹äº`"images"`ï¼Œç›¸æ¯”åŸæœ¬çš„COCOå¤šäº†`"caption"`ï¼Œè¿™ä¸ªæ˜¯æ•´å¼ å›¾ç‰‡çš„æ ‡æ³¨ã€‚
- å¯¹äº`"annotations"`ï¼Œç›¸æ¯”åŸç‰ˆçš„COCOå¤šäº†`"score"`å’Œ`"tokens"`
  - `"score"`ï¼šCLIPæ¨¡å‹ç»™å¯¹è±¡-tokensçš„åŒ¹é…åº¦å¾—åˆ†
  - `"tokens"`ï¼šå½“å‰å¯¹è±¡çš„æœ€ç®€å•æè¿°ï¼ˆåè¯ï¼‰

# 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ï¼ˆTraining & Evaluationï¼‰

YOLO-Worldé‡‡ç”¨MMYOLOé»˜è®¤çš„è®­ç»ƒæˆ–è¯„ä¼°è„šæœ¬ã€‚åœ¨ `configs/pretrain`å’Œ`configs/finetune_coco` ä¸­æä¾›äº†ç”¨äºé¢„è®­ç»ƒå’Œå¾®è°ƒçš„é…ç½®ã€‚

| é…ç½®æ–‡ä»¶åç§°                                                               | ç‰ˆæœ¬  | æ¨¡å‹è§„æ ¼ |      ç”¨é€”      | æ–‡æœ¬-å›¾åƒè¿æ¥æ–¹å¼ | åˆ†å‰²æ ‡ç­¾ç»†åŒ– | ä¼˜åŒ–å™¨ | Epochs |  LR   |
| :------------------------------------------------------------------------- | :---: | :------: | :------------: | :---------------: | :----------: | :----: | :----: | :---: |
| yolo_world_l_dual_vlpan_2e-4_80e_8gpus_finetune_coco.py                    |  v1   |    L     |    å¾®è°ƒCOCO    |    Dual VL-PAN    |      -       | AdamW  |   80   | 2e-4  |
| yolo_world_l_dual_vlpan_2e-4_80e_8gpus_mask-refine_finetune_coco.py        |  v1   |    L     |    å¾®è°ƒCOCO    |    Dual VL-PAN    |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_l_efficient_neck_2e-4_80e_8gpus_mask-refine_finetune_coco.py    |  v1   |    L     |    å¾®è°ƒCOCO    |  Efficient Neck   |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_l_efficient_neck_2e-4_80e_8gpus_mask-refine_finetune_coco.py |  v2   |    L     |    å¾®è°ƒCOCO    |  Efficient Neck   |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py       |  v2   |    L     |    å¾®è°ƒCOCO    |      VL-PAN       |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_l_vlpan_bn_sgd_1e-3_40e_8gpus_finetune_coco.py               |  v2   |    L     |    å¾®è°ƒCOCO    |      VL-PAN       |      -       |  SGD   |   40   | 1e-3  |
| yolo_world_v2_l_vlpan_bn_sgd_1e-3_80e_8gpus_mask-refine_finetune_coco.py   |  v2   |    L     |    å¾®è°ƒCOCO    |      VL-PAN       |      âˆš       |  SGD   |   80   | 1e-3  |
| yolo_world_v2_m_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py       |  v2   |    M     |    å¾®è°ƒCOCO    |      VL-PAN       |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_s_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py             |  v2   |    S     |    å¾®è°ƒCOCO    |         -         |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_s_rep_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py   |  v2   |    S     |    å¾®è°ƒCOCO    |    Rep-VL-PAN     |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_s_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py       |  v2   |    S     |    å¾®è°ƒCOCO    |      VL-PAN       |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_xl_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py      |  v2   |    XL    |    å¾®è°ƒCOCO    |      VL-PAN       |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_x_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py       |  v2   |    X     |    å¾®è°ƒCOCO    |      VL-PAN       |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_mask-refine_prompt_tuning_coco.py  |  v2   |    L     | æç¤ºè¯å¾®è°ƒCOCO |      VL-PAN       |      âˆš       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_prompt_tuning_coco.py              |  v2   |    L     | æç¤ºè¯å¾®è°ƒCOCO |      VL-PAN       |      -       | AdamW  |   80   | 2e-4  |
| yolo_world_v2_l_vlpan_bn_sgd_1e-3_80e_8gpus_all_finetuning_coco.py         |  v2   |    L     | æç¤ºè¯å¾®è°ƒCOCO |      VL-PAN       |      -       |  SGD   |   80   | 1e-3  |

## 3.1 æ¨¡å‹è®­ç»ƒ

è®­ç»ƒ YOLO-World å¾ˆç®€å•ï¼š

```bash
chmod +x tools/dist_train.sh

# sample command for pre-training, use AMP for mixed-precision training
./tools/dist_train.sh configs/pretrain/yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py 8 --amp
```

> æ³¨æ„ï¼šYOLO-Worldåœ¨4 ä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ‰ 8ä¸ªGPUï¼ˆæ€»å…± 32ä¸ªGPUï¼‰ã€‚å¯¹äºé¢„è®­ç»ƒï¼Œåº”æŒ‡å®šå¤šèŠ‚ç‚¹è®­ç»ƒçš„ node_rankå’Œnnodes ã€‚

è„šæœ¬å†…å®¹å¦‚ä¸‹ï¼š

```bash
#!/usr/bin/env bash

CONFIG=$1
GPUS=$2
NNODES=${NNODES:-1}
NODE_RANK=${NODE_RANK:-0}
PORT=${MASTER_PORT:-29500}
MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}

PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
python -m torch.distributed.launch \
    --nnodes=$NNODES \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_ADDR \
    --nproc_per_node=$GPUS \
    --master_port=$PORT \
    $(dirname "$0")/train.py \
    $CONFIG \
    --launcher pytorch ${@:3}
```

å…¶ä¸­ï¼š
- `CONFIG=$1`å’Œ`GPUS=$2`è¡¨ç¤ºè¯»å–è„šæœ¬çš„ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒå‚æ•°åˆ†åˆ«ç»™ä¸¤ä¸ªå˜é‡
- `NNODES=${NNODES:-1}`ï¼šè®¾ç½® `NNODES` å˜é‡ï¼Œé»˜è®¤å€¼ä¸º 1ï¼Œè¡¨ç¤ºå•ä¸ªèŠ‚ç‚¹
- `NODE_RANK=${NODE_RANK:-0}`ï¼šè®¾ç½® `NODE_RANK` å˜é‡ï¼Œé»˜è®¤å€¼ä¸º 0ï¼Œè¡¨ç¤ºèŠ‚ç‚¹çš„åºå·
- `PORT=${MASTER_PORT:-29500}`ï¼šè®¾ç½® `PORT` å˜é‡ï¼Œé»˜è®¤å€¼ä¸º 29500ï¼Œè¡¨ç¤ºä½¿ç”¨çš„ç«¯å£å·
- `MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}`ï¼šè®¾ç½® `MASTER_ADDR` å˜é‡ï¼Œé»˜è®¤å€¼ä¸º `"127.0.0.1"`ï¼Œè¡¨ç¤ºä¸»èŠ‚ç‚¹çš„åœ°å€
- `PYTHONPATH="$(dirname $0)/..":$PYTHONPATH`ï¼šè®¾ç½® `PYTHONPATH` ç¯å¢ƒå˜é‡ï¼Œæ·»åŠ å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€çº§ç›®å½•åˆ° `PYTHONPATH`
- `python -m torch.distributed.launch`ï¼šè°ƒç”¨ python å‘½ä»¤ï¼Œä½¿ç”¨ `torch.distributed.launch` æ¨¡å—å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
  - `--nnodes=$NNODES`ï¼šæŒ‡å®šèŠ‚ç‚¹æ•°é‡
  - `--node_rank=$NODE_RANK`ï¼šæŒ‡å®šå½“å‰èŠ‚ç‚¹çš„åºå·
  - `--master_addr=$MASTER_ADDR`ï¼šæŒ‡å®šä¸»èŠ‚ç‚¹çš„åœ°å€
  - `--nproc_per_node=$GPUS`ï¼šæŒ‡å®šæ¯ä¸ªèŠ‚ç‚¹ä¸Šä½¿ç”¨çš„ GPU æ•°é‡
  - `--master_port=$PORT`ï¼šæŒ‡å®šé€šä¿¡ç«¯å£
  - `$(dirname "$0")/train.py`ï¼šæŒ‡å®šè®­ç»ƒè„šæœ¬çš„è·¯å¾„
  - `$CONFIG`ï¼šä¼ é€’é…ç½®æ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°
  - `--launcher pytorch ${@:3}`  # ä½¿ç”¨ pytorch ä½œä¸ºå¯åŠ¨å™¨ï¼Œå¹¶ä¼ é€’è„šæœ¬çš„å…¶ä½™å‚æ•°

> `${@:3}` è¡¨ç¤ºä»è„šæœ¬çš„ç¬¬å››ä¸ªå‚æ•°å¼€å§‹ï¼Œä¼ é€’æ‰€æœ‰å‰©ä½™çš„å‚æ•°ç»™ `train.py` è„šæœ¬

## 3.2 æ¨¡å‹è¯„ä¼°

è¯„ä¼° YOLO-World ä¹Ÿå¾ˆç®€å•ï¼š

```bash
chmod +x tools/dist_test.sh

./tools/dist_test.sh path/to/config path/to/weights 8
```

> æ³¨ï¼šè¿™é‡Œä¸»è¦è¯„ä¼° LVIS-minival é¢„è®­ç»ƒçš„æ€§èƒ½ã€‚

## 3.3 æ‰¾åˆ°best.pth

YOLO-Worldé»˜è®¤ä¸ä¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æƒé‡ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰ä¸¤ç§æ–¹æ³•æ¥æ‰¾åˆ°`best.pth`ï¼š

1. åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
2. ä½¿ç”¨è„šæœ¬æŸ¥è¯¢æœ€ä½³æ¨¡å‹çš„epoch

### 3.3.1 åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹

å°†`save_best=None`ä¿®æ”¹ä¸º`save_best='auto'`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
default_hooks = dict(param_scheduler=dict(scheduler_type='linear',
                                          lr_factor=0.01,
                                          max_epochs=max_epochs),
                     checkpoint=dict(max_keep_ckpts=-1,
                                     save_best=None,  # ğŸ’¡ å°†save_best=Noneä¿®æ”¹ä¸ºsave_best='auto'
                                     interval=save_epoch_intervals))
```

### 3.3.2 ä½¿ç”¨è„šæœ¬æŸ¥è¯¢æœ€ä½³æ¨¡å‹çš„epoch

åˆ›å»ºä¸€ä¸ªåä¸º`find_best_epoch.py`çš„æ–‡ä»¶ï¼Œå°†ä¸‹é¢å†…å®¹ç²˜è´´è¿›å»ï¼š

```python
import json
from pathlib import Path


# ============================== å‚æ•° ==============================
src_dir = 'work_dirs/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus-refine_finetune'  # ğŸ’¡ æ¨¡å‹æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„
dst_file_name = 'scalars.json'  # è¦ä½¿ç”¨çš„jsonæ–‡ä»¶åï¼ˆå»ºè®®ä¸æ”¹ï¼‰
# ==================================================================

src_dir = Path(src_dir)
if src_dir.is_file():
    src_dir = src_dir.parent

json_paths = [json_path for json_path in src_dir.rglob(dst_file_name)]

assert len(json_paths) == 1, f"âŒ There are either 0 or more than one {dst_file_name} files present. \
    Please specify a more detailed directory path to avoid conflicts."

json_path = json_paths[0]
dst_path = json_path.parent.joinpath('best_epoch.txt')

with json_path.open('r') as f:
    lines = f.readlines()
    
lines = [line.strip() for line in lines]
lines = [json.loads(line) for line in lines]

# æ‰¾åˆ°æœ€å¤§çš„epoch
best_mAP50 = 0
best_dict = {}
for line in lines:
    if line.get('coco/bbox_mAP_50', -1) > best_mAP50:
        best_mAP50 = line['coco/bbox_mAP_50']
        best_dict = line

if best_dict:
    with dst_path.open('w') as f:
        for k, v in best_dict.items():
            f.write(f"{k}: {v}\n")
    print(f"âœ… The 'best_epoch.txt' saves in {str(dst_path)}")
else:
    print(f"âŒ Best epoch not found!")
```

è¿è¡Œè¯¥è„šæœ¬åï¼Œç»“æœå¦‚ä¸‹ï¼š

```
(yolo-world) root@xxxxx:/home/Le0v1n/code/YOLO-World# python find_best_epoch.py
âœ… The 'best_epoch.txt' saves in work_dirs/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus-refine_finetune/20240618_xxxxxx/vis_data/best_epoch.txt
```

æ‰€æœ‰çš„ç»“æœéƒ½ä¿å­˜åœ¨äº†`scalars.json`æ‰€åœ¨æ–‡ä»¶å¤¹çš„`best_epoch.txt`æ–‡ä»¶ä¸­ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```
coco/bbox_mAP: 0.648
coco/bbox_mAP_50: 0.884
coco/bbox_mAP_75: 0.691
coco/bbox_mAP_s: 0.194
coco/bbox_mAP_m: 0.5
coco/bbox_mAP_l: 0.797
data_time: 0.000408179329528557
time: 0.023147689028407362
step: 80  # ğŸ’¡ è¿™ä¸ªå°±æ˜¯æœ€ä½³çš„epochæ•°
```

### 3.3.3 FAQ

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šä½¿ç”¨ç¬¬äºŒç§æ–¹æ³•æ‰¾åˆ°çš„epochæ²¡æœ‰è¢«ä¿å­˜æ€ä¹ˆåŠï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šYOLO-Worldé»˜è®¤æ˜¯æ¯5ä¸ªepochä¿å­˜ä¸€ä¸ªæƒé‡ï¼Œæ‰€ä»¥å¯èƒ½ä¼šå‡ºç°è¿™æ ·çš„é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªèƒ½æŒ‰ç…§æœ€ä½³epochæ‰¾åˆ°æœ€è¿‘çš„epochæƒé‡æ–‡ä»¶ä½œä¸º`best.pth`ã€‚

# 4. å¾®è°ƒï¼ˆFine-tuningï¼‰

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-17-10-39-25.png
    width=60%></br><center>é€‰æ‹©ä½œè€…é¢„å…ˆè®­ç»ƒå¥½çš„YOLO-Worldæƒé‡ï¼ˆckptï¼‰å¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼</center>
</div></br>

YOLO-World æ”¯æŒé›¶æ ·æœ¬æ¨ç†å’Œä¸‰ç§ç±»å‹çš„å¾®è°ƒæ–¹æ³•ï¼š(1) æ™®é€šå¾®è°ƒï¼Œ(2) æç¤ºå¾®è°ƒï¼Œ(3) é‡å‚æ•°åŒ–å¾®è°ƒã€‚

## 4.1 æ™®é€šå¾®è°ƒï¼ˆnormal fine-tuningï¼‰

### 4.1.1 å¾®è°ƒè¦æ±‚

å¾®è°ƒ YOLO-World å¾ˆä¾¿å®œï¼š
- å®ƒä¸éœ€è¦32ä¸ªGPUæ¥è¿›è¡Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒã€‚ 8ä¸ªç”šè‡³1ä¸ªGPUå°±è¶³å¤Ÿäº†ã€‚
- å®ƒä¸éœ€è¦å¾ˆé•¿çš„æ—¶é—´å®‰æ’ï¼Œä¾‹å¦‚è®­ç»ƒYOLOv5æˆ–YOLOv8éœ€è¦300ä¸ªepochæˆ–500ä¸ªepochã€‚è€ƒè™‘åˆ°åŸä½œè€…æä¾›äº†è‰¯å¥½çš„é¢„è®­ç»ƒæƒé‡ï¼Œ<font color='red'><b>80ä¸ªæˆ–æ›´å°‘çš„epochå°±è¶³å¤Ÿäº†</b></font>ã€‚

### 4.1.2 æ•°æ®å‡†å¤‡

å¾®è°ƒæ•°æ®é›†åº”å…·æœ‰ä¸é¢„è®­ç»ƒæ•°æ®é›†ç±»ä¼¼çš„æ ¼å¼ï¼ˆå³COCOçš„æ ¼å¼ï¼‰ã€‚å¯ä»¥å‚è€ƒ [ç¬¬äºŒç« çš„å†…å®¹](#Title_2) äº†è§£æœ‰å…³å¦‚ä½•æ„å»ºæ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼š

- å¦‚æœæˆ‘ä»¬å¯¹ YOLO-World è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œé—­é›†/è‡ªå®šä¹‰è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆclose-set / custom vocabulary object detectionï¼‰ï¼Œåˆ™é¦–é€‰ä½¿ç”¨ `MultiModalDataset`å’Œ`text json`ã€‚
- å¦‚æœæˆ‘ä»¬å¯¹ YOLO-World è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œå¯Œæ–‡æœ¬ï¼ˆrich textsï¼‰æˆ–groundingä»»åŠ¡ï¼ˆgrounding tasksï¼‰çš„å¼€æ”¾è¯æ±‡æ£€æµ‹ï¼Œåˆ™é¦–é€‰ä½¿ç”¨ `MixedGroundingDataset`ã€‚

### 4.1.3 è¶…å‚æ•°å’Œé…ç½®æ–‡ä»¶ï¼ˆHyper-parameters and Configï¼‰

#### 1. åŸºæœ¬é…ç½®æ–‡ä»¶ï¼ˆBasic config fileï¼‰

å¦‚æœå¾®è°ƒæ•°æ®é›†åŒ…å«maskæ³¨é‡Šï¼š

```python
_base_ = ('../../third_party/mmyolo/configs/yolov8/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco.py')
```

å¦‚æœå¾®è°ƒæ•°æ®é›†ä¸åŒ…å«maskæ³¨é‡Šï¼š

```python
_base_ = ('../../third_party/mmyolo/configs/yolov8/yolov8_l_syncbn_fast_8xb16-500e_coco.py')
```

> ğŸ’¡ è¿™é‡Œçš„maskæ³¨é‡Šçš„æ„æ€æ˜¯ï¼Œæ•°æ®é›†ä¸­æ˜¯å¦æœ‰åˆ†å‰²æ ‡æ³¨åæ ‡ï¼Œå¦‚æœæœ‰åˆ™åŒ…å«maskæ³¨é‡Šï¼Œå¦åˆ™ä¸åŒ…å«maskæ³¨é‡Šã€‚

#### 2. è®­ç»ƒç­–ç•¥ï¼ˆTraining Schemesï¼‰

å‡å°‘ epoch å¹¶è°ƒæ•´å­¦ä¹ ç‡ï¼š

```python
max_epochs = 80
base_lr = 2e-4
weight_decay = 0.05
train_batch_size_per_gpu = 16
close_mosaic_epochs=10

train_cfg = dict(
    max_epochs=max_epochs,
    val_interval=5,
    dynamic_intervals=[((max_epochs - close_mosaic_epochs),
                        _base_.val_interval_stage2)])
```

#### 3. æ•°æ®é›†

```python
coco_train_dataset = dict(
    _delete_=True,
    type='MultiModalDataset',
    dataset=dict(
        type='YOLOv5CocoDataset',
        data_root='data/coco',
        ann_file='annotations/instances_train2017.json',
        data_prefix=dict(img='train2017/'),
        filter_cfg=dict(filter_empty_gt=False, min_size=32)),
    class_text_path='data/texts/coco_class_texts.json',
    pipeline=train_pipeline)
```

### 4.1.4 ğŸš€ æ— éœ€ RepVL-PAN æˆ–æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œå¾®è°ƒ

ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•ˆç‡å’Œç®€å•æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ²¡æœ‰ `RepVL-PAN` å’Œæ–‡æœ¬ç¼–ç å™¨çš„æƒ…å†µä¸‹å¾®è°ƒ YOLO-World çš„é«˜æ•ˆç‰ˆæœ¬ã€‚ YOLO-World çš„é«˜æ•ˆç‰ˆæœ¬ä¸åŸå§‹ YOLOv8 å…·æœ‰ç›¸ä¼¼çš„æ¶æ„æˆ–å±‚ï¼Œä½†ä½œè€…æä¾›äº†å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒæƒé‡ã€‚é¢„è®­ç»ƒçš„YOLO-Worldå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸åœ¨COCOæ•°æ®é›†ä¸Šè®­ç»ƒçš„YOLOv8ç›¸æ¯”æ›´åŠ é²æ£’ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯ä»¥å‚è€ƒ`configs/finetune_coco/yolo_world_v2_l_efficient_neck_2e-4_80e_8gpus_mask-refine_finetune_coco.py`ã€‚

> é«˜æ•ˆçš„YOLO-Worldé‡‡ç”¨ `EfficientCSPLayerWithTwoConv`ï¼Œå¹¶ä¸”å¯ä»¥<font color='blue'><b>åœ¨æ¨ç†æˆ–å¯¼å‡ºæ¨¡å‹æ—¶åˆ é™¤æ–‡æœ¬ç¼–ç å™¨</b></font>ã€‚

> é‡‡ç”¨è¿™ç§æ–¹å¼å°±ç›¸å½“äºä¸å†ä½¿ç”¨Text Embeddingè¿™äº›ä¸œè¥¿ï¼Œè€Œæ˜¯ç›´æ¥æŠŠYOLO-Worldå½“åšYOLOv8é‚£æ ·ä½¿ç”¨ã€‚

```python
model = dict(
    type='YOLOWorldDetector',
    mm_neck=True,
    neck=dict(type='YOLOWorldPAFPN',
              guide_channels=text_channels,
              embed_channels=neck_embed_channels,
              num_heads=neck_num_heads,
              block_cfg=dict(type='EfficientCSPLayerWithTwoConv')))
```

#### 6. å®Œæ•´çš„é…ç½®æ–‡ä»¶

<details><summary>ğŸª ç‚¹å‡»å±•å¼€æŸ¥çœ‹å®Œæ•´çš„é…ç½®æ–‡ä»¶</summary>

```python
_base_ = (
    '../../third_party/mmyolo/configs/yolov8/'
    'yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco.py')
custom_imports = dict(
    imports=['yolo_world'],
    allow_failed_imports=False)

# hyper-parameters
num_classes = 80
num_training_classes = 80
max_epochs = 80  # Maximum training epochs
close_mosaic_epochs = 10
save_epoch_intervals = 5
text_channels = 512
neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]
neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]
base_lr = 2e-4
weight_decay = 0.05
train_batch_size_per_gpu = 16
# æƒé‡ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth
load_from = 'pretrained_models/yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth'
text_model_name = '../pretrained_models/clip-vit-base-patch32-projection'
text_model_name = 'openai/clip-vit-base-patch32'
persistent_workers = False

# model settings
model = dict(
    type='YOLOWorldDetector',
    mm_neck=True,
    num_train_classes=num_training_classes,
    num_test_classes=num_classes,
    data_preprocessor=dict(type='YOLOWDetDataPreprocessor'),
    backbone=dict(
        _delete_=True,
        type='MultiModalYOLOBackbone',
        image_model={{_base_.model.backbone}},
        text_model=dict(
            type='HuggingCLIPLanguageBackbone',
            model_name=text_model_name,
            frozen_modules=['all'])),
    neck=dict(type='YOLOWorldPAFPN',
              guide_channels=text_channels,
              embed_channels=neck_embed_channels,
              num_heads=neck_num_heads,
              block_cfg=dict(type='EfficientCSPLayerWithTwoConv')),
    bbox_head=dict(type='YOLOWorldHead',
                   head_module=dict(type='YOLOWorldHeadModule',
                                    use_bn_head=True,
                                    embed_dims=text_channels,
                                    num_classes=num_training_classes)),
    train_cfg=dict(assigner=dict(num_classes=num_training_classes)))

# dataset settings
text_transform = [
    dict(type='RandomLoadText',
         num_neg_samples=(num_classes, num_classes),
         max_num_samples=num_training_classes,
         padding_to_max=True,
         padding_value=''),
    dict(type='mmdet.PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',
                    'flip_direction', 'texts'))
]
mosaic_affine_transform = [
    dict(
        type='MultiModalMosaic',
        img_scale=_base_.img_scale,
        pad_val=114.0,
        pre_transform=_base_.pre_transform),
    dict(type='YOLOv5CopyPaste', prob=_base_.copypaste_prob),
    dict(
        type='YOLOv5RandomAffine',
        max_rotate_degree=0.0,
        max_shear_degree=0.0,
        max_aspect_ratio=100.,
        scaling_ratio_range=(1 - _base_.affine_scale,
                             1 + _base_.affine_scale),
        # img_scale is (width, height)
        border=(-_base_.img_scale[0] // 2, -_base_.img_scale[1] // 2),
        border_val=(114, 114, 114),
        min_area_ratio=_base_.min_area_ratio,
        use_mask_refine=_base_.use_mask2refine)
]
train_pipeline = [
    *_base_.pre_transform,
    *mosaic_affine_transform,
    dict(
        type='YOLOv5MultiModalMixUp',
        prob=_base_.mixup_prob,
        pre_transform=[*_base_.pre_transform,
                       *mosaic_affine_transform]),
    *_base_.last_transform[:-1],
    *text_transform
]
train_pipeline_stage2 = [
    *_base_.train_pipeline_stage2[:-1],
    *text_transform
]
coco_train_dataset = dict(
    _delete_=True,
    type='MultiModalDataset',
    dataset=dict(
        type='YOLOv5CocoDataset',
        data_root='data/coco',
        ann_file='annotations/instances_train2017.json',
        data_prefix=dict(img='train2017/'),
        filter_cfg=dict(filter_empty_gt=False, min_size=32)),
    class_text_path='data/texts/coco_class_texts.json',
    pipeline=train_pipeline)

train_dataloader = dict(
    persistent_workers=persistent_workers,
    batch_size=train_batch_size_per_gpu,
    collate_fn=dict(type='yolow_collate'),
    dataset=coco_train_dataset)
test_pipeline = [
    *_base_.test_pipeline[:-1],
    dict(type='LoadText'),
    dict(
        type='mmdet.PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                   'scale_factor', 'pad_param', 'texts'))
]
coco_val_dataset = dict(
    _delete_=True,
    type='MultiModalDataset',
    dataset=dict(
        type='YOLOv5CocoDataset',
        data_root='data/coco',
        ann_file='annotations/instances_val2017.json',
        data_prefix=dict(img='val2017/'),
        filter_cfg=dict(filter_empty_gt=False, min_size=32)),
    class_text_path='data/texts/coco_class_texts.json',
    pipeline=test_pipeline)
val_dataloader = dict(dataset=coco_val_dataset)
test_dataloader = val_dataloader
# training settings
default_hooks = dict(
    param_scheduler=dict(
        scheduler_type='linear',
        lr_factor=0.01,
        max_epochs=max_epochs),
    checkpoint=dict(
        max_keep_ckpts=-1,
        save_best=None,
        interval=save_epoch_intervals))
custom_hooks = [
    dict(
        type='EMAHook',
        ema_type='ExpMomentumEMA',
        momentum=0.0001,
        update_buffers=True,
        strict_load=False,
        priority=49),
    dict(
        type='mmdet.PipelineSwitchHook',
        switch_epoch=max_epochs - close_mosaic_epochs,
        switch_pipeline=train_pipeline_stage2)
]
train_cfg = dict(
    max_epochs=max_epochs,
    val_interval=5,
    dynamic_intervals=[((max_epochs - close_mosaic_epochs),
                        _base_.val_interval_stage2)])
optim_wrapper = dict(
    optimizer=dict(
        _delete_=True,
        type='AdamW',
        lr=base_lr,
        weight_decay=weight_decay,
        batch_size_per_gpu=train_batch_size_per_gpu),
    paramwise_cfg=dict(
        custom_keys={'backbone.text_model': dict(lr_mult=0.01),
                     'logit_scale': dict(weight_decay=0.0)}),
    constructor='YOLOWv5OptimizerConstructor')

# evaluation settings
val_evaluator = dict(
    _delete_=True,
    type='mmdet.CocoMetric',
    proposal_nums=(100, 1, 10),
    ann_file='data/coco/annotations/instances_val2017.json',
    metric='bbox')
```
</details>

### 4.1.5 å¯åŠ¨å¾®è°ƒ

```bash
./dist_train.sh <é…ç½®æ–‡ä»¶è·¯å¾„> <NUM_GPUS> --amp
```

ä¾‹å­ï¼š

```bash
CUDA_VISIBLE_DEVICES=1,2 ./dist_train.sh configs/finetune_coco/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus-refine_finetune.py 2 --amp
```

### 4.1.6 ğŸ”¥ æ™®é€šå¾®è°ƒç¤ºä¾‹

#### 1. æ•°æ®é›†è½¬æ¢ <a id=4.1.6.1></a>

æˆ‘ä»¬æœ‰ä¸€ä¸ªç±»ä¼¼äºcoco128çš„æ•°æ®é›†ï¼Œå®ƒçš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
data/coco128
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ images
â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”œâ”€â”€ 000002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ labels
â”‚   â”‚   â”œâ”€â”€ 000001.txt
â”‚   â”‚   â”œâ”€â”€ 000002.txt
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ images
â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”œâ”€â”€ 000002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ labels
â”‚   â”‚   â”œâ”€â”€ 000001.txt
â”‚   â”‚   â”œâ”€â”€ 000002.txt
â”‚   â”‚   â””â”€â”€ ...
â””â”€â”€ val
    â”œâ”€â”€ images
    â”‚   â”œâ”€â”€ 000001.jpg
    â”‚   â”œâ”€â”€ 000002.jpg
    â”‚   â””â”€â”€ ...
    â””â”€â”€ labels
        â”œâ”€â”€ 000001.txt
        â”œâ”€â”€ 000002.txt
        â””â”€â”€ ...
```

æˆ‘ä»¬éœ€è¦åœ¨`train`ã€`val`ã€`test`æ–‡ä»¶å¤¹ä¸­éƒ½æ·»åŠ ä¸€ä¸ªç±»åˆ«æ–‡ä»¶ï¼š`classes.txt`ï¼Œå†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š

```
person, bicycle, car, motorcycle,...
```

ä¹‹åå°†æ•°æ®é›†è½¬æ¢ä¸ºCOCO2017çš„æ ·å¼ï¼Œæ‰§è¡Œå¦‚ä¸‹è„šæœ¬ï¼š

```bash
python third_party/mmyolo/tools/dataset_converters/yolo2coco.py data/coco128/train
python third_party/mmyolo/tools/dataset_converters/yolo2coco.py data/coco128/val
python third_party/mmyolo/tools/dataset_converters/yolo2coco.py data/coco128/test

mkdir data/coco128/annotations
cp data/coco128/train/annotations/result.json data/coco128/annotations/train.json
cp data/coco128/val/annotations/result.json data/coco128/annotations/val.json
cp data/coco128/test/annotations/result.json data/coco128/annotations/test.json
```

æ‰§è¡Œå®Œæ¯•åï¼Œç›®å½•ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

```
data/coco128
â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ test.json   # æµ‹è¯•é›†æ ‡ç­¾æ–‡ä»¶
â”‚   â”œâ”€â”€ train.json  # è®­ç»ƒé›†æ ‡ç­¾æ–‡ä»¶
â”‚   â””â”€â”€ val.json    # éªŒè¯é›†æ ‡ç­¾æ–‡ä»¶
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ annotations
â”‚   â”‚   â””â”€â”€ result.json
â”‚   â”œâ”€â”€ classes.txt
â”‚   â”œâ”€â”€ images
â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”œâ”€â”€ 000002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ labels
â”‚   â”‚   â”œâ”€â”€ 000001.txt
â”‚   â”‚   â”œâ”€â”€ 000002.txt
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ annotations
â”‚   â”‚   â””â”€â”€ result.json
â”‚   â”œâ”€â”€ classes.txt
â”‚   â”œâ”€â”€ images
â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”œâ”€â”€ 000002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ labels
â”‚   â”‚   â”œâ”€â”€ 000001.txt
â”‚   â”‚   â”œâ”€â”€ 000002.txt
â”‚   â”‚   â””â”€â”€ ...
â””â”€â”€ val
    â”œâ”€â”€ annotations
    â”‚   â””â”€â”€ result.json
    â”œâ”€â”€ classes.txt
    â”œâ”€â”€ images
    â”‚   â”œâ”€â”€ 000001.jpg
    â”‚   â”œâ”€â”€ 000002.jpg
    â”‚   â””â”€â”€ ...
    â””â”€â”€ labels
        â”œâ”€â”€ 000001.txt
        â”œâ”€â”€ 000002.txt
        â””â”€â”€ ...
```

ä¹‹åæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª`text json`ï¼Œä½ç½®ä¸ºï¼š`data/texts/coco128_class_texts.json`ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```json
[
    [
        "person"
    ],
    [
        "bicycle"
    ],
    [
        "car"
    ],
    [
        "motorcycle"
    ],
    [
        "..."
    ]
]
```

ä¾æ¬¡æè¿°æ•°æ®é›†ä¸­æ¯ä¸ªç±»åˆ«ã€‚

#### 2. åˆ›å»ºé…ç½®æ–‡ä»¶

è¿™é‡Œæˆ‘ä»¬é€‰ç”¨ï¼š`yolo_world_v2_s_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py`è¿™ä¸ªé…ç½®æ–‡ä»¶ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æç¤ºè¯ï¼Œæ‰€ä»¥ä¸éœ€è¦ä½¿ç”¨VL-PATHã€‚

æˆ‘ä»¬éœ€è¦å¤åˆ¶è¿™ä¸ªé…ç½®æ–‡ä»¶ï¼Œç„¶åé‡å‘½åï¼š

```bash
cp configs/finetune_coco/yolo_world_v2_s_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py configs/finetune_coco/yolo_world_v2_s_bn_2e-4_80e_1gpus_finetune_coco128.py
```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªGPUï¼Œä¸”æ ‡ç­¾ä¸­æ²¡æœ‰`mask`ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨å‘½åæ—¶æŠŠå®ƒä»¬åˆ æ‰äº†ã€‚ä½†æ˜¯æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé—®é¢˜ï¼šè¿™ä¸ªé…ç½®æ–‡ä»¶å¯¹åº”çš„`.pth`æƒé‡æ–‡ä»¶ä½œè€…å¹¶æ²¡æœ‰æä¾›ï¼Œæ‰€ä»¥è¦æƒ³ä½¿ç”¨è¿™ä¸ªé…ç½®æ–‡ä»¶ï¼Œæˆ‘ä»¬å¿…é¡»æŠŠé¢„è®­ç»ƒæƒé‡è¿™è¡Œä»£ç æ³¨é‡Šæ‰ï¼Œè¿™å°±ç›¸å½“äºæ˜¯ä»å¤´å¼€å§‹è®­ç»ƒäº†è€Œéå¾®è°ƒäº†ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åªèƒ½ä½¿ç”¨å…¶ä»–çš„é…ç½®æ–‡ä»¶ã€‚è¿™é‡Œæˆ‘é€‰æ‹©äº†`yolo_world_v2_s_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py`è¿™ä¸ªé…ç½®æ–‡ä»¶ï¼Œè¿™ä¸ªé…ç½®æ–‡ä»¶å’Œ`yolo_world_v2_s_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py`çš„ä¸åŒåœ¨äºåè€…ä½¿ç”¨äº†VL-PANå’Œmask-refineã€‚

- å› ä¸ºä½¿ç”¨äº†VL-PANï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶æˆ‘ä»¬å…¶å®åˆšåˆšå·²ç»å®šä¹‰å¥½äº†ï¼Œå³`data/texts/coco128_class_texts.json`ã€‚
- ç”±äºå®ƒä¹Ÿä½¿ç”¨äº†mask-refineï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­å…³é—­å®ƒï¼ˆè¿™é‡Œå…¶å®ä¸ç”¨å…³ä¹Ÿå¯ä»¥ğŸ˜‚ï¼‰ã€‚

æˆ‘ä»¬éœ€è¦æŠŠé…ç½®æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œç„¶åä¿®æ”¹å®ƒï¼š

```bash
cp configs/finetune_coco/yolo_world_v2_s_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py configs/finetune_coco/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus_finetune_coco128.py
```

è¯¥é…ç½®æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

<details><summary>ğŸª ç‚¹å‡»æŸ¥çœ‹å®Œæ•´çš„é…ç½®æ–‡ä»¶</summary>

```python
_base_ = (
    '../../third_party/mmyolo/configs/yolov8/'
    'yolov8_s_mask-refine_syncbn_fast_8xb16-500e_coco.py')
custom_imports = dict(
    imports=['yolo_world'],
    allow_failed_imports=False)

# hyper-parameters
num_classes = 80
num_training_classes = 80
max_epochs = 80  # Maximum training epochs
close_mosaic_epochs = 10
save_epoch_intervals = 5
text_channels = 512
neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]
neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]
base_lr = 2e-4
weight_decay = 0.05
train_batch_size_per_gpu = 16
# æƒé‡ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth
load_from = 'pretrained_models/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth'
text_model_name = '../pretrained_models/clip-vit-base-patch32-projection'
text_model_name = 'openai/clip-vit-base-patch32'
persistent_workers = False
mixup_prob = 0.15
copypaste_prob = 0.3

# model settings
model = dict(
    type='YOLOWorldDetector',
    mm_neck=True,
    num_train_classes=num_training_classes,
    num_test_classes=num_classes,
    data_preprocessor=dict(type='YOLOWDetDataPreprocessor'),
    backbone=dict(
        _delete_=True,
        type='MultiModalYOLOBackbone',
        image_model={{_base_.model.backbone}},
        text_model=dict(
            type='HuggingCLIPLanguageBackbone',
            model_name=text_model_name,
            frozen_modules=['all'])),
    neck=dict(type='YOLOWorldPAFPN',
              guide_channels=text_channels,
              embed_channels=neck_embed_channels,
              num_heads=neck_num_heads,
              block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv')),
    bbox_head=dict(type='YOLOWorldHead',
                   head_module=dict(type='YOLOWorldHeadModule',
                                    use_bn_head=True,
                                    embed_dims=text_channels,
                                    num_classes=num_training_classes)),
    train_cfg=dict(assigner=dict(num_classes=num_training_classes)))

# dataset settings
text_transform = [
    dict(type='RandomLoadText',
         num_neg_samples=(num_classes, num_classes),
         max_num_samples=num_training_classes,
         padding_to_max=True,
         padding_value=''),
    dict(type='mmdet.PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',
                    'flip_direction', 'texts'))
]
mosaic_affine_transform = [
    dict(
        type='MultiModalMosaic',
        img_scale=_base_.img_scale,
        pad_val=114.0,
        pre_transform=_base_.pre_transform),
    dict(type='YOLOv5CopyPaste', prob=copypaste_prob),
    dict(
        type='YOLOv5RandomAffine',
        max_rotate_degree=0.0,
        max_shear_degree=0.0,
        max_aspect_ratio=100.,
        scaling_ratio_range=(1 - _base_.affine_scale,
                             1 + _base_.affine_scale),
        # img_scale is (width, height)
        border=(-_base_.img_scale[0] // 2, -_base_.img_scale[1] // 2),
        border_val=(114, 114, 114),
        min_area_ratio=_base_.min_area_ratio,
        use_mask_refine=False)  # ğŸ’¡ è¿™é‡Œæˆ‘ä»¬ä¸ä½¿ç”¨äº†mask-refineï¼Œç›´æ¥è®¾ç½®ä¸ºFalse
]
train_pipeline = [
    *_base_.pre_transform,
    *mosaic_affine_transform,
    dict(
        type='YOLOv5MultiModalMixUp',
        prob=mixup_prob,
        pre_transform=[*_base_.pre_transform,
                       *mosaic_affine_transform]),
    *_base_.last_transform[:-1],
    *text_transform
]
train_pipeline_stage2 = [
    *_base_.train_pipeline_stage2[:-1],
    *text_transform
]
coco_train_dataset = dict(
    _delete_=True,
    type='MultiModalDataset',
    dataset=dict(
        type='YOLOv5CocoDataset',
        data_root='data/coco128',
        ann_file='annotations/train.json',
        data_prefix=dict(img='train/images/'),
        filter_cfg=dict(filter_empty_gt=False, min_size=32)),
    class_text_path='data/texts/coco128_class_texts.json',  # ğŸ’¡ è¿™é‡Œæˆ‘ä»¬å®šä¹‰å¥½ç±»åˆ«çš„æ–‡æœ¬æè¿°jsonæ–‡ä»¶
    pipeline=train_pipeline)

train_dataloader = dict(
    persistent_workers=persistent_workers,
    batch_size=train_batch_size_per_gpu,
    collate_fn=dict(type='yolow_collate'),
    dataset=coco_train_dataset)
test_pipeline = [
    *_base_.test_pipeline[:-1],
    dict(type='LoadText'),
    dict(
        type='mmdet.PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                   'scale_factor', 'pad_param', 'texts'))
]
coco_val_dataset = dict(
    _delete_=True,
    type='MultiModalDataset',
    dataset=dict(
        type='YOLOv5CocoDataset',
        data_root='data/coco128',
        ann_file='annotations/val.json',
        data_prefix=dict(img='val/images/'),
        filter_cfg=dict(filter_empty_gt=False, min_size=32)),
    class_text_path='data/texts/coco128_class_texts.json',
    pipeline=test_pipeline)
val_dataloader = dict(dataset=coco_val_dataset)
test_dataloader = val_dataloader
# training settings
default_hooks = dict(
    param_scheduler=dict(
        scheduler_type='linear',
        lr_factor=0.01,
        max_epochs=max_epochs),
    checkpoint=dict(
        max_keep_ckpts=-1,
        save_best=None,
        interval=save_epoch_intervals))
custom_hooks = [
    dict(
        type='EMAHook',
        ema_type='ExpMomentumEMA',
        momentum=0.0001,
        update_buffers=True,
        strict_load=False,
        priority=49),
    dict(
        type='mmdet.PipelineSwitchHook',
        switch_epoch=max_epochs - close_mosaic_epochs,
        switch_pipeline=train_pipeline_stage2)
]
train_cfg = dict(
    max_epochs=max_epochs,
    val_interval=5,
    dynamic_intervals=[((max_epochs - close_mosaic_epochs),
                        _base_.val_interval_stage2)])
optim_wrapper = dict(
    optimizer=dict(
        _delete_=True,
        type='AdamW',
        lr=base_lr,
        weight_decay=weight_decay,
        batch_size_per_gpu=train_batch_size_per_gpu),
    paramwise_cfg=dict(
        custom_keys={'backbone.text_model': dict(lr_mult=0.01),
                     'logit_scale': dict(weight_decay=0.0)}),
    constructor='YOLOWv5OptimizerConstructor')

# evaluation settings
val_evaluator = dict(
    _delete_=True,
    type='mmdet.CocoMetric',
    proposal_nums=(100, 1, 10),
    ann_file='data/coco128/annotations/val.json',
    metric='bbox')
```

</details>

#### 3. å¼€å§‹å¾®è°ƒ

è¿è¡Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

```
# è®¾ç½®è¦ä½¿ç”¨çš„GPUç´¢å¼•
export CUDA_VISIBLE_DEVICES=1

# è°ƒç”¨è„šæœ¬å¼€å§‹è®­ç»ƒï¼š
#     PARAM1: å…·ä½“çš„é…ç½®æ–‡ä»¶è·¯å¾„
#     PARAM2: ä½¿ç”¨çš„GPUæ•°é‡
#     PARAM3: æ˜¯å¦è¦å¼€å¯AMPï¼ˆAutoMixedPrecisionï¼Œè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰
bash tools/dist_train.sh \
    configs/finetune_coco/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus_finetune_coco128.py \
    1 \
    --amp
```

#### 4. è¿‡ç¨‹å±•ç¤º

```log
2024/06/19 01:21:38 - mmengine - INFO - Load checkpoint from pretrained_models/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth
2024/06/19 01:21:38 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2024/06/19 01:21:38 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2024/06/19 01:21:38 - mmengine - INFO - Checkpoints will be saved to /home/Le0v1n/code/YOLO-World/work_dirs/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus_finetune_coco128.
2024/06/19 01:21:51 - mmengine - INFO - Epoch(train)  [1][  50/1509]  base_lr: 2.0000e-04 lr: 2.1648e-06  eta: 8:27:55  time: 0.2526  data_time: 0.0334  memory: 9182  grad_norm: nan  loss: 59.1782  loss_cls: 21.0711  loss_bbox: 18.8034  loss_dfl: 19.3038
2024/06/19 01:21:59 - mmengine - INFO - Epoch(train)  [1][ 100/1509]  base_lr: 2.0000e-04 lr: 4.3738e-06  eta: 7:02:30  time: 0.1678  data_time: 0.0038  memory: 4594  grad_norm: 439.5705  loss: 53.0932  loss_cls: 16.6841  loss_bbox: 17.5144  loss_dfl: 18.8946
```

#### 5. FAQ

å¦‚æœåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æŠ¥é”™ï¼Œå¯ä»¥å‚è€ƒè¿™ä½å¤§ä½¬å†™çš„åšå®¢ğŸ˜ï¼š[æœ¬åœ°åŠäº‘æœåŠ¡å™¨ä¸Šéƒ¨ç½²yoloworldçš„è¿‡ç¨‹ä¸­é‡åˆ°ä¸€äº›é—®é¢˜æ•´ç†è®°å½•](https://blog.csdn.net/ITdaka/article/details/138863017)

## 4.2 æç¤ºè¯å¾®è°ƒï¼ˆprompt tuningï¼‰

### 4.2.1 å¸¦embeddingçš„ç®€å• YOLO-Worldï¼ˆSimple YOLO-World with Embeddingsï¼‰

ä¸ºäº†ç®€åŒ–YOLO-Worldå¹¶æ‘†è„±è¯­è¨€æ¨¡å‹ï¼Œä½œè€…å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŸºæœ¬æ£€æµ‹å™¨`SimpleYOLOWorldDetector`ï¼š

`SimpleYOLOWorldDetector` æ”¯æŒæç¤ºè¯åµŒå…¥ï¼ˆprompt embeddingsï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸”<font color='red'><b>ä¸å†åŒ…å«è¯­è¨€æ¨¡å‹ï¼</b></font>ç°åœ¨ï¼ŒYOLO-Worldé‡‡ç”¨ embeddings ä½œä¸ºè¯­è¨€è¾“å…¥ï¼ŒåµŒå…¥æ”¯æŒå‡ ç§ï¼š
- ï¼ˆ1ï¼‰æ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥ï¼ˆtext embeddingsï¼‰ï¼Œä¾‹å¦‚CLIP<font color='red'><b>è¯­è¨€</b></font>ç¼–ç å™¨
- ï¼ˆ2ï¼‰æ¥è‡ªè§†è§‰æ¨¡å‹çš„å›¾åƒåµŒå…¥ï¼ˆimage embeddingsï¼‰ï¼Œä¾‹å¦‚ï¼ŒCLIP<font color='red'><b>è§†è§‰</b></font>ç¼–ç å™¨
- ï¼ˆ3ï¼‰å›¾åƒæ–‡æœ¬èåˆåµŒå…¥
- ï¼ˆ4ï¼‰éšæœºåµŒå…¥

å…¶ä¸­ï¼š
- (1)(2)(3)æ”¯æŒé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰æ¨ç†
- (1)(2)(3)(4)å¯ä»¥å¿«é€Ÿè°ƒæ•´è‡ªå®šä¹‰æ•°æ®ã€‚

> ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šæ€ä¹ˆç†è§£â€œ<u>ä¸ºäº†ç®€åŒ–YOLO-Worldå¹¶æ‘†è„±è¯­è¨€æ¨¡å‹</u>â€è¿™å¥è¯ï¼Ÿ
> ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šæˆ‘ä»¬åˆšæ‰åœ¨æ™®é€šå¾®è°ƒçš„æ—¶å€™ï¼Œæ˜¯ä¸æ˜¯éœ€è¦æŒ‡å®šä¸€ä¸ªjsonæ–‡ä»¶ï¼Œé‡Œé¢æ˜¯æ¯ä¸ªç±»åˆ«çš„captionsï¼Œé‚£ä¹ˆæ—¢ç„¶è¿™ä¸ªjsonæ–‡ä»¶æ˜¯ä¸€ä¸ªæ–‡æœ¬ï¼Œé‚£ä¹ˆYOLO-Worldå°±éœ€è¦ä¸€ä¸ªè¯­è¨€æ¨¡å‹æ¥å¤„ç†è¿™ä¸ªæ–‡æœ¬ã€‚æ‰€ä»¥YOLO-Worldç›¸å½“äºæ˜¯è°ƒç”¨äº†å¦å¤–ä¸€ä¸ªæ¨¡å‹æ¥åšè¿™ä¸ªäº‹æƒ…ã€‚â€œ<u>æ–°çš„åŸºæœ¬æ£€æµ‹å™¨SimpleYOLOWorldDetector</u>â€æ„å‘³ç€ï¼ŒYOLO-Worldåœ¨å¤„ç†æ–‡æœ¬çš„æ—¶å€™ä¸éœ€è¦è°ƒç”¨å…¶ä»–æ¨¡å‹äº†ï¼Œå› ä¸ºè¿™ä¸ªDetectorè¦çš„ä¸æ˜¯ä¸€ä¸ªæ–‡æœ¬ï¼Œè€Œæ˜¯ä¸€ä¸ªText Embeddingï¼Œå³ä¸€ä¸ªæ–‡æœ¬åµŒå…¥çš„å‘é‡ï¼ˆå…·ä½“æ¥è¯´æ˜¯ä¸€ä¸ª`ndarray`å¯¹è±¡ï¼‰ã€‚æœ‰äº†`ndarray`å¯¹è±¡ï¼ŒYOLO-Worldçš„VL-PANæ¨¡å—å°±å¯ä»¥ç›´æ¥å¤„ç†äº†ï¼ˆå…¶å®è°ƒç”¨å…¶ä»–è¯­è¨€æ¨¡å‹æ¥å¤„ç†æ–‡æœ¬ä¹Ÿæ˜¯æƒ³åˆ°è·å–ä¸€ä¸ª`ndarray`å¯¹è±¡ğŸ˜‚ï¼‰ã€‚

åŸºæœ¬æ£€æµ‹å™¨`SimpleYOLOWorldDetector`å®šä¹‰å¦‚ä¸‹ï¼š

```python
class SimpleYOLOWorldDetector(YOLODetector):
    """Implementation of YOLO World Series"""

    def __init__(self,
                 *args,
                 mm_neck: bool = False,
                 num_train_classes=80,
                 num_test_classes=80,
                 prompt_dim=512,
                 num_prompts=80,
                 embedding_path='',
                 freeze_prompt=False,
                 use_mlp_adapter=False,
                 **kwargs)
```

è¦ä»¥é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰çš„æ–¹å¼ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬éœ€è¦é¢„å…ˆè®¡ç®—æ–‡æœ¬åµŒå…¥text embeddingsï¼ˆå›¾åƒåµŒå…¥image embeddingsï¼‰å¹¶å°†å…¶ä¿å­˜ä¸ºå…·æœ‰ `NxD` å½¢çŠ¶çš„ `numpy array (*.npy)` ï¼ˆå…¶ä¸­ï¼Œ`N` æ˜¯æç¤ºè¯çš„æ•°é‡ï¼Œ`D` æ˜¯åµŒå…¥çš„ç»´åº¦ï¼‰ã€‚<font color='red'><b>ç›®å‰ï¼Œä½œè€…åªæ”¯æŒä¸€ä¸ªç±»åˆ«æ‹¥æœ‰ä¸€ä¸ªæç¤ºè¯</b></font>ã€‚æˆ‘ä»¬å¯ä»¥å¯¹ä¸€ç±»ä½¿ç”¨å¤šä¸ªæç¤ºè¯ï¼Œä½†éœ€è¦åœ¨åå¤„ç†æ­¥éª¤ä¸­åˆå¹¶ç»“æœã€‚

### 4.2.2 æç¤ºè¯å¾®è°ƒYOLO-World

ä½œè€…å¯¹ YOLO-World è¿›è¡Œäº†å³æ—¶è°ƒæ•´ï¼Œä»¥ä¿æŒé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜è‡ªå®šä¹‰æ•°æ®é›†çš„æ€§èƒ½ã€‚æœ‰å…³ç¼–å†™æç¤ºè¯è°ƒæ•´é…ç½®çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒ `configs/prompt_tuning_coco/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_mask-refine_prompt_tuning_coco.py`ã€‚

#### 1. ä½¿ç”¨éšæœºæç¤ºè¯

```python
dict(
    type='SimpleYOLOWorldDetector',
    mm_neck=True,
    num_train_classes=num_training_classes,
    num_test_classes=num_classes,
    prompt_dim=text_channels,
    num_prompts=80,  # ä¸€ä¸ªç±»åˆ«æœ‰ä¸€ä¸ªæç¤ºè¯
    ...
)
```

#### 2. ä½¿ç”¨ CLIP åµŒå…¥ï¼ˆæ–‡æœ¬ã€å›¾åƒæˆ–æ–‡æœ¬å›¾åƒåµŒå…¥ï¼‰

> clip_vit_b32_coco_80_embeddings.npy å¯ä»¥åœ¨ HuggingFace ä¸‹è½½ï¼Œå…·ä½“ä¸ºï¼š[clip_vit_b32_coco_80_embeddings.npy](https://huggingface.co/wondervictor/YOLO-World/blob/main/clip_vit_b32_coco_80_embeddings.npy)ã€‚

```python
dict(
    type='SimpleYOLOWorldDetector',
    mm_neck=True,
    num_train_classes=num_training_classes,
    num_test_classes=num_classes,
    embedding_path='embeddings/clip_vit_b32_coco_80_embeddings.npy',
    prompt_dim=text_channels,
    num_prompts=80,
    ...
)
```

ä½¿ç”¨CLIPæ¨¡å‹è·å–å›¾åƒå’Œæ–‡æœ¬åµŒå…¥å°†ä¿æŒé›¶æ ·æœ¬æ€§èƒ½ã€‚

| Model | Config |  AP  | AP50 | AP75  | APS | APM | APL |
| :---- | :----: | :--: | :--: | :---: | :-: | :-: | :-: |
| YOLO-World-v2-L | Zero-shot | 45.7 | 61.6 | 49.8 | 29.9 | 50.0 | 60.8 |
| [YOLO-World-v2-L](./../configs/prompt_tuning_coco/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_mask-refine_prompt_tuning_coco.py) | Prompt tuning | 47.9 | 64.3 | 52.5 | 31.9 | 52.6 | 61.3 | 

å®Œæ•´çš„é…ç½®æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š

<details><summary>ğŸª ç‚¹å‡»æŸ¥çœ‹å®Œæ•´çš„æç¤ºè¯å¾®è°ƒé…ç½®æ–‡ä»¶</summary>

```python
_base_ = ('../../third_party/mmyolo/configs/yolov8/'
          'yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco.py')
custom_imports = dict(imports=['yolo_world'], allow_failed_imports=False)

# hyper-parameters
num_classes = 80
num_training_classes = 80
max_epochs = 80  # Maximum training epochs
close_mosaic_epochs = 10
save_epoch_intervals = 5
text_channels = 512
neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]
neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]
base_lr = 2e-3
weight_decay = 0.05
train_batch_size_per_gpu = 16
# æƒé‡ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth
load_from = 'pretrained_models/yolo_world_l_clip_t2i_bn_2e-3adamw_32xb16-100e_obj365v1_goldg_cc3mlite_train-ca93cd1f.pth'
persistent_workers = False

# model settings
model = dict(type='SimpleYOLOWorldDetector',
             mm_neck=True,
             num_train_classes=num_training_classes,
             num_test_classes=num_classes,
             embedding_path='embeddings/clip_vit_b32_coco_80_embeddings.npy',  # ğŸ’¡ è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºæˆ‘ä»¬è‡ªå·±çš„Text Embedding
             prompt_dim=text_channels,
             num_prompts=80,
             data_preprocessor=dict(type='YOLOv5DetDataPreprocessor'),
             backbone=dict(_delete_=True,
                           type='MultiModalYOLOBackbone',
                           text_model=None,  # ğŸ’¡ è¿™é‡Œä¸å†ä½¿ç”¨æ–‡æœ¬è¯­è¨€æ¨¡å‹
                           image_model={{_base_.model.backbone}},
                           frozen_stages=4,
                           with_text_model=False),  # ğŸ’¡ è¿™é‡Œä¸å†ä½¿ç”¨æ–‡æœ¬è¯­è¨€æ¨¡å‹
             neck=dict(type='YOLOWorldPAFPN',
                       freeze_all=True,  # ğŸ’¡ å†»ç»“Neckçš„æƒé‡ï¼ˆä¸å‚ä¸å¾®è°ƒï¼‰
                       guide_channels=text_channels,
                       embed_channels=neck_embed_channels,
                       num_heads=neck_num_heads,
                       block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv')),
             bbox_head=dict(type='YOLOWorldHead',
                            head_module=dict(
                                type='YOLOWorldHeadModule',
                                freeze_all=True,  # ğŸ’¡ å†»ç»“Headçš„æƒé‡ï¼ˆä¸å‚ä¸å¾®è°ƒï¼‰
                                use_bn_head=True,
                                embed_dims=text_channels,
                                num_classes=num_training_classes)),
             train_cfg=dict(assigner=dict(num_classes=num_training_classes)))

# dataset settings
final_transform = [
    dict(type='mmdet.PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',
                    'flip_direction'))
]
mosaic_affine_transform = [
    dict(type='Mosaic',
         img_scale=_base_.img_scale,
         pad_val=114.0,
         pre_transform=_base_.pre_transform),
    dict(type='YOLOv5CopyPaste', prob=_base_.copypaste_prob),
    dict(
        type='YOLOv5RandomAffine',
        max_rotate_degree=0.0,
        max_shear_degree=0.0,
        max_aspect_ratio=100.,
        scaling_ratio_range=(1 - _base_.affine_scale, 1 + _base_.affine_scale),
        # img_scale is (width, height)
        border=(-_base_.img_scale[0] // 2, -_base_.img_scale[1] // 2),
        border_val=(114, 114, 114),
        min_area_ratio=_base_.min_area_ratio,
        use_mask_refine=_base_.use_mask2refine)  # ğŸ’¡ å¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†æ²¡æœ‰Segmentä¿¡æ¯ï¼Œé‚£ä¹ˆå°†use_mask_refine=False
]
train_pipeline = [
    *_base_.pre_transform, *mosaic_affine_transform,
    dict(type='YOLOv5MixUp',
         prob=_base_.mixup_prob,
         pre_transform=[*_base_.pre_transform, *mosaic_affine_transform]),
    *_base_.last_transform[:-1], *final_transform
]

train_pipeline_stage2 = [*_base_.train_pipeline_stage2[:-1], *final_transform]

coco_train_dataset = dict(type='YOLOv5CocoDataset',
                          data_root='data/coco',
                          ann_file='annotations/instances_train2017.json',
                          data_prefix=dict(img='train2017/'),
                          filter_cfg=dict(filter_empty_gt=False, min_size=32),
                          pipeline=train_pipeline)

train_dataloader = dict(persistent_workers=persistent_workers,
                        batch_size=train_batch_size_per_gpu,
                        collate_fn=dict(type='yolow_collate'),
                        dataset=coco_train_dataset)

train_dataloader = dict(persistent_workers=persistent_workers,
                        batch_size=train_batch_size_per_gpu,
                        collate_fn=dict(type='yolow_collate'),
                        dataset=coco_train_dataset)
test_pipeline = [
    *_base_.test_pipeline[:-1],
    dict(type='mmdet.PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                    'scale_factor', 'pad_param'))
]
coco_val_dataset = dict(type='YOLOv5CocoDataset',
                        data_root='data/coco',
                        ann_file='annotations/instances_val2017.json',
                        data_prefix=dict(img='val2017/'),
                        filter_cfg=dict(filter_empty_gt=False, min_size=32),
                        pipeline=test_pipeline)

val_dataloader = dict(dataset=coco_val_dataset)
test_dataloader = val_dataloader
# training settings
default_hooks = dict(param_scheduler=dict(scheduler_type='linear',
                                          lr_factor=0.01,
                                          max_epochs=max_epochs),
                     checkpoint=dict(max_keep_ckpts=-1,
                                     save_best=None,
                                     interval=save_epoch_intervals))
custom_hooks = [
    dict(type='EMAHook',
         ema_type='ExpMomentumEMA',
         momentum=0.0001,
         update_buffers=True,
         strict_load=False,
         priority=49),
    dict(type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - close_mosaic_epochs,
         switch_pipeline=train_pipeline_stage2)
]
train_cfg = dict(max_epochs=max_epochs,
                 val_interval=5,
                 dynamic_intervals=[((max_epochs - close_mosaic_epochs),
                                     _base_.val_interval_stage2)])
optim_wrapper = dict(optimizer=dict(
    _delete_=True,
    type='AdamW',
    lr=base_lr,
    weight_decay=weight_decay,
    batch_size_per_gpu=train_batch_size_per_gpu),
                     paramwise_cfg=dict(bias_decay_mult=0.0,
                                        norm_decay_mult=0.0,
                                        custom_keys={
                                            'backbone.text_model':
                                            dict(lr_mult=0.01),
                                            'logit_scale':
                                            dict(weight_decay=0.0),
                                            'embeddings':
                                            dict(weight_decay=0.0)
                                        }),
                     constructor='YOLOWv5OptimizerConstructor')

# evaluation settings
val_evaluator = dict(_delete_=True,
                     type='mmdet.CocoMetric',
                     proposal_nums=(100, 1, 10),
                     ann_file='data/coco/annotations/instances_val2017.json',
                     metric='bbox')
find_unused_parameters = True
```

</details>

### 4.2.3 ğŸ”¥ æç¤ºè¯å¾®è°ƒç¤ºä¾‹ï¼ˆExample of prompt finetuningï¼‰

#### 1. æ•°æ®é›†å‡†å¤‡

è¿™é‡Œè¿˜æ˜¯å°†æ•°æ®é›†è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼Œè¯¦æƒ…å‚è€ƒï¼š[æ™®é€šå¾®è°ƒç¤ºä¾‹ä¹‹æ•°æ®é›†å‡†å¤‡](#4.1.6.1)ã€‚

#### 2. ç”Ÿæˆ text embeddings <a id=4.2.3.2></a>

éœ€è¦é€šè¿‡ `tools/generate_text_prompts.py` ç”Ÿæˆæ–‡æœ¬åµŒå…¥å¹¶å°†å…¶ä¿å­˜ä¸ºå½¢çŠ¶ä¸º `NxD`çš„`numpy.array`ã€‚æˆ‘ä»¬é¦–å…ˆéœ€è¦å‡†å¤‡ä¸€ä¸ª.jsonæ–‡ä»¶ï¼Œé‡Œé¢æ˜¯æ¯ä¸ªç±»åˆ«çš„captionsï¼Œè¿™é‡Œæˆ‘ä»¬ä»¥coco128ä¸ºä¾‹å­ï¼š

```json
[["person"], ["bicycle"], ["car"], ["motorcycle"], ["airplane"], ["bus"], ["train"], ["truck"], ["boat"], ["traffic light"], ["fire hydrant"], ["stop sign"], ["parking meter"], ["bench"], ["bird"], ["cat"], ["dog"], ["horse"], ["sheep"], ["cow"], ["elephant"], ["bear"], ["zebra"], ["giraffe"], ["backpack"], ["umbrella"], ["handbag"], ["tie"], ["suitcase"], ["frisbee"], ["skis"], ["snowboard"], ["sports ball"], ["kite"], ["baseball bat"], ["baseball glove"], ["skateboard"], ["surfboard"], ["tennis racket"], ["bottle"], ["wine glass"], ["cup"], ["fork"], ["knife"], ["spoon"], ["bowl"], ["banana"], ["apple"], ["sandwich"], ["orange"], ["broccoli"], ["carrot"], ["hot dog"], ["pizza"], ["donut"], ["cake"], ["chair"], ["couch"], ["potted plant"], ["bed"], ["dining table"], ["toilet"], ["tv"], ["laptop"], ["mouse"], ["remote"], ["keyboard"], ["cell phone"], ["microwave"], ["oven"], ["toaster"], ["sink"], ["refrigerator"], ["book"], ["clock"], ["vase"], ["scissors"], ["teddy bear"], ["hair drier"], ["toothbrush"]]
```

æˆ‘ä»¬å°†è¿™ä¸ªæ–‡ä»¶å‘½åä¸ºï¼š`data/texts/coco_class_captions.json`ã€‚ä¹‹åæˆ‘ä»¬éœ€è¦ä½¿ç”¨CLIPå¯¹å…¶è¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°ä¸€ä¸ªembeddingå‘é‡ï¼š

```bash
python tools/generate_text_prompts.py \
    --model openai/clip-vit-base-patch32 \
    --text data/texts/coco128_class_captions.json \
    --out data/texts/coco128_class_captions_embedding.npy
```

> ğŸ’¡ openai/clip-vit-base-patch32ä¸‹è½½åœ°å€ä¸ºï¼š[openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32/tree/main)ï¼Œå°†æ‰€æœ‰æ–‡ä»¶éƒ½ä¸‹è½½ä¸‹æ¥ï¼Œæ”¾åˆ°`openai`è¿™ä¸ªæ–‡ä»¶å¤¹ä¸­å³å¯ã€‚

#### 3. åˆ›å»ºå’Œä¿®æ”¹é…ç½®æ–‡ä»¶

é¦–å…ˆæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼š

```bash
cp configs/prompt_tuning_coco/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_prompt_tuning_coco.py configs/prompt_tuning_coco/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus_prompt_tuning_coco128.py
```

ä¹‹åæˆ‘ä»¬éœ€è¦ä¿®æ”¹å®ƒçš„å†…å®¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

<details><summary>ğŸª ç‚¹å‡»æŸ¥çœ‹å®Œæ•´çš„é…ç½®æ–‡ä»¶</summary>

```python
_base_ = ('../../third_party/mmyolo/configs/yolov8/'
          'yolov8_l_syncbn_fast_8xb16-500e_coco.py')
custom_imports = dict(imports=['yolo_world'], allow_failed_imports=False)

# hyper-parameters
num_classes = 80  # ğŸ’¡ ä¿®æ”¹ä¸ºè‡ªå·±çš„ç±»åˆ«æ•°
num_training_classes = 80  # ğŸ’¡ ä¿®æ”¹ä¸ºè‡ªå·±çš„ç±»åˆ«æ•°
max_epochs = 80  # ğŸ’¡ æƒ³è¦å¾®è°ƒçš„epochæ•°
close_mosaic_epochs = 10
save_epoch_intervals = 5
text_channels = 512
neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]
neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]
base_lr = 2e-4
weight_decay = 0.05
train_batch_size_per_gpu = 16  # ğŸ’¡ æ¯ä¸ªGPUçš„batchå¤§å°
# æƒé‡ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth
load_from = 'pretrained_models/yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth'
persistent_workers = False

# model settings
model = dict(type='SimpleYOLOWorldDetector',
             mm_neck=True,
             num_train_classes=num_training_classes,
             num_test_classes=num_classes,
             embedding_path='data/texts/coco128_class_captions_embedding.npy',  # ğŸ’¡ ä¿®æ”¹ä¸ºæˆ‘ä»¬è‡ªå·±ç”Ÿæˆçš„embedding vectorè·¯å¾„
             prompt_dim=text_channels,
             num_prompts=80,  # ğŸ’¡ ä¹Ÿè¦ä¿®æ”¹
             freeze_prompt=False,
             data_preprocessor=dict(type='YOLOv5DetDataPreprocessor'),
             backbone=dict(_delete_=True,
                           type='MultiModalYOLOBackbone',
                           text_model=None,
                           image_model={{_base_.model.backbone}},
                           frozen_stages=4,
                           with_text_model=False),
             neck=dict(type='YOLOWorldPAFPN',
                       freeze_all=True,
                       guide_channels=text_channels,
                       embed_channels=neck_embed_channels,
                       num_heads=neck_num_heads,
                       block_cfg=dict(type='MaxSigmoidCSPLayerWithTwoConv')),
             bbox_head=dict(type='YOLOWorldHead',
                            head_module=dict(
                                type='YOLOWorldHeadModule',
                                freeze_all=True,
                                use_bn_head=True,
                                embed_dims=text_channels,
                                num_classes=num_training_classes)),
             train_cfg=dict(assigner=dict(num_classes=num_training_classes)))

# dataset settings
coco_train_dataset = dict(type='YOLOv5CocoDataset',
                          data_root='data/coco128',
                          ann_file='annotations/train.json',
                          data_prefix=dict(img='train/images/'),
                          filter_cfg=dict(filter_empty_gt=False, min_size=32),
                          pipeline=_base_.train_pipeline)

train_dataloader = dict(persistent_workers=persistent_workers,
                        batch_size=train_batch_size_per_gpu,
                        collate_fn=dict(type='yolow_collate'),
                        dataset=coco_train_dataset)

coco_val_dataset = dict(type='YOLOv5CocoDataset',
                        data_root='data/coco128',
                        ann_file='annotations/val.json',
                        data_prefix=dict(img='val/images/'),
                        filter_cfg=dict(filter_empty_gt=False, min_size=32),
                        pipeline=_base_.test_pipeline)

val_dataloader = dict(dataset=coco_val_dataset)
test_dataloader = val_dataloader
# training settings
default_hooks = dict(param_scheduler=dict(scheduler_type='linear',
                                          lr_factor=0.01,
                                          max_epochs=max_epochs),
                     checkpoint=dict(max_keep_ckpts=-1,
                                     save_best=None,
                                     interval=save_epoch_intervals))
custom_hooks = [
    dict(type='EMAHook',
         ema_type='ExpMomentumEMA',
         momentum=0.0001,
         update_buffers=True,
         strict_load=False,
         priority=49),
    dict(type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - close_mosaic_epochs,
         switch_pipeline=_base_.train_pipeline_stage2)
]
train_cfg = dict(max_epochs=max_epochs,
                 val_interval=5,
                 dynamic_intervals=[((max_epochs - close_mosaic_epochs),
                                     _base_.val_interval_stage2)])

optim_wrapper = dict(optimizer=dict(
    _delete_=True,
    type='AdamW',
    lr=base_lr,
    weight_decay=weight_decay,
    batch_size_per_gpu=train_batch_size_per_gpu),
                     paramwise_cfg=dict(custom_keys={
                                            'backbone.text_model':
                                            dict(lr_mult=0.01),
                                            'logit_scale':
                                            dict(weight_decay=0.0),
                                            'embeddings':
                                            dict(weight_decay=0.0)
                                        }),
                     constructor='YOLOWv5OptimizerConstructor')

# evaluation settings
val_evaluator = dict(_delete_=True,
                     type='mmdet.CocoMetric',
                     proposal_nums=(100, 1, 10),
                     ann_file='data/coco128/annotations/val.json',
                     metric='bbox')
```

</details>


#### 4. å¼€å§‹è®­ç»ƒ

```bash
export CUDA_VISIBLE_DEVICES=1
bash tools/dist_train.sh \
    configs/prompt_tuning_coco/yolo_world_v2_s_vlpan_bn_2e-4_80e_1gpus_prompt_tuning_coco128.py \
    1 \
    --amp
```

#### 5. è®­ç»ƒè¿‡ç¨‹å±•ç¤º

```log
2024/06/19 09:21:17 - mmengine - INFO - Load checkpoint from pretrained_models/yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth
2024/06/19 09:21:17 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2024/06/19 09:21:17 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2024/06/19 09:21:17 - mmengine - INFO - Checkpoints will be saved to /home/Le0v1n/code/YOLO-World/work_dirs/yolo_world_v2_l_vlpan_bn_2e-4_80e_1gpus_prompt_tuning_coco128.
2024/06/19 09:21:27 - mmengine - INFO - Epoch(train)  [1][  50/1509]  base_lr: 2.0000e-04 lr: 2.1648e-06  eta: 6:39:41  time: 0.1987  data_time: 0.0376  memory: 14918  grad_norm: nan  loss: 48.7437  loss_cls: 13.6035  loss_bbox: 16.5325  loss_dfl: 18.6077
2024/06/19 09:21:33 - mmengine - INFO - Epoch(train)  [1][ 100/1509]  base_lr: 2.0000e-04 lr: 4.3738e-06  eta: 5:29:57  time: 0.1295  data_time: 0.0203  memory: 4644  grad_norm: 112.0175  loss: 47.6533  loss_cls: 13.0782  loss_bbox: 16.1408  loss_dfl: 18.4343
2024/06/19 09:21:40 - mmengine - INFO - Epoch(train)  [1][ 150/1509]  base_lr: 2.0000e-04 lr: 6.5827e-06  eta: 5:03:34  time: 0.1250  data_time: 0.0161  memory: 4418  grad_norm: 72.2968  loss: 46.9883  loss_cls: 12.8797  loss_bbox: 15.8713  loss_dfl: 18.2373
2024/06/19 09:21:46 - mmengine - INFO - Epoch(train)  [1][ 200/1509]  base_lr: 2.0000e-04 lr: 8.7917e-06  eta: 4:48:44  time: 0.1218  data_time: 0.0121  memory: 4591  grad_norm: 51.6703  loss: 45.8311  loss_cls: 12.2765  loss_bbox: 15.6204  loss_dfl: 17.9342
2024/06/19 09:21:52 - mmengine - INFO - Epoch(train)  [1][ 250/1509]  base_lr: 2.0000e-04 lr: 1.1001e-05  eta: 4:42:53  time: 0.1295  data_time: 0.0219  memory: 4818  grad_norm: 53.6273  loss: 46.9283  loss_cls: 12.7817  loss_bbox: 15.9318  loss_dfl: 18.2147
```

#### 6. FAQ

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’-1ï¼šåœ¨è®­ç»ƒå¼€å§‹çš„æ—¶å€™ï¼Œæç¤ºå¾ˆå¤škeyåŒ¹é…ä¸ä¸Šã€‚
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“-1ï¼šæˆ‘ä¹Ÿä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œåœ¨å®˜æ–¹çš„issueä¸­å‘ç°å…¶ä»–äººä¹Ÿæœ‰ç±»ä¼¼çš„ç°è±¡ã€‚ä½†æ˜¯ç»è¿‡æˆ‘è‡ªå·±çš„è®­ç»ƒï¼Œè¿™æ ·æ²¡æœ‰ä»€ä¹ˆå¤§é—®é¢˜ï¼Œæ¨¡å‹ä»ç„¶å¯ä»¥è®­ç»ƒã€‚

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’-2ï¼šPrompt Finetuningä¹‹åï¼Œæ¨¡å‹çš„zero-shotèƒ½åŠ›æ¶ˆå¤±äº†ã€‚
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“-2ï¼šè¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ä½¿ç”¨äº†text embeddingï¼Œæ‰€ä»¥æ¨¡å‹ä¸­çš„ç›¸å…³æ¨¡å—çš„å‚æ•°è¢«æ”¹å˜äº†ï¼Œè¿™å°±å¯¼è‡´äº†æ¨¡å‹ä¸¢å¤±zero-shotèƒ½åŠ›ã€‚

## 4.3 é‡å‚æ•°åŒ–å¾®è°ƒï¼ˆRe-parameterized fine-tuningï¼‰

### 4.3.1 åŸç†

é‡å‚æ•°åŒ–å°†æ–‡æœ¬åµŒå…¥ï¼ˆtext embeddingï¼‰ä½œä¸ºå‚æ•°åˆå¹¶åˆ°æ¨¡å‹ä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨æœ€ç»ˆçš„åˆ†ç±»å±‚ä¸­ï¼Œ<font color='red'><b>æ–‡æœ¬åµŒå…¥ï¼ˆtext embeddingï¼‰è¢«é‡å‚æ•°åŒ–ä¸ºç®€å•çš„ 1Ã—1 å·ç§¯å±‚</b></font>ã€‚

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-17-11-28-09.png
    width=75%></br><center></center>
</div></br>

### 4.3.2 é‡å‚æ•°åŒ–çš„ä¸»è¦ä¼˜åŠ¿

- **zero-shot**ï¼šé‡å‚æ•°åŒ–åçš„YOLO-World<font color='green'><b>ä»ç„¶å…·æœ‰é›¶æ ·æœ¬èƒ½åŠ›</b></font>ï¼ˆPrompt Finetuningä¹‹åå°±æ²¡æœ‰zero-shotçš„èƒ½åŠ›äº†ï¼‰ï¼
- **æ•ˆç‡**ï¼šé‡å‚æ•°åŒ–çš„ YOLO-World å…·æœ‰ç®€å•é«˜æ•ˆçš„æ¶æ„ï¼Œå› ä¸º `conv1x1`æ¯”`transpose & matmul` æ›´å¿«ã€‚æ­¤å¤–ï¼Œè€Œä¸”è¿˜å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–éƒ¨ç½²ã€‚
- **å‡†ç¡®æ€§**ï¼šé‡å‚æ•°åŒ–çš„YOLO-Worldæ”¯æŒå¾®è°ƒã€‚ä¸æ™®é€šçš„ `fine-tuning`æˆ–`prompt tuning` ç›¸æ¯”ï¼Œé‡å‚æ•°åŒ–ç‰ˆæœ¬å¯ä»¥ç‹¬ç«‹ä¼˜åŒ– `neck`å’Œ`head`ï¼Œå› ä¸º `neck`å’Œ`head` æœ‰ä¸åŒçš„å‚æ•°ï¼Œ<font color='red'><b>âš ï¸ ä¸å†ä¾èµ–äº text embeddings</b></font>ï¼ä¾‹å¦‚ï¼Œåœ¨COCO val2017ä¸Šé‡å‚æ•°åŒ–å¾®è°ƒçš„YOLO-Worldçš„mAPä¸º46.3ï¼Œè€Œæ™®é€šå¾®è°ƒï¼ˆNormal Finetuningï¼‰ç‰ˆæœ¬çš„mAPä¸º46.1ï¼Œæ‰€æœ‰è¶…å‚æ•°ä¿æŒä¸å˜ã€‚

### 4.3.3 å¦‚ä½•ä½¿ç”¨

#### 1. å‡†å¤‡è‡ªå®šä¹‰Text Embeddingå‘é‡

éœ€è¦é€šè¿‡ `tools/generate_text_prompts.py` ç”Ÿæˆæ–‡æœ¬åµŒå…¥å¹¶å°†å…¶ä¿å­˜ä¸ºå½¢çŠ¶ä¸º `NxD`çš„`numpy.array`ã€‚

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šä¸æ˜¯è¯´Re-parameterize Finetuningä¸å†ä¾èµ–äºText Embeddingäº†å—ï¼Ÿä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜è¦ç”Ÿæˆè¿™ä¸ª`ndarray`çš„åµŒå…¥å‘é‡ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šåªæ˜¯è¯´åœ¨è®­ç»ƒé˜¶æ®µä¸å†ä¾èµ–Text Embeddingå‘é‡ï¼Œåœ¨è®­ç»ƒå¼€å§‹å‰æˆ‘ä»¬æ˜¯éœ€è¦å®ƒçš„ï¼Œç›®çš„æ˜¯ç”Ÿæˆä¸€ä¸ªåµŒå…¥äº†è¯¥å‘é‡çš„æ¨¡å‹ã€‚


#### 2. é‡å‚æ•°åŒ–é¢„è®­ç»ƒæƒé‡

è¿™ä¸€æ­¥éœ€è¦æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ–‡ä»¶ï¼š

- text embeddingsï¼šæ–‡æœ¬åµŒå…¥
- model checkpointï¼šæ¨¡å‹æƒé‡æ–‡ä»¶

text embeddingsæˆ‘ä»¬åœ¨ç¬¬ä¸€æ­¥åˆšåˆšç”Ÿæˆï¼Œç°åœ¨æ˜¯è¦æŒ‘é€‰ä¸€ä¸ªåˆé€‚çš„ckptæ¥è¿›è¡Œé‡å‚æ•°åŒ–ã€‚è¿™é‡Œæˆ‘ä»¬é€‰æ‹©`pretrained_models/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth`è¿›è¡Œé‡å‚æ•°åŒ–ã€‚é‡å‚æ•°åŒ–ä¼šæ”¹å˜ä¸¤ä¸ª`module`ï¼š

- head (`YOLOWorldHeadModule`) 
- neck (`MaxSigmoidCSPLayerWithTwoConv`) 

> ğŸ’¡ æƒé‡ä¸‹è½½é“¾æ¥ï¼š[yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth](https://huggingface.co/wondervictor/YOLO-World/blob/main/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth)

é‚£æˆ‘ä»¬å¼€å§‹é‡å‚æ•°åŒ–ï¼š

```bash
python tools/reparameterize_yoloworld.py \
    --model pretrained_models/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth \
    --out-dir pretrained_models/re-parameterized/ \
    --text-embed data/texts/coco128_class_captions_embedding.npy \
    --conv-neck
```

ç„¶åè¿è¡Œ -> æŠ¥é”™ğŸ˜‘ï¼š

```
Traceback (most recent call last):
  File "/home/Le0v1n/code/YOLO-World/tools/reparameterize_yoloworld.py", line 139, in <module>
    main()
  File "/home/Le0v1n/code/YOLO-World/tools/reparameterize_yoloworld.py", line 135, in main
    torch.save(model, os.path.join(args.out_dir, model_name))
  File "/root/anaconda3/envs/yolo-world/lib/python3.9/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/root/anaconda3/envs/yolo-world/lib/python3.9/site-packages/torch/serialization.py", line 492, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/root/anaconda3/envs/yolo-world/lib/python3.9/site-packages/torch/serialization.py", line 463, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory pretrained_models/re-parameterized does not exist.
```

å¥½ï¼Œé‚£æˆ‘ä»¬è‡ªå·±ä¿®æ”¹ä¸€ä¸‹è¿™ä¸ªä»£ç ï¼Œåˆ«è®©å®ƒé‚£ä¹ˆå‘†ï¼š

```python
import argparse
import torch
import numpy as np
from pathlib import Path


def parse_args():
    parser = argparse.ArgumentParser("Reparameterize YOLO-World")
    parser.add_argument('--model', help='model checkpoints to reparameterize')
    parser.add_argument('--out-dir', help='output checkpoints')
    parser.add_argument('--text-embed', help='text embeddings to reparameterized into YOLO-World')
    parser.add_argument('--conv-neck', action='store_true', help='whether using 1x1 conv in RepVL-PAN')

    args = parser.parse_args()
    return args


def convert_head(scale, bias, text_embed):
    N, D = text_embed.shape
    weight = (text_embed * scale.exp()).view(N, D, 1, 1)
    bias = torch.ones(N) * bias
    return weight, bias


def reparameterize_head(state_dict, embeds):

    cls_layers = [
        'bbox_head.head_module.cls_contrasts.0',
        'bbox_head.head_module.cls_contrasts.1',
        'bbox_head.head_module.cls_contrasts.2'
    ]

    for i in range(3):
        scale = state_dict[cls_layers[i] + '.logit_scale']
        bias = state_dict[cls_layers[i] + '.bias']
        weight, bias = convert_head(scale, bias, embeds)
        state_dict[cls_layers[i] + '.conv.weight'] = weight
        state_dict[cls_layers[i] + '.conv.bias'] = bias
        del state_dict[cls_layers[i] + '.bias']
        del state_dict[cls_layers[i] + '.logit_scale']
    return state_dict


def convert_neck_split_conv(input_state_dict, block_name, text_embeds,
                            num_heads):
    if block_name + '.guide_fc.weight' not in input_state_dict:
        return input_state_dict
    guide_fc_weight = input_state_dict[block_name + '.guide_fc.weight']
    guide_fc_bias = input_state_dict[block_name + '.guide_fc.bias']
    guide = text_embeds @ guide_fc_weight.transpose(0,
                                                    1) + guide_fc_bias[None, :]
    N, D = guide.shape
    guide = list(guide.split(D // num_heads, dim=1))
    del input_state_dict[block_name + '.guide_fc.weight']
    del input_state_dict[block_name + '.guide_fc.bias']
    for i in range(num_heads):
        input_state_dict[block_name +
                         f'.guide_convs.{i}.weight'] = guide[i][:, :, None,
                                                                None]
    return input_state_dict


def convert_neck_weight(input_state_dict, block_name, embeds, num_heads):
    guide_fc_weight = input_state_dict[block_name + '.guide_fc.weight']
    guide_fc_bias = input_state_dict[block_name + '.guide_fc.bias']
    guide = embeds @ guide_fc_weight.transpose(0, 1) + guide_fc_bias[None, :]
    N, D = guide.shape
    del input_state_dict[block_name + '.guide_fc.weight']
    del input_state_dict[block_name + '.guide_fc.bias']
    input_state_dict[block_name + '.guide_weight'] = guide.view(
        N, D // num_heads, num_heads)
    return input_state_dict


def reparameterize_neck(state_dict, embeds, type='conv'):

    neck_blocks = [
        'neck.top_down_layers.0.attn_block',
        'neck.top_down_layers.1.attn_block',
        'neck.bottom_up_layers.0.attn_block',
        'neck.bottom_up_layers.1.attn_block'
    ]
    if "neck.top_down_layers.0.attn_block.bias" not in state_dict:
        return state_dict
    for block in neck_blocks:
        num_heads = state_dict[block + '.bias'].shape[0]
        if type == 'conv':
            convert_neck_split_conv(state_dict, block, embeds, num_heads)
        else:
            convert_neck_weight(state_dict, block, embeds, num_heads)
    return state_dict


def main():
    args = parse_args()

    # åŠ è½½ckpt
    model = torch.load(args.model, map_location='cpu')
    state_dict = model['state_dict']

    # åŠ è½½Text Embeddingå‘é‡
    embeddings = torch.from_numpy(np.load(args.text_embed))

    # ç§»é™¤æ–‡æœ¬ç¼–ç å™¨
    keys = list(state_dict.keys())
    keys = [x for x in keys if "text_model" not in x]

    state_dict_wo_text = {x: state_dict[x] for x in keys}
    print("âœ… Removing text encoder")

    state_dict_wo_text = reparameterize_head(state_dict_wo_text, embeddings)
    print("âœ… Reparameterizing HEAD")

    if args.conv_neck:
        neck_type = "conv"
    else:
        neck_type = "linear"

    state_dict_wo_text = reparameterize_neck(state_dict_wo_text, embeddings, neck_type)
    print("âœ… Reparameterizing HEAD")

    # ç”¨æ–°å†…å®¹æ›¿æ¢ä¹‹å‰çš„ckptå­—å…¸
    model['state_dict'] = state_dict_wo_text
    
    # ä¿å­˜æ–°çš„ckpt
    model_name = Path(args.model)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºæ–‡ä»¶å¤¹
    dst_path = out_dir.joinpath(model_name.stem + f'_rep_{neck_type}' + model_name.suffix)
    
    torch.save(model, str(dst_path))
    print(f"âœ… The reparameterized ckpt save in {str(dst_path)}.")


if __name__ == "__main__":
    main()
```

ä»£ç æ”¹åŠ¨åœ¨`main()`å‡½æ•°ä¸­ï¼Œä¸»è¦æ˜¯å°†ä¹‹å‰çš„`os`åº“æ›¿æ¢ä¸ºäº†`pathlib`åº“ã€‚

è¿è¡Œå®Œæ¯•åä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„ckptï¼š

```log
(yolo-world) root@Xxxxx:/home/Le0v1n/code/YOLO-World# bash tools/re-parameterize_ckpt.sh
âœ… Removing text encoder
âœ… Reparameterizing HEAD
âœ… Reparameterizing HEAD
âœ… The reparameterized ckpt save in pretrained_models/re-parameterized/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea_rep_conv.pth.
```

```
pretrained_models
â”œâ”€â”€ re-parameterized
â”‚   â””â”€â”€ yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea_rep_conv.pth  # ğŸ’¡ æ–°ç”Ÿæˆçš„ckpt
â”œâ”€â”€ yolo_world_v2_l_obj365v1_goldg_cc3mlite_pretrain-ca93cd1f.pth  # é¢„è®­ç»ƒæƒé‡
â”œâ”€â”€ yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea.pth  # é¢„è®­ç»ƒæƒé‡
â””â”€â”€ yolo_world_v2_s_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco_ep80-492dc329.pth  # é¢„è®­ç»ƒæƒé‡
```

#### 3. å‡†å¤‡æ¨¡å‹é…ç½®

æˆ‘ä»¬ä»¥`configs/finetune_coco/yolo_world_v2_s_rep_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py`è¿™ä¸ªé…ç½®ä¸ºä¾‹è¿›è¡Œé‡å‚æ•°åŒ–è®­ç»ƒã€‚åœ¨è¿™ä¸ªé…ç½®æ–‡ä»¶ä¸­ï¼Œä¸»è¦å…³æ³¨çš„æ˜¯ï¼š

- `RepYOLOWorldHeadModule`
- `RepConvMaxSigmoidCSPLayerWithTwoConv`

```python
# RepConvMaxSigmoidCSPLayerWithTwoConv
neck=dict(type='YOLOWorldPAFPN',  # ğŸ’¡ Neckçš„freeze_all=Trueæ²¡æœ‰äº†ï¼Œè¯´æ˜neckçš„å‚æ•°ä¹Ÿä¼šè¢«è°ƒæ•´
          guide_channels=num_classes,
          embed_channels=neck_embed_channels,
          num_heads=neck_num_heads,
          block_cfg=dict(type='RepConvMaxSigmoidCSPLayerWithTwoConv',
                         guide_channels=num_classes)),

# RepYOLOWorldHeadModule
bbox_head=dict(head_module=dict(type='RepYOLOWorldHeadModule',  # ğŸ’¡ Headçš„freeze_all=Trueæ²¡æœ‰äº†ï¼Œè¯´æ˜Headçš„å‚æ•°ä¹Ÿä¼šè¢«è°ƒæ•´
                                embed_dims=text_channels,
                                num_guide=num_classes,
                                num_classes=num_classes)),
```

- `neck`å’Œ`bbox_head`ä¸­çš„`freeze_all=True`è¿™ä¸ªå‚æ•°æ²¡æœ‰äº†ï¼Œè¯´æ˜è¿™ä¸¤ä¸ªæ¨¡å—çš„ä¸­çš„æƒé‡ä¼šè¢«å¾®è°ƒã€‚
- `neck`çš„`YOLOWorldPAFPN`ä¸­çš„`block_cfg`ç±»å‹ä»`MaxSigmoidCSPLayerWithTwoConv`å˜ä¸ºäº†`RepConvMaxSigmoidCSPLayerWithTwoConv`ï¼Œæ–°å¢çš„`Rep`å­—æ®µè¡¨æ˜Reparameterizedï¼Œå³é‡å‚æ•°åŒ–çš„ã€‚
- `bbox_head`ä¸­çš„`head_module`çš„ç±»å‹ä»`YOLOWorldHeadModule`å˜ä¸ºäº†`RepYOLOWorldHeadModule`ï¼Œï¼Œæ–°å¢çš„`Rep`å­—æ®µè¡¨æ˜Reparameterizedï¼Œå³é‡å‚æ•°åŒ–çš„ã€‚

---

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šä¹‹å‰çš„Prompt Finetuningçš„é…ç½®æ–‡ä»¶ä¸­æœ‰Text Embeddingçš„è·¯å¾„ï¼ŒRe-paramterize Finetuningä¸éœ€è¦å—ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šæ˜¯çš„ï¼Œåœ¨Prompt Finetuningä¸­æˆ‘ä»¬éœ€è¦ä¼ å…¥ä¸€ä¸ªText Embeddingå‘é‡ï¼Œä»è€Œå®ç°æç¤ºè¯å¾®è°ƒã€‚ä½†åœ¨é‡å‚æ•°åŒ–å¾®è°ƒä¸­ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ç¬¬äºŒæ­¥çš„æ—¶å€™å·²ç»ä½¿ç”¨Text Embeddingå‘é‡ç”Ÿæˆäº†ä¸€ä¸ªckptï¼Œæ‰€ä»¥è¿™é‡Œåªéœ€è¦ä¼ å…¥ckptè·¯å¾„å°±è¡Œï¼Œä¸éœ€è¦Text Embeddingäº†ã€‚

#### 4. å¯åŠ¨å¾®è°ƒ

å’Œä¹‹å‰çš„è®­ç»ƒæ–¹å¼ä¸€æ ·ï¼Œæˆ‘ä»¬åªæ˜¯ä¼ å…¥çš„é…ç½®æ–‡ä»¶ä¸åŒè€Œå·²ã€‚

```bash
./dist_train.sh <é…ç½®æ–‡ä»¶è·¯å¾„> <NUM_GPUS> --amp
```

### 4.3.4 ğŸ”¥ é‡å‚æ•°åŒ–å¾®è°ƒç¤ºä¾‹

#### 1. å‡†å¤‡æ•°æ®é›†

è¿™é‡Œè¿˜æ˜¯å°†æ•°æ®é›†è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼Œè¯¦æƒ…å‚è€ƒï¼š[æ™®é€šå¾®è°ƒç¤ºä¾‹ä¹‹æ•°æ®é›†å‡†å¤‡](#4.1.6.1)ã€‚

#### 2. ç”Ÿæˆ text embeddings

ä¸Prompt Finetuningä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆText Embeddingï¼‰ï¼Œæµç¨‹å‚è€ƒ[4.2.3.2 ç”Ÿæˆ text embeddings](#4.2.3.2)ã€‚

#### 3. åˆ›å»ºå’Œä¿®æ”¹é…ç½®æ–‡ä»¶

é¦–å…ˆæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼š

```bash
cp configs/finetune_coco/yolo_world_v2_s_rep_vlpan_bn_2e-4_80e_8gpus_mask-refine_finetune_coco.py configs/finetune_coco/yolo_world_v2_s_rep_vlpan_bn_2e-4_80e_1gpus_finetune_coco128.py
```

ç„¶åå¯¹å…¶è¿›è¡Œä¿®æ”¹ï¼Œå®Œæ•´çš„é…ç½®æ–‡ä»¶ç¤ºä¾‹å¦‚ä¸‹ï¼š

<details><summary>ğŸª ç‚¹å‡»æŸ¥çœ‹å®Œæ•´çš„é‡å‚æ•°åŒ–å¾®è°ƒé…ç½®æ–‡ä»¶</summary>

```python
_base_ = ('../../third_party/mmyolo/configs/yolov8/'
          'yolov8_s_mask-refine_syncbn_fast_8xb16-500e_coco.py')
custom_imports = dict(imports=['yolo_world'], allow_failed_imports=False)

# hyper-parameters
num_classes = 80  # ğŸ’¡ æ›¿æ¢ä¸ºè‡ªå·±çš„ç±»åˆ«æ•°
num_training_classes = 80  # ğŸ’¡ æ›¿æ¢ä¸ºè‡ªå·±çš„ç±»åˆ«æ•°
max_epochs = 80  # Maximum training epochs
close_mosaic_epochs = 10
save_epoch_intervals = 5
text_channels = 512
neck_embed_channels = [128, 256, _base_.last_stage_out_channels // 2]
neck_num_heads = [4, 8, _base_.last_stage_out_channels // 2 // 32]
base_lr = 2e-4
weight_decay = 0.05
train_batch_size_per_gpu = 16
# â— è¿™é‡Œçš„é¢„è®­ç»ƒæƒé‡åº”è¯¥é€‰æ‹©æˆ‘ä»¬è‡ªå·±ç”Ÿæˆçš„
load_from = 'pretrained_models/re-parameterized/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea_rep_conv.pth'
persistent_workers = False
mixup_prob = 0.15
copypaste_prob = 0.3

# model settings
model = dict(type='SimpleYOLOWorldDetector',
             mm_neck=True,
             num_train_classes=num_classes,
             num_test_classes=num_classes,
             reparameterized=True,  # ğŸ’¡ å¼€å¯é‡å‚æ•°åŒ–
             data_preprocessor=dict(type='YOLOv5DetDataPreprocessor'),
             backbone=dict(_delete_=True,
                           type='MultiModalYOLOBackbone',
                           text_model=None,  # ğŸ’¡ ä¸å†ä½¿ç”¨è¯­è¨€æ¨¡å‹
                           image_model={{_base_.model.backbone}},
                           with_text_model=False),  # ğŸ’¡ ä¸å†ä½¿ç”¨è¯­è¨€æ¨¡å‹
             neck=dict(type='YOLOWorldPAFPN',  # ğŸ’¡ Neckçš„freeze_all=Trueæ²¡æœ‰äº†ï¼Œè¯´æ˜neckçš„å‚æ•°ä¹Ÿä¼šè¢«è°ƒæ•´
                       guide_channels=num_classes,
                       embed_channels=neck_embed_channels,
                       num_heads=neck_num_heads,
                       block_cfg=dict(type='RepConvMaxSigmoidCSPLayerWithTwoConv',  # ğŸ’¡ blockä¹Ÿä½¿ç”¨çš„æ˜¯Repå¼€å¤´çš„ï¼Œè¡¨æ˜æ˜¯é‡å‚æ•°åŒ–
                                      guide_channels=num_classes)),
             bbox_head=dict(head_module=dict(type='RepYOLOWorldHeadModule',  # ğŸ’¡ Headçš„freeze_all=Trueæ²¡æœ‰äº†ï¼Œè¯´æ˜Headçš„å‚æ•°ä¹Ÿä¼šè¢«è°ƒæ•´
                                             embed_dims=text_channels,
                                             num_guide=num_classes,
                                             num_classes=num_classes)),
             train_cfg=dict(assigner=dict(num_classes=num_classes)))

# dataset settings
final_transform = [
    dict(type='mmdet.PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',
                    'flip_direction'))
]
mosaic_affine_transform = [
    dict(type='Mosaic',
         img_scale=_base_.img_scale,
         pad_val=114.0,
         pre_transform=_base_.pre_transform),
    dict(type='YOLOv5CopyPaste', prob=copypaste_prob),
    dict(
        type='YOLOv5RandomAffine',
        max_rotate_degree=0.0,
        max_shear_degree=0.0,
        max_aspect_ratio=100.,
        scaling_ratio_range=(1 - _base_.affine_scale, 1 + _base_.affine_scale),
        # img_scale is (width, height)
        border=(-_base_.img_scale[0] // 2, -_base_.img_scale[1] // 2),
        border_val=(114, 114, 114),
        min_area_ratio=_base_.min_area_ratio,
        use_mask_refine=_base_.use_mask2refine)
]
train_pipeline = [
    *_base_.pre_transform, *mosaic_affine_transform,
    dict(type='YOLOv5MixUp',
         prob=mixup_prob,
         pre_transform=[*_base_.pre_transform, *mosaic_affine_transform]),
    *_base_.last_transform[:-1], *final_transform
]

train_pipeline_stage2 = [*_base_.train_pipeline_stage2[:-1], *final_transform]

coco_train_dataset = dict(type='YOLOv5CocoDataset',
                          data_root='data/coco128',
                          ann_file='annotations/train.json',
                          data_prefix=dict(img='train/images/'),
                          filter_cfg=dict(filter_empty_gt=False, min_size=32),
                          pipeline=train_pipeline)

train_dataloader = dict(persistent_workers=persistent_workers,
                        batch_size=train_batch_size_per_gpu,
                        collate_fn=dict(type='yolow_collate'),
                        dataset=coco_train_dataset)

train_dataloader = dict(persistent_workers=persistent_workers,
                        batch_size=train_batch_size_per_gpu,
                        collate_fn=dict(type='yolow_collate'),
                        dataset=coco_train_dataset)
test_pipeline = [
    *_base_.test_pipeline[:-1],
    dict(type='mmdet.PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                    'scale_factor', 'pad_param'))
]
coco_val_dataset = dict(type='YOLOv5CocoDataset',
                        data_root='data/coco128',
                        ann_file='annotations/val.json',
                        data_prefix=dict(img='val/images/'),
                        filter_cfg=dict(filter_empty_gt=False, min_size=32),
                        pipeline=test_pipeline)

val_dataloader = dict(dataset=coco_val_dataset)
test_dataloader = val_dataloader
# training settings
default_hooks = dict(param_scheduler=dict(scheduler_type='linear',
                                          lr_factor=0.01,
                                          max_epochs=max_epochs),
                     checkpoint=dict(max_keep_ckpts=-1,
                                     save_best='auto',  # ğŸ’¡ ä»Noneä¿®æ”¹ä¸º'auto'
                                     interval=save_epoch_intervals))
custom_hooks = [
    dict(type='EMAHook',
         ema_type='ExpMomentumEMA',
         momentum=0.0001,
         update_buffers=True,
         strict_load=False,
         priority=49),
    dict(type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - close_mosaic_epochs,
         switch_pipeline=train_pipeline_stage2)
]
train_cfg = dict(max_epochs=max_epochs,
                 val_interval=5,
                 dynamic_intervals=[((max_epochs - close_mosaic_epochs),
                                     _base_.val_interval_stage2)])
optim_wrapper = dict(optimizer=dict(
    _delete_=True,
    type='AdamW',
    lr=base_lr,
    weight_decay=weight_decay,
    batch_size_per_gpu=train_batch_size_per_gpu),
                     constructor='YOLOWv5OptimizerConstructor')

# evaluation settings
val_evaluator = dict(_delete_=True,
                     type='mmdet.CocoMetric',
                     proposal_nums=(100, 1, 10),
                     ann_file='data/coco128/annotations/val.json',
                     metric='bbox')
```

</details>

#### 4. å¼€å§‹è®­ç»ƒ

```bash
export CUDA_VISIBLE_DEVICES=1
bash tools/dist_train.sh \
    configs/finetune_coco/yolo_world_v2_s_rep_vlpan_bn_2e-4_80e_1gpus_finetune_coco128.py \
    1 \
    --amp
```

#### 5. è®­ç»ƒè¿‡ç¨‹å±•ç¤º

```
2024/06/20 06:22:44 - mmengine - INFO - Load checkpoint from pretrained_models/re-parameterized/yolo_world_v2_s_obj365v1_goldg_pretrain-55b943ea_rep_conv.pth
2024/06/20 06:22:44 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2024/06/20 06:22:44 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2024/06/20 06:22:44 - mmengine - INFO - Checkpoints will be saved to /home/Le0v1n/code/YOLO-World/work_dirs/yolo_world_v2_s_rep_vlpan_bn_2e-4_80e_1gpus_finetune_pedestrian30k.
2024/06/20 06:22:56 - mmengine - INFO - Epoch(train)  [1][  50/1509]  lr: 2.1648e-06  eta: 7:48:04  time: 0.2327  data_time: 0.0576  memory: 8696  grad_norm: nan  loss: 59.4169  loss_cls: 20.9526  loss_bbox: 18.8704  loss_dfl: 19.5938
2024/06/20 06:23:03 - mmengine - INFO - Epoch(train)  [1][ 100/1509]  lr: 4.3738e-06  eta: 6:04:49  time: 0.1302  data_time: 0.0266  memory: 4375  grad_norm: 496.5040  loss: 52.9109  loss_cls: 16.6957  loss_bbox: 17.5310  loss_dfl: 18.6842
2024/06/20 06:23:09 - mmengine - INFO - Epoch(train)  [1][ 150/1509]  lr: 6.5827e-06  eta: 5:31:30  time: 0.1320  data_time: 0.0365  memory: 4389  grad_norm: 477.9625  loss: 50.6395  loss_cls: 14.8803  loss_bbox: 17.0048  loss_dfl: 18.7545
2024/06/20 06:23:16 - mmengine - INFO - Epoch(train)  [1][ 200/1509]  lr: 8.7917e-06  eta: 5:15:02  time: 0.1324  data_time: 0.0474  memory: 4389  grad_norm: 475.2882  loss: 51.2433  loss_cls: 14.6030  loss_bbox: 17.6751  loss_dfl: 18.9652
2024/06/20 06:23:23 - mmengine - INFO - Epoch(train)  [1][ 250/1509]  lr: 1.1001e-05  eta: 5:05:49  time: 0.1342  data_time: 0.0332  memory: 4975  grad_norm: 417.9657  loss: 50.3242  loss_cls: 14.2822  loss_bbox: 17.4215  loss_dfl: 18.6204
2024/06/20 06:23:29 - mmengine - INFO - Epoch(train)  [1][ 300/1509]  lr: 1.3210e-05  eta: 4:58:14  time: 0.1300  data_time: 0.0280  memory: 4335  grad_norm: 440.3105  loss: 48.2986  loss_cls: 13.4765  loss_bbox: 16.5409  loss_dfl: 18.2812
2024/06/20 06:23:35 - mmengine - INFO - Epoch(train)  [1][ 350/1509]  lr: 1.5419e-05  eta: 4:51:48  time: 0.1265  data_time: 0.0245  memory: 4455  grad_norm: 415.5017  loss: 47.7501  loss_cls: 13.3075  loss_bbox: 16.3856  loss_dfl: 18.0570
2024/06/20 06:23:41 - mmengine - INFO - Epoch(train)  [1][ 400/1509]  lr: 1.7628e-05  eta: 4:45:45  time: 0.1218  data_time: 0.0200  memory: 4469  grad_norm: 459.6522  loss: 48.0474  loss_cls: 13.4490  loss_bbox: 16.4455  loss_dfl: 18.1529
```

#### 6. FAQ

æš‚æ— ã€‚

# 5. å‰ç½®çŸ¥è¯†

## 5.1 é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰<a id=6.1></a>

### 5.1.1 æ¦‚å¿µ

é›¶æ ·æœ¬å­¦ä¹ ï¼ˆzero-shot learningï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸä¸­çš„ä¸€ç§æŠ€æœ¯ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨æ²¡æœ‰æ¥å—è¿‡<font color='red'><b>ç‰¹å®šç±»åˆ«</b></font>è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«æˆ–é¢„æµ‹è¿™äº›ç±»åˆ«ã€‚è¿™é€šå¸¸é€šè¿‡åˆ©ç”¨æ¨¡å‹å¯¹å…¶ä»–ç±»åˆ«çš„å·²æœ‰çŸ¥è¯†æ¥å®ç°ï¼Œæˆ–è€…é€šè¿‡æŸç§å½¢å¼çš„è¯­ä¹‰æˆ–å±æ€§æè¿°æ¥è¾…åŠ©æ¨¡å‹ç†è§£æ–°çš„ç±»åˆ«ã€‚

> ğŸ’¡ æ¨èé˜…è¯»ã€Š[é›¶æ¬¡å­¦ä¹ ï¼ˆZero-Shot Learningï¼‰å…¥é—¨](https://zhuanlan.zhihu.com/p/34656727)ã€‹ï¼Œè¯¥æ–‡ç« è®²å¾—éå¸¸å¥½ã€‚

### 5.1.2 FAQ

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šzero-shotå’Œæµ‹è¯•é›†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œä¸éƒ½æ˜¯æ¨¡å‹æ²¡æœ‰è§è¿‡çš„å—ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šç¡®å®ï¼Œzero-shot learning å’Œæµ‹è¯•é›†éƒ½æ¶‰åŠåˆ°æ¨¡å‹åœ¨é¢å¯¹æœªè§è¿‡çš„æ•°æ®æ—¶çš„è¡¨ç°ï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨ä¸€äº›å…³é”®çš„åŒºåˆ«ï¼š

- æµ‹è¯•é›†ï¼šæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰è§åˆ°è¿‡è¯¥å›¾ç‰‡ï¼Œè®©æ¨¡å‹å»é¢„æµ‹ï¼Œæ¨¡å‹ä¼šé¢„æµ‹å‡ºå®ƒ<font color='red'><b>è§è¿‡çš„ç±»åˆ«</b></font>ï¼ˆæ³¨æ„è¿™é‡Œæ˜¯ç±»åˆ«ï¼‰ã€‚
- zero-shotï¼šæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰è§åˆ°è¿‡è¯¥ç±»åˆ«ï¼Œè®©æ¨¡å‹å»é¢„æµ‹ï¼Œæ¨¡å‹ä¸å…‰ä¼šé¢„æµ‹å‡ºå®ƒè§è¿‡çš„ç±»åˆ«ï¼Œä¹Ÿä¼š<font color='red'><b>é¢„æµ‹å‡ºå®ƒæ²¡æœ‰è§è¿‡çš„ç±»åˆ«</b></font>ã€‚

---

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šzero-shotã€one-shotã€few-shotæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼š

|           | ä¸­æ–‡ç¿»è¯‘ | ç‰¹ç‚¹                                                                                                                 |
| :-------: | :------: | :------------------------------------------------------------------------------------------------------------------- |
| zero-shot |  é›¶æ ·æœ¬  | æ¨¡å‹ä»æ¥æ²¡æœ‰å­¦ä¹ è¿‡è¿™ä¸ªç±»åˆ«çš„å›¾ç‰‡ï¼Œä½†ä»ç„¶å¯ä»¥è¯†åˆ«å‡ºè¿™ä¸ªç±»åˆ«                                                           |
| one-shot  |  å•æ ·æœ¬  | æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªå­¦ä¹ è¿‡<font color='red'><b>ä¸€å¼ </b></font>æœ‰è¯¥ç±»åˆ«çš„å›¾ç‰‡ï¼Œåœ¨åç»­çš„ä½¿ç”¨ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ­£ç¡®æ¨ç†å‡ºè¯¥ç±»åˆ« |
| few-shot  |  å°‘æ ·æœ¬  | æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªå­¦ä¹ è¿‡<font color='red'><b>å°‘é‡</b></font>æœ‰è¯¥ç±»åˆ«çš„å›¾ç‰‡ï¼Œåœ¨åç»­çš„ä½¿ç”¨ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ­£ç¡®æ¨ç†å‡ºè¯¥ç±»åˆ« |

- ä¸‰è€…çš„ç®€å•ç¤ºä¾‹ï¼š
  - **zero-shot**ï¼šå¦‚æœä¸€ä¸ªæ¨¡å‹åœ¨è®­ç»ƒæ—¶å­¦ä¼šäº†è¯†åˆ«é©¬ã€ç†ŠçŒ«å’Œé¸Ÿï¼Œå®ƒå¯ä»¥åœ¨æ²¡æœ‰è§è¿‡çš„ç±»åˆ«ï¼ˆå¦‚æ–‘é©¬ï¼‰ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå› ä¸ºå®ƒäº†è§£åˆ°æ–‘é©¬æ˜¯ä¸€ç§åŠ¨ç‰©ï¼Œæ‹¥æœ‰å’Œé©¬ä¸€æ ·çš„ä½“å‹ï¼Œæœ‰ç±»ä¼¼ç†ŠçŒ«çš„é»‘ç™½è‰²æ¯›ã€‚zero-shotèƒ½å¤Ÿé€šè¿‡å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„å…³ç³»å’Œå…±äº«çš„ç‰¹å¾æ¥å®ç°å¯¹æœªçŸ¥ä»»åŠ¡çš„å¤„ç†ã€‚
  - **one-shot**ï¼šå¦‚æœä¸€ä¸ªæ¨¡å‹åªè§è¿‡ä¸€å¼ çŒ«çš„å›¾ç‰‡ï¼Œå®ƒå¯ä»¥é€šè¿‡è¿™å¼ å›¾ç‰‡è¿›è¡Œå­¦ä¹ ï¼Œå¹¶åœ¨ä¹‹åå¯¹æ–°çš„çŒ«çš„å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚one-shotæ˜¯åœ¨éå¸¸æœ‰é™çš„æ•°æ®æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ å’Œæ¨æ–­çš„ä¸€ç§èƒ½åŠ›ã€‚
  - **few-shot**ï¼šå¦‚æœä¸€ä¸ªæ¨¡å‹ä»…ä»…é€šè¿‡è§‚å¯Ÿå‡ ä¸ªå›¾åƒï¼Œå°±å¯ä»¥å­¦ä¼šè¯†åˆ«ä¸åŒå“ç§çš„ç‹—ï¼Œç„¶åèƒ½å¤Ÿå¯¹æ–°çš„ç‹—å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚few-shotè¦æ±‚æ¨¡å‹èƒ½å¤Ÿä»å°‘é‡ç¤ºä¾‹ä¸­æŠ½å–å‡ºå…³é”®ç‰¹å¾å’Œæ¨¡å¼ï¼Œä»¥ä¾¿è¿›è¡Œå‡†ç¡®çš„é¢„æµ‹ã€‚

## 5.2 CLIPï¼ˆContrastive Language-Image pre-trainingï¼‰<a id=6.2></a>

### 5.2.1 ä»‹ç»

CLIPï¼ˆContrastive Languageâ€“Image Pre-trainingï¼‰æ˜¯ä¸€ç§å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹ï¼Œç”±OpenAIåœ¨2021å¹´æå‡ºã€‚å®ƒçš„ä½œç”¨ä¸»è¦åŒ…æ‹¬ï¼š

1. **å›¾åƒå’Œæ–‡æœ¬çš„è”åˆè¡¨ç¤ºå­¦ä¹ **ï¼šCLIPé€šè¿‡åœ¨å¤§é‡å›¾åƒå’Œæ–‡æœ¬å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ <font color='blue'><b>å°†å›¾åƒå†…å®¹ä¸ç›¸åº”çš„æè¿°æ–‡æœ¬æ˜ å°„åˆ°ä¸€ä¸ªå…±åŒçš„ç‰¹å¾ç©ºé—´ä¸­</b></font>ã€‚
2. **é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰åˆ†ç±»**ï¼šCLIPèƒ½å¤Ÿåœ¨æ²¡æœ‰ä¼ ç»Ÿè®­ç»ƒè¿‡ç¨‹çš„æƒ…å†µä¸‹ï¼Œå¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚<font color='green'><b>åªéœ€è¦æä¾›ç±»åˆ«çš„æ–‡æœ¬æè¿°ï¼ŒCLIPå°±å¯ä»¥è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ï¼Œå³ä½¿å®ƒä¹‹å‰æ²¡æœ‰è§è¿‡è¿™äº›å…·ä½“ç±»åˆ«</b></font>ã€‚
3. **å›¾åƒæ£€ç´¢**ï¼šåˆ©ç”¨CLIPæ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æ–‡æœ¬æŸ¥è¯¢æ¥æ£€ç´¢ä¸æ–‡æœ¬æè¿°æœ€åŒ¹é…çš„å›¾åƒã€‚
4. **æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆ**ï¼šè™½ç„¶CLIPæœ¬èº«ä¸æ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œä½†å®ƒçš„ç¼–ç å™¨ï¼ˆEncoderï¼‰å¯ä»¥ä¸ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANsï¼‰ç»“åˆï¼Œç”¨äºç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸åŒ¹é…çš„å›¾åƒã€‚
5. **è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ **ï¼šCLIPé€šè¿‡æœ€å°åŒ–æ­£æ ·æœ¬å¯¹ï¼ˆpairï¼‰ä¹‹é—´çš„è·ç¦»å¹¶æœ€å¤§åŒ–è´Ÿæ ·æœ¬å¯¹ï¼ˆpairï¼‰ä¹‹é—´çš„è·ç¦»æ¥è¿›è¡Œè®­ç»ƒï¼Œä»è€Œ<font color='pink'><b>å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§</b></font>ã€‚
6. **å¤šè¯­è¨€æ”¯æŒ**ï¼šCLIPæ”¯æŒå¤šç§è¯­è¨€çš„æ–‡æœ¬è¾“å…¥ï¼Œè¿™ä½¿å¾—å®ƒèƒ½å¤Ÿå¤„ç†ä¸åŒè¯­è¨€çš„å›¾åƒæè¿°ã€‚

> CLIPæ¨¡å‹å› å…¶å¼ºå¤§çš„é€šç”¨æ€§å’Œçµæ´»æ€§ï¼Œåœ¨å›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¢«å¹¿æ³›ç ”ç©¶å’Œåº”ç”¨ã€‚

### 5.2.2 è¾“å…¥è¾“å‡º

CLIPæ¨¡å‹çš„è¾“å…¥å¦‚ä¸‹ï¼š

1. **å›¾åƒè¾“å…¥**ï¼šCLIPæ¨¡å‹æ¥å—å›¾åƒä½œä¸ºè¾“å…¥ã€‚è¿™äº›å›¾åƒå¯ä»¥æ˜¯JPEGã€PNGç­‰æ ¼å¼ï¼Œå®ƒä»¬é¦–å…ˆè¢«æ¨¡å‹é¢„å¤„ç†ï¼ŒåŒ…æ‹¬è°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–ç­‰æ­¥éª¤ï¼Œä»¥é€‚åº”æ¨¡å‹çš„è¾“å…¥è¦æ±‚ã€‚
2. **æ–‡æœ¬è¾“å…¥**ï¼šCLIPæ¨¡å‹åŒæ—¶æ¥å—æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚æ–‡æœ¬å¯ä»¥æ˜¯ç±»åˆ«åç§°ã€ç‰©ä½“æè¿°ã€åœºæ™¯æè¿°ç­‰è‡ªç„¶è¯­è¨€æè¿°ã€‚æ–‡æœ¬è¾“å…¥é€šå¸¸ç»è¿‡åˆ†è¯å¤„ç†ï¼Œå¹¶è½¬æ¢ä¸ºæ¨¡å‹èƒ½ç†è§£çš„åµŒå…¥è¡¨ç¤ºã€‚

CLIPæ¨¡å‹çš„è¾“å‡ºå¦‚ä¸‹ï¼š

1. **å›¾åƒ-æ–‡æœ¬åµŒå…¥**ï¼šCLIPæ¨¡å‹çš„ç¼–ç å™¨ä¼šå°†è¾“å…¥çš„å›¾åƒå’Œæ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„åµŒå…¥å‘é‡ã€‚è¿™äº›åµŒå…¥å‘é‡æ•æ‰äº†å›¾åƒå†…å®¹å’Œæ–‡æœ¬æè¿°çš„è¯­ä¹‰ä¿¡æ¯ã€‚
2. **ç›¸ä¼¼åº¦åˆ†æ•°**ï¼š<font color='red'><b>å¯¹äºç»™å®šçš„å›¾åƒå’Œæ–‡æœ¬å¯¹ï¼ŒCLIPæ¨¡å‹è¾“å‡ºä¸€ä¸ªç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¯¥åˆ†æ•°è¡¨ç¤ºå›¾åƒä¸æ–‡æœ¬æè¿°çš„åŒ¹é…ç¨‹åº¦</b></font>ã€‚åˆ†æ•°è¶Šé«˜ï¼Œè¡¨ç¤ºæ¨¡å‹è®¤ä¸ºå›¾åƒå’Œæ–‡æœ¬è¶Šç›¸å…³ã€‚
3. **åˆ†ç±»ç»“æœ**ï¼ˆé›¶æ ·æœ¬åˆ†ç±»ï¼‰ï¼šå¦‚æœæ–‡æœ¬è¾“å…¥æ˜¯ç±»åˆ«æè¿°ï¼ŒCLIPå¯ä»¥è¾“å‡ºå›¾åƒå±äºå„ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œå®ç°é›¶æ ·æœ¬åˆ†ç±»ã€‚
4. **æ£€ç´¢ç»“æœ**ï¼šåœ¨å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒCLIPå¯ä»¥ä¸ºç»™å®šçš„æ–‡æœ¬æŸ¥è¯¢è¿”å›æœ€ç›¸å…³çš„å›¾åƒé›†åˆã€‚ã€
5. **ç‰¹å¾å›¾**ï¼ˆé«˜çº§åº”ç”¨ï¼‰ï¼šåœ¨æŸäº›åº”ç”¨ä¸­ï¼ŒCLIPçš„ä¸­é—´å±‚å¯ä»¥æä¾›å›¾åƒçš„ç‰¹å¾å›¾ï¼Œè¿™äº›ç‰¹å¾å›¾å¯ä»¥ç”¨äºæ›´å¤æ‚çš„è§†è§‰åˆ†æä»»åŠ¡ã€‚

> CLIPæ¨¡å‹çš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤šç§æ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­æä¾›æœ‰ç”¨çš„è¾“å‡ºï¼Œè¿™ä½¿å¾—å®ƒåœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸéå¸¸å—æ¬¢è¿ã€‚

### 5.2.3 FAQ

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šCLIPå¯ä»¥åªè¾“å…¥ä¸€ä¸ªæ–‡æœ¬æˆ–ä¸€å¼ å›¾ç‰‡å—ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šCLIPæ¨¡å‹è®¾è®¡æ—¶ä¸»è¦æ˜¯ä¸ºäº†å¤„ç†æˆå¯¹ï¼ˆpairï¼‰çš„è¾“å…¥ï¼Œå³å›¾åƒå’Œæ–‡æœ¬çš„ç»„åˆï¼Œä»¥ä¾¿å­¦ä¹ ä¸¤è€…ä¹‹é—´çš„å…³è”ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦ç»„ä»¶â€”â€”å›¾åƒç¼–ç å™¨ï¼ˆImage Encoderï¼‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼ˆText Encoderï¼‰éƒ½æ˜¯å¯ä»¥ç‹¬ç«‹ä½¿ç”¨çš„ã€‚

1. **æ–‡æœ¬ç¼–ç å™¨ï¼ˆText Encoderï¼‰**ï¼šå¯ä»¥å•ç‹¬æ¥æ”¶æ–‡æœ¬è¾“å…¥ï¼Œå¹¶å°†å…¶ç¼–ç æˆåµŒå…¥å‘é‡ï¼ˆembedding vectorï¼‰ã€‚è¿™ä¸ªå‘é‡å¯ä»¥ä»£è¡¨æ–‡æœ¬çš„è¯­ä¹‰å†…å®¹ï¼Œç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ¯”å¦‚æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ã€æ–‡æœ¬èšç±»ç­‰ã€‚

2. **å›¾åƒç¼–ç å™¨ï¼ˆImage Encoderï¼‰**ï¼šåŒæ ·å¯ä»¥ç‹¬ç«‹æ¥æ”¶å›¾åƒè¾“å…¥ï¼Œå¹¶å°†å…¶ç¼–ç æˆåµŒå…¥å‘é‡ï¼ˆembedding vectorï¼‰ã€‚è¿™äº›å‘é‡å¯ä»¥ç”¨äºå›¾åƒæ£€ç´¢ã€å›¾åƒç›¸ä¼¼åº¦è®¡ç®—ç­‰ä»»åŠ¡ã€‚

å°½ç®¡CLIPçš„å®Œæ•´æ¨¡å‹æ˜¯ä¸ºå›¾åƒ-æ–‡æœ¬å¯¹è®¾è®¡çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬åªéœ€è¦å¤„ç†æ–‡æœ¬æˆ–è€…å›¾åƒä¸­çš„ä¸€ä¸ªï¼Œå¯ä»¥åªä½¿ç”¨ç›¸åº”çš„ç¼–ç å™¨éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ–‡æœ¬ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“å®ƒä¸å…¶ä»–æ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨æ¥è·å–åµŒå…¥å‘é‡ï¼Œç„¶åæ¯”è¾ƒè¿™äº›å‘é‡ã€‚

ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦CLIPæ¨¡å‹è¾“å‡ºå›¾åƒä¸æ–‡æœ¬çš„åŒ¹é…åˆ†æ•°ï¼Œé‚£ä¹ˆå°±éœ€è¦åŒæ—¶æä¾›å›¾åƒå’Œæ–‡æœ¬çš„è¾“å…¥ã€‚å¦‚æœåªæœ‰æ–‡æœ¬è€Œæ²¡æœ‰å›¾åƒï¼Œå°±æ— æ³•ä½¿ç”¨CLIPæ¨¡å‹æ¥è¯„ä¼°å›¾åƒä¸æ–‡æœ¬çš„ä¸€è‡´æ€§æˆ–è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ã€‚

## 5.3 å¼€é›†ç›®æ ‡æ£€æµ‹<a id=6.3></a>

> æ¨èé˜…è¯»æ–‡ç« ã€Š[Grounding DINOæ£€æµ‹ä¸€åˆ‡](https://zhuanlan.zhihu.com/p/664623532)ã€‹ã€‚

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šzero-shotå’Œå¼€é›†ç›®æ ‡æ£€æµ‹æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šZero-shot learningï¼ˆé›¶æ ·æœ¬å­¦ä¹ ï¼‰å’Œå¼€é›†ç›®æ ‡æ£€æµ‹ï¼ˆOpen-set Object Detectionï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸä¸­çš„ä¸¤ç§ä¸åŒçš„æ¦‚å¿µï¼Œä½†å®ƒä»¬åœ¨æŸäº›æ–¹é¢å­˜åœ¨è”ç³»ï¼š

1. **Zero-shot Learning**ï¼š
   - é›¶æ ·æœ¬å­¦ä¹ çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ç›´æ¥è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¯†åˆ«æ–°çš„ç±»åˆ«ã€‚è¿™æ„å‘³ç€æ¨¡å‹éœ€è¦åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°è¶³å¤Ÿçš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥ä¾¿åœ¨é¢å¯¹æœªçŸ¥ç±»åˆ«æ—¶åšå‡ºæ­£ç¡®çš„é¢„æµ‹ã€‚
   - é›¶æ ·æœ¬å­¦ä¹ <font color='red'><b>é€šå¸¸ä¾èµ–äºè¾…åŠ©ä¿¡æ¯ï¼Œå¦‚ç±»åˆ«çš„æè¿°ã€å±æ€§æˆ–ç›¸ä¼¼ç±»åˆ«çš„è¯­ä¹‰ä¿¡æ¯</b></font>ã€‚
2. **å¼€é›†ç›®æ ‡æ£€æµ‹ï¼ˆOpen-set Object Detectionï¼‰**ï¼š
   - å¼€é›†ç›®æ ‡æ£€æµ‹æ˜¯æŒ‡æ¨¡å‹åœ¨æ£€æµ‹è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°è®­ç»ƒé˜¶æ®µæœªè§è¿‡çš„æ–°ç±»åˆ«ã€‚ä¸é›¶æ ·æœ¬å­¦ä¹ ä¸åŒï¼Œå¼€é›†ç›®æ ‡æ£€æµ‹å¹¶ä¸è¦æ±‚æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«è¿™äº›æ–°ç±»åˆ«ï¼Œè€Œæ˜¯è¦æ±‚æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å‡ºè¿™äº›æœªçŸ¥ç±»åˆ«çš„å­˜åœ¨ï¼Œå³ä½¿å®ƒä¸èƒ½å‡†ç¡®åœ°åˆ†ç±»å®ƒä»¬ã€‚
   - å¼€é›†ç›®æ ‡æ£€æµ‹çš„æŒ‘æˆ˜åœ¨äºåŒºåˆ†å·²çŸ¥ç±»åˆ«å’ŒæœªçŸ¥ç±»åˆ«ï¼Œå¹¶ä¸”å¯¹æœªçŸ¥ç±»åˆ«åšå‡ºé€‚å½“çš„å“åº”ã€‚

**ä¸¤è€…çš„å…³ç³»**ï¼š
- **æ³›åŒ–èƒ½åŠ›**ï¼šé›¶æ ·æœ¬å­¦ä¹ å’Œå¼€é›†ç›®æ ‡æ£€æµ‹éƒ½è¦æ±‚æ¨¡å‹å…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚é›¶æ ·æœ¬å­¦ä¹ ä¾§é‡äºåˆ©ç”¨æ³›åŒ–èƒ½åŠ›è¯†åˆ«æ–°ç±»åˆ«ï¼Œè€Œå¼€é›†ç›®æ ‡æ£€æµ‹ä¾§é‡äºåˆ©ç”¨æ³›åŒ–èƒ½åŠ›åŒºåˆ†å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«ã€‚
- **æœªçŸ¥ç±»åˆ«**ï¼šåœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­ï¼Œæ¨¡å‹éœ€è¦è¯†åˆ«å®Œå…¨æœªçŸ¥çš„ç±»åˆ«ï¼›è€Œåœ¨å¼€é›†ç›®æ ‡æ£€æµ‹ä¸­ï¼Œæ¨¡å‹éœ€è¦è¯†åˆ«å¹¶åŒºåˆ†å‡ºæœªçŸ¥ç±»åˆ«ï¼Œä½†ä¸ä¸€å®šéœ€è¦å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç±»ã€‚
- **è¾…åŠ©ä¿¡æ¯çš„ä½¿ç”¨**ï¼šé›¶æ ·æœ¬å­¦ä¹ é€šå¸¸ä¾èµ–äºè¾…åŠ©ä¿¡æ¯ï¼Œå¦‚ç±»åˆ«æè¿°ï¼Œæ¥å¸®åŠ©è¯†åˆ«æ–°ç±»åˆ«ã€‚å¼€é›†ç›®æ ‡æ£€æµ‹å¯èƒ½ä¸ä¾èµ–äºè¿™äº›è¾…åŠ©ä¿¡æ¯ï¼Œè€Œæ˜¯ä¾èµ–äºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹æ•°æ®åˆ†å¸ƒçš„ç†è§£ã€‚
- **åº”ç”¨åœºæ™¯**ï¼šé›¶æ ·æœ¬å­¦ä¹ å’Œå¼€é›†ç›®æ ‡æ£€æµ‹éƒ½å¯ä»¥åº”ç”¨äºç°å®ä¸–ç•Œä¸­ï¼Œå…¶ä¸­æ¨¡å‹å¯èƒ½é‡åˆ°æœªåœ¨è®­ç»ƒæ•°æ®ä¸­è§è¿‡çš„æ–°å¯¹è±¡æˆ–ç±»åˆ«ã€‚
- **æŒ‘æˆ˜å’Œé™åˆ¶**ï¼šä¸¤è€…éƒ½é¢ä¸´ç€æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¦‚ä½•å¤„ç†å’Œè¯†åˆ«æœªçŸ¥ç±»åˆ«çš„é™åˆ¶ã€‚

æ€»çš„æ¥è¯´ï¼Œé›¶æ ·æœ¬å­¦ä¹ å’Œå¼€é›†ç›®æ ‡æ£€æµ‹éƒ½æ¶‰åŠåˆ°æ¨¡å‹å¯¹æœªçŸ¥ç±»åˆ«çš„å¤„ç†ï¼Œä½†å®ƒä»¬çš„ç„¦ç‚¹å’Œç›®æ ‡ç•¥æœ‰ä¸åŒã€‚é›¶æ ·æœ¬å­¦ä¹ æ›´ä¾§é‡äºè¯†åˆ«æ–°ç±»åˆ«ï¼Œè€Œå¼€é›†ç›®æ ‡æ£€æµ‹æ›´ä¾§é‡äºåŒºåˆ†å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¸¤è€…åœ¨æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œå¤„ç†æœªçŸ¥ç±»åˆ«æ–¹é¢å­˜åœ¨ä¸€å®šçš„è”ç³»ã€‚

## 5.4 LVISæ•°æ®é›†<a id=6.4></a>

LVISï¼ˆLarge Vocabulary Instance Segmentationï¼‰æ•°æ®é›†æ˜¯ç”±Facebook AI Research (FAIR)å¼€å‘å¹¶å‘å¸ƒçš„ä¸€ä¸ªå¤§è§„æ¨¡ç»†ç²’åº¦è¯æ±‡çº§æ ‡è®°æ•°æ®é›†ã€‚è¿™ä¸ªæ•°æ®é›†ä¸“é—¨ç”¨äºå¯¹è±¡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²çš„ç ”ç©¶åŸºå‡†ï¼Œå®ƒåŒ…å«äº†è¶…è¿‡1000ç±»ç‰©ä½“çš„çº¦200ä¸‡ä¸ªé«˜è´¨é‡çš„å®ä¾‹åˆ†å‰²æ ‡æ³¨ï¼Œæ¶µç›–äº†164kå¤§å°çš„å›¾åƒã€‚

**LVISæ•°æ®é›†çš„ç‰¹ç‚¹åŒ…æ‹¬**ï¼š
1. **å¤§è§„æ¨¡å’Œç»†ç²’åº¦**ï¼šæ•°æ®é›†è¦†ç›–äº†å¹¿æ³›çš„ç‰©ä½“ç±»åˆ«ï¼Œæä¾›äº†è¯¦å°½çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬å°çš„ã€éƒ¨åˆ†è¢«é®æŒ¡çš„æˆ–éš¾ä»¥è¾¨è®¤çš„å¯¹è±¡å®ä¾‹ã€‚
2. **é«˜è´¨é‡æ ‡æ³¨**ï¼šä¸COCOå’Œ ADE20Kæ•°æ®é›†ç›¸æ¯”ï¼ŒLVISæ•°æ®é›†çš„æ ‡æ³¨è´¨é‡æ›´é«˜ï¼Œå…·æœ‰æ›´å¤§çš„é‡å é¢ç§¯å’Œæ›´å¥½çš„è¾¹ç•Œè¿ç»­æ€§ã€‚
3. **é•¿å°¾åˆ†å¸ƒ**ï¼šLVISæ•°æ®é›†åæ˜ äº†è‡ªç„¶å›¾åƒä¸­ç±»åˆ«çš„Zipfianåˆ†å¸ƒï¼Œå³<font color='red'><b>å°‘æ•°å¸¸è§ç±»åˆ«å’Œå¤§é‡ç½•è§ç±»åˆ«çš„é•¿å°¾åˆ†å¸ƒ</b></font>ã€‚
4. **è¯„ä¼°ä¼˜å…ˆçš„è®¾è®¡åŸåˆ™**ï¼šæ•°æ®é›†çš„æ„å»ºé‡‡ç”¨äº†è¯„ä¼°ä¼˜å…ˆçš„è®¾è®¡åŸåˆ™ï¼Œå³é¦–å…ˆç¡®å®šå¦‚ä½•æ‰§è¡Œå®šé‡è¯„ä¼°ï¼Œç„¶åè®¾è®¡å’Œæ„å»ºæ•°æ®é›†æ”¶é›†æµç¨‹ä»¥æ»¡è¶³è¯„ä¼°æ‰€éœ€æ•°æ®çš„éœ€æ±‚ã€‚
5. **è”åˆæ•°æ®é›†**ï¼šLVISç”±å¤§é‡è¾ƒå°çš„ç»„æˆæ•°æ®é›†è”åˆå½¢æˆï¼Œæ¯ä¸ªå°æ•°æ®é›†ä¸ºå•ä¸ªç±»åˆ«æä¾›è¯¦å°½æ ‡æ³¨çš„åŸºæœ¬ä¿è¯ï¼Œå³è¯¥ç±»åˆ«çš„æ‰€æœ‰å®ä¾‹éƒ½è¢«æ ‡æ³¨ã€‚è¿™ç§è®¾è®¡å‡å°‘äº†æ•´ä½“çš„æ ‡æ³¨å·¥ä½œé‡ï¼ŒåŒæ—¶ä¿æŒäº†è¯„ä¼°çš„å…¬å¹³æ€§ã€‚

LVISæ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹åŒ…æ‹¬å…­ä¸ªé˜¶æ®µï¼šç›®æ ‡å®šä½ã€ç©·å°½æ ‡è®°ã€å®ä¾‹åˆ†å‰²ã€éªŒè¯ã€ç©·å°½æ ‡æ³¨éªŒè¯ä»¥åŠè´Ÿä¾‹é›†æ ‡æ³¨ã€‚æ•°æ®é›†çš„è¯æ±‡è¡¨ Væ˜¯é€šè¿‡è¿­ä»£è¿‡ç¨‹æ„å»ºçš„ï¼Œä»å¤§å‹è¶…çº§è¯æ±‡è¡¨å¼€å§‹ï¼Œå¹¶ä½¿ç”¨ç›®æ ‡å®šä½è¿‡ç¨‹é€æ­¥ç¼©å°ï¼Œ<font color='red'><b>æœ€ç»ˆç¡®å®šåŒ…å« 1723ä¸ªåŒä¹‰è¯çš„è¯æ±‡è¡¨</b></font>ï¼Œè¿™ä¹Ÿæ˜¯å¯ä»¥å‡ºç°åœ¨ LVISä¸­çš„ç±»åˆ«æ•°é‡çš„ä¸Šé™ã€‚

### 5.4.1 ğŸ”¥ COCOæ•°æ®é›†

#### 1. ä»‹ç»

COCO2017æ•°æ®é›†ï¼Œå…¨ç§°ä¸ºCommon Objects in Context 2017ï¼Œæ˜¯ä¸€ä¸ªå¤§å‹çš„ã€ä¸°å¯Œä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹è±¡æ£€æµ‹ã€åˆ†å‰²å’Œå­—å¹•ç”Ÿæˆæ•°æ®é›†ã€‚å®ƒæ˜¯Common Objects in Contextï¼ˆCOCOï¼‰æ•°æ®é›†ç³»åˆ—ä¸­çš„ä¸€ä¸ªç‰ˆæœ¬ï¼Œç”±å¾®è½¯å’Œå“ˆä½›çš„ç ”ç©¶äººå‘˜åˆ›å»ºï¼Œå¹¶åœ¨2017å¹´å‘å¸ƒã€‚

ä»¥ä¸‹æ˜¯COCO2017æ•°æ®é›†çš„ä¸€äº›å…³é”®ç‰¹ç‚¹ï¼š

1. **å¤šä»»åŠ¡æ•°æ®é›†**ï¼šCOCO2017ä¸ä»…åŒ…å«å¯¹è±¡æ£€æµ‹ä»»åŠ¡ï¼ˆdetectï¼‰ï¼Œè¿˜åŒ…æ‹¬å¯¹è±¡åˆ†å‰²ï¼ˆSegmentï¼‰å’Œå­—å¹•ç”Ÿæˆä»»åŠ¡ï¼ˆcaptionï¼‰ã€‚
2. **å¤§è§„æ¨¡**ï¼šæ•°æ®é›†åŒ…å«è¶…è¿‡330,000å¼ æ ‡è®°å›¾åƒï¼Œæ¶µç›–äº†80ä¸ªç±»åˆ«çš„å¯¹è±¡ï¼ŒåŒ…æ‹¬æ—¥å¸¸ç‰©å“ã€åŠ¨ç‰©ã€è½¦è¾†ç­‰ã€‚
3. **é«˜è´¨é‡æ ‡æ³¨**ï¼šå›¾åƒä¸­çš„æ¯ä¸ªå¯¹è±¡éƒ½æœ‰è¯¦ç»†çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬<font color='red'><b>è¾¹ç•Œæ¡†ã€åˆ†å‰²æ©ç å’Œ/æˆ–å­—å¹•</b></font>ã€‚
4. **ç±»åˆ«ä¸°å¯Œ**ï¼šæ•°æ®é›†ä¸­çš„å¯¹è±¡ç±»åˆ«éå¸¸ä¸°å¯Œï¼ŒåŒ…æ‹¬äººã€è½¦è¾†ã€åŠ¨ç‰©ã€å®¶å…·ã€ç”µå­äº§å“ç­‰ã€‚
5. **ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šCOCOæ•°æ®é›†çš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯å¼ºè°ƒå¯¹è±¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå³å¯¹è±¡ä¸å…¶å‘¨å›´ç¯å¢ƒçš„å…³ç³»ã€‚
6. **æŒ‘æˆ˜æ€§**ï¼šç”±äºå›¾åƒä¸­å¯¹è±¡çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼ŒCOCOæ•°æ®é›†å¯¹è®¡ç®—æœºè§†è§‰ç®—æ³•æå‡ºäº†å¾ˆé«˜çš„æŒ‘æˆ˜ã€‚
7. **å¹¿æ³›ä½¿ç”¨**ï¼šCOCOæ•°æ®é›†åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¢«å¹¿æ³›ä½¿ç”¨ï¼Œæ˜¯è®¸å¤šç®—æ³•åŸºå‡†æµ‹è¯•çš„æ ‡å‡†æ•°æ®é›†ã€‚
8. **å¹´åº¦ç«èµ›**ï¼šCOCOæ•°æ®é›†è¿˜ä¸å¹´åº¦çš„COCOç«èµ›ç›¸å…³è”ï¼Œè¯¥ç«èµ›å¸å¼•äº†å…¨çƒçš„ç ”ç©¶å›¢é˜Ÿå‚ä¸ï¼Œæ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„å‘å±•ã€‚

COCO2017æ•°æ®é›†é€šå¸¸åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šè®­ç»ƒé›†ï¼ˆTraining setï¼‰ã€éªŒè¯é›†ï¼ˆValidation setï¼‰å’Œæµ‹è¯•é›†ï¼ˆTest setï¼‰ã€‚è®­ç»ƒé›†ç”¨äºæ¨¡å‹çš„è®­ç»ƒï¼ŒéªŒè¯é›†ç”¨äºæ¨¡å‹çš„è°ƒä¼˜å’ŒéªŒè¯ï¼Œè€Œæµ‹è¯•é›†åˆ™ç”¨äºæœ€ç»ˆè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ•°æ®é›†çš„ç»„ç»‡ç»“æ„å’Œè¯¦ç»†çš„æ ‡æ³¨ä¿¡æ¯ä½¿å…¶æˆä¸ºç ”ç©¶å’Œå¼€å‘å…ˆè¿›è§†è§‰ç®—æ³•çš„é‡è¦èµ„æºã€‚

#### 2. ç›®å½•ç»“æ„

COCO2017æ•°æ®é›†çš„ç›®å½•ç»“æ„ç»„ç»‡å¾—éå¸¸æ¸…æ™°ï¼Œä¾¿äºç®¡ç†å’Œä½¿ç”¨æ•°æ®ã€‚ä»¥ä¸‹æ˜¯ç›®å½•ç»“æ„ï¼š

```
coco2017
â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ captions_train2017.json
â”‚   â”œâ”€â”€ captions_val2017.json
â”‚   â”œâ”€â”€ instances_train2017.json
â”‚   â”œâ”€â”€ instances_val2017.json
â”‚   â”œâ”€â”€ person_keypoints_train2017.json
â”‚   â””â”€â”€ person_keypoints_val2017.json
â”œâ”€â”€ test2017
â”‚   â”œâ”€â”€ 000000000001.jpg
â”‚   â”œâ”€â”€ 000000000016.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train2017
â”‚   â”œâ”€â”€ 000000000009.jpg
â”‚   â”œâ”€â”€ 000000000025.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ val2017
    â”œâ”€â”€ 000000000139.jpg
    â”œâ”€â”€ 000000000285.jpg
    â””â”€â”€ ...
```


- `coco2017`ï¼šè¿™æ˜¯æ•°æ®é›†çš„æ ¹ç›®å½•ï¼ŒåŒ…å«äº†æ‰€æœ‰ç›¸å…³çš„å­ç›®å½•å’Œæ–‡ä»¶ã€‚
  - `annotations`ï¼šè¿™ä¸ªç›®å½•åŒ…å«äº†æ‰€æœ‰ä¸æ³¨é‡Šç›¸å…³çš„JSONæ–‡ä»¶ï¼Œç”¨äºå­˜å‚¨å›¾åƒä¸­å¯¹è±¡çš„æ ‡æ³¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­—å¹•ã€å®ä¾‹åˆ†å‰²å’Œäººä½“å…³é”®ç‚¹ã€‚
    - `captions_train2017.json`å’Œ`captions_val2017.json`ï¼šè¿™ä¸¤ä¸ªæ–‡ä»¶åŒ…å«äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†å›¾åƒçš„å­—å¹•ä¿¡æ¯ã€‚
    - `instances_train2017.json`å’Œ`instances_val2017.json`ï¼šè¿™ä¸¤ä¸ªæ–‡ä»¶åŒ…å«äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†å›¾åƒä¸­å¯¹è±¡çš„å®ä¾‹åˆ†å‰²ä¿¡æ¯ï¼Œå³æ¯ä¸ªå¯¹è±¡çš„ç²¾ç¡®åƒç´ çº§æ©ç ã€‚
    - `person_keypoints_train2017.json`å’Œ`person_keypoints_val2017.json`ï¼šè¿™ä¸¤ä¸ªæ–‡ä»¶ä¸“é—¨åŒ…å«äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­äººä½“å›¾åƒçš„å…³é”®ç‚¹æ ‡æ³¨ä¿¡æ¯ã€‚
  - `test2017`ï¼šè¿™ä¸ªç›®å½•åŒ…å«äº†æµ‹è¯•é›†çš„å›¾åƒæ–‡ä»¶ã€‚æµ‹è¯•é›†çš„å›¾åƒç”¨äºç®—æ³•çš„æœ€ç»ˆè¯„ä¼°ï¼Œé€šå¸¸ä¸åŒ…å«æ ‡æ³¨ä¿¡æ¯ï¼Œæˆ–è€…æ ‡æ³¨ä¿¡æ¯æ˜¯éšè—çš„ï¼Œä»…ç”¨äºå®˜æ–¹è¯„ä¼°ã€‚
    - ç›®å½•å†…åŒ…å«å›¾åƒæ–‡ä»¶ï¼Œæ–‡ä»¶åä»¥`.jpg`ç»“å°¾ï¼Œæ–‡ä»¶åå‰ç¼€æ˜¯è¿ç»­çš„æ•°å­—ï¼Œè¡¨ç¤ºå›¾åƒçš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
  - `train2017`ï¼šè¿™ä¸ªç›®å½•åŒ…å«äº†è®­ç»ƒé›†çš„å›¾åƒæ–‡ä»¶ï¼Œç”¨äºæ¨¡å‹çš„è®­ç»ƒã€‚
    - åŒ`test2017`ï¼Œç›®å½•å†…åŒ…å«ä»¥æ•°å­—å‘½åçš„`.jpg`å›¾åƒæ–‡ä»¶ã€‚
  - `val2017`ï¼šè¿™ä¸ªç›®å½•åŒ…å«äº†éªŒè¯é›†çš„å›¾åƒæ–‡ä»¶ï¼Œç”¨äºæ¨¡å‹çš„è¯„ä¼°å’Œè°ƒå‚ã€‚
    - åŒ`test2017`å’Œ`train2017`ï¼Œç›®å½•å†…åŒ…å«ä»¥æ•°å­—å‘½åçš„`.jpg`å›¾åƒæ–‡ä»¶ã€‚

æ•´ä¸ªç›®å½•ç»“æ„å°†å›¾åƒæ•°æ®å’Œæ³¨é‡Šæ•°æ®æ¸…æ™°åœ°åˆ†å¼€ï¼Œä¾¿äºåœ¨ä¸åŒçš„ä»»åŠ¡ï¼ˆå¦‚è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ï¼‰ä¸­ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†åˆ†åˆ«å­˜æ”¾åœ¨ä¸åŒçš„ç›®å½•ä¸­ï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

#### 3. ç›®æ ‡æ£€æµ‹æ ‡ç­¾æ–‡ä»¶

<details><summary>ğŸª ç‚¹å‡»æŸ¥çœ‹COCOæ ‡ç­¾å†…å®¹</summary>

```json
{
    "info": {
        "description": "COCO 2017 Dataset",
        "url": "http://cocodataset.org",
        "version": "1.0",
        "year": 2017,
        "contributor": "COCO Consortium",
        "date_created": "2017/09/01"
    },
    "licenses": [
        {
            "url": "http://creativecommons.org/licenses/by-nc-sa/2.0/",
            "id": 1,
            "name": "Attribution-NonCommercial-ShareAlike License"
        },
        {
            "url": "http://creativecommons.org/licenses/by-nc/2.0/",
            "id": 2,
            "name": "Attribution-NonCommercial License"
        },
        {
            "...": "..."
        }
    ],
    "images": [
        {
            "license": 4,
            "file_name": "000000397133.jpg",
            "coco_url": "http://images.cocodataset.org/val2017/000000397133.jpg",
            "height": 427,
            "width": 640,
            "date_captured": "2013-11-14 17:02:52",
            "flickr_url": "http://farm7.staticflickr.com/6116/6255196340_da26cf2c9e_z.jpg",
            "id": 397133
        },
        {
            "license": 1,
            "file_name": "000000037777.jpg",
            "coco_url": "http://images.cocodataset.org/val2017/000000037777.jpg",
            "height": 230,
            "width": 352,
            "date_captured": "2013-11-14 20:55:31",
            "flickr_url": "http://farm9.staticflickr.com/8429/7839199426_f6d48aa585_z.jpg",
            "id": 37777
        },
        {
            "...": "..."
        }
    ],
    "annotations": [
        {
            "segmentation": [
                [
                    510.66,
                    423.01,
                    511.72,
                    "......",
                    424.6,
                    498.02,
                    510.45,
                    423.01
                ]
            ],
            "area": 702.1057499999998,
            "iscrowd": 0,
            "image_id": 289343,
            "bbox": [
                473.07,
                395.93,
                38.65,
                28.67
            ],
            "category_id": 18,
            "id": 1768
        },
        {
            "segmentation": [
                [
                    289.74,
                    443.39,
                    302.29,
                    "......", 
                    288.64,
                    444.27,
                    291.88,
                    443.74
                ]
            ],
            "area": 27718.476299999995,
            "iscrowd": 0,
            "image_id": 61471,
            "bbox": [
                272.1,
                200.23,
                151.97,
                279.77
            ],
            "category_id": 18,
            "id": 1773
        },
        {
            "...": "..."
        }
    ],
    "categories": [
        {
            "supercategory": "person",
            "id": 1,
            "name": "person"
        },
        {
            "supercategory": "vehicle",
            "id": 2,
            "name": "bicycle"
        },
        {
            "...": "..."
        }
    ]
}
```

</details>

1. **infoï¼ˆä¿¡æ¯ï¼‰**:
   - æè¿°ï¼ˆdescriptionï¼‰: COCO 2017æ•°æ®é›†
   - ç½‘å€ï¼ˆurlï¼‰: [COCO Dataset](http://cocodataset.org)
   - ç‰ˆæœ¬ï¼ˆversionï¼‰: 1.0
   - å¹´ä»½ï¼ˆyearï¼‰: 2017
   - è´¡çŒ®è€…ï¼ˆcontributorï¼‰: COCO Consortium
   - åˆ›å»ºæ—¥æœŸï¼ˆdate_createdï¼‰: 2017å¹´9æœˆ1æ—¥
2. **licensesï¼ˆè®¸å¯åè®®ï¼‰**:
   - åŒ…å«å¤šä¸ªè®¸å¯åè®®å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡æœ‰ä»¥ä¸‹å±æ€§ï¼š
     - urlï¼ˆç½‘å€ï¼‰: æŒ‡å‘è®¸å¯åè®®çš„é“¾æ¥
     - idï¼ˆç¼–å·ï¼‰: è®¸å¯åè®®çš„å”¯ä¸€æ ‡è¯†ç¬¦
     - nameï¼ˆåç§°ï¼‰: è®¸å¯åè®®çš„åç§°
3. **imagesï¼ˆå›¾åƒï¼‰**:
   - åŒ…å«å¤šä¸ªå›¾åƒå¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡æœ‰ä»¥ä¸‹å±æ€§ï¼š
     - licenseï¼ˆè®¸å¯ï¼‰: å›¾åƒä½¿ç”¨çš„è®¸å¯åè®®ç¼–å·
     - file_nameï¼ˆæ–‡ä»¶åï¼‰: å›¾åƒæ–‡ä»¶çš„åç§°
     - coco_urlï¼ˆCOCOç½‘å€ï¼‰: COCOæ•°æ®é›†ä¸­å›¾åƒçš„é“¾æ¥
     - heightï¼ˆé«˜åº¦ï¼‰: å›¾åƒçš„é«˜åº¦ï¼ˆåƒç´ ï¼‰
     - widthï¼ˆå®½åº¦ï¼‰: å›¾åƒçš„å®½åº¦ï¼ˆåƒç´ ï¼‰
     - date_capturedï¼ˆæ‹æ‘„æ—¥æœŸï¼‰: å›¾åƒæ‹æ‘„çš„æ—¥æœŸå’Œæ—¶é—´
     - flickr_urlï¼ˆFlickrç½‘å€ï¼‰: å›¾åƒåœ¨Flickrä¸Šçš„é“¾æ¥
     - idï¼ˆç¼–å·ï¼‰: å›¾åƒçš„å”¯ä¸€æ ‡è¯†ç¬¦
4. **annotationsï¼ˆæ³¨é‡Šï¼‰**:
   - åŒ…å«å¤šä¸ªæ³¨é‡Šå¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡æœ‰ä»¥ä¸‹å±æ€§ï¼š
     - segmentationï¼ˆåˆ†å‰²ï¼‰: å›¾åƒä¸­å¯¹è±¡çš„å¤šè¾¹å½¢é¡¶ç‚¹åæ ‡åˆ—è¡¨
     - areaï¼ˆé¢ç§¯ï¼‰: å¤šè¾¹å½¢æ‰€å›´æˆçš„åŒºåŸŸé¢ç§¯
     - iscrowdï¼ˆæ˜¯å¦ä¸ºäººç¾¤ï¼‰: ä¸€ä¸ªæ ‡å¿—ï¼Œè¡¨ç¤ºè¯¥æ³¨é‡Šæ˜¯å¦è¡¨ç¤ºä¸€ä¸ªäººç¾¤
     - image_idï¼ˆå›¾åƒç¼–å·ï¼‰: æ³¨é‡Šæ‰€å¯¹åº”çš„å›¾åƒçš„å”¯ä¸€æ ‡è¯†ç¬¦
     - bboxï¼ˆè¾¹ç•Œæ¡†ï¼‰: è¡¨ç¤ºå¯¹è±¡åœ¨å›¾åƒä¸­çš„ä½ç½®å’Œå¤§å°çš„è¾¹ç•Œæ¡†ï¼ˆæ ¼å¼ä¸º[x_min, y_min, width, height]ï¼‰
     - category_idï¼ˆç±»åˆ«ç¼–å·ï¼‰: æ³¨é‡Šæ‰€å±çš„ç±»åˆ«ç¼–å·
     - idï¼ˆç¼–å·ï¼‰: æ³¨é‡Šçš„å”¯ä¸€æ ‡è¯†ç¬¦
5. **categoriesï¼ˆç±»åˆ«ï¼‰**:
   - åŒ…å«å¤šä¸ªç±»åˆ«å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡æœ‰ä»¥ä¸‹å±æ€§ï¼š
     - supercategoryï¼ˆä¸Šçº§ç±»åˆ«ï¼‰: ç±»åˆ«çš„ä¸Šçº§åˆ†ç±»
     - idï¼ˆç¼–å·ï¼‰: ç±»åˆ«çš„å”¯ä¸€æ ‡è¯†ç¬¦
     - nameï¼ˆåç§°ï¼‰: ç±»åˆ«çš„åç§°

#### 4. captionsæ ‡ç­¾æ–‡ä»¶

<details><summary>ğŸª ç‚¹å‡»æŸ¥çœ‹captions.jsonçš„å†…å®¹</summary>

```json
{
    "info": {
        "description": "COCO 2017 Dataset",
        "url": "http://cocodataset.org",
        "version": "1.0",
        "year": 2017,
        "contributor": "COCO Consortium",
        "date_created": "2017/09/01"
    },
    "licenses": [
        {
            "url": "http://creativecommons.org/licenses/by-nc-sa/2.0/",
            "id": 1,
            "name": "Attribution-NonCommercial-ShareAlike License"
        },
        {
            "...": "..."
        },
    ],
    "images": [
        {
            "license": 4,
            "file_name": "000000397133.jpg",
            "coco_url": "http://images.cocodataset.org/val2017/000000397133.jpg",
            "height": 427,
            "width": 640,
            "date_captured": "2013-11-14 17:02:52",
            "flickr_url": "http://farm7.staticflickr.com/6116/6255196340_da26cf2c9e_z.jpg",
            "id": 397133
        },
        {
            "license": 1,
            "file_name": "000000037777.jpg",
            "coco_url": "http://images.cocodataset.org/val2017/000000037777.jpg",
            "height": 230,
            "width": 352,
            "date_captured": "2013-11-14 20:55:31",
            "flickr_url": "http://farm9.staticflickr.com/8429/7839199426_f6d48aa585_z.jpg",
            "id": 37777
        },
        {
            "...": "..."
        }
    ],
    "annotations": [
        {
            "image_id": 179765,
            "id": 38,
            "caption": "A black Honda motorcycle parked in front of a garage."
        },
        {
            "image_id": 179765,
            "id": 182,
            "caption": "A Honda motorcycle parked in a grass driveway"
        },
        {
            "image_id": 190236,
            "id": 401,
            "caption": "An office cubicle with four different types of computers."
        },
        {
            "image_id": 331352,
            "id": 441,
            "caption": "A small closed toilet in a cramped space."
        },
        {
            "...": "..."
        }
    ]
}
```

</details>

1. **infoï¼ˆä¿¡æ¯ï¼‰**ï¼šå’Œå‰é¢çš„ä¸€æ ·ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚
2. **licensesï¼ˆè®¸å¯åè®®ï¼‰**ï¼šå’Œå‰é¢çš„ä¸€æ ·ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚
3. **imagesï¼ˆå›¾åƒï¼‰**ï¼šå’Œå‰é¢çš„ä¸€æ ·ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚
4. **annotationsï¼ˆæ³¨é‡Šï¼‰**:
   - åŒ…å«å­—å¹•çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å¹•æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå…·æœ‰ä»¥ä¸‹å±æ€§ï¼š
     - `image_id`ï¼ˆå›¾åƒç¼–å·ï¼‰: ä¸å­—å¹•å…³è”çš„å›¾åƒçš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
     - `id`ï¼ˆç¼–å·ï¼‰: æ³¨é‡Šçš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
     - `caption`ï¼ˆå­—å¹•ï¼‰: å›¾åƒçš„æè¿°æ€§æ–‡æœ¬ï¼Œç”¨è‡ªç„¶è¯­è¨€æè¿°å›¾åƒå†…å®¹ã€‚

ä¾‹å¦‚ï¼Œæ³¨é‡Šä¸­çš„ä¸€æ¡è®°å½•ï¼š

```json
{
    "image_id": 179765,
    "id": 38,
    "caption": "A black Honda motorcycle parked in front of a garage."
}
```

æˆ‘ä»¬ä»COCOå®˜ç½‘è·å–è¿™å¼ å›¾ç‰‡ï¼š

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-06-18-11-34-02.png
    width=35%></br><center>http://images.cocodataset.org/val2017/000000179765.jpg</center>
</div></br>

è¡¨ç¤ºå›¾åƒIDä¸º179765çš„å›¾åƒæœ‰ä¸€ä¸ªå­—å¹•ï¼Œè¯¥å­—å¹•çš„IDæ˜¯38ï¼Œæè¿°æ˜¯"A black Honda motorcycle parked in front of a garage."ï¼ˆä¸€è¾†é»‘è‰²æœ¬ç”°æ‘©æ‰˜è½¦åœåœ¨è½¦åº“å‰ï¼‰ã€‚

æˆ‘ä»¬ä¹Ÿå‘ç°ï¼Œè¿˜æœ‰ä¸€æ¡æ³¨é‡Šä¹Ÿå¯¹è¿™å¼ å›¾ç‰‡è¿›è¡Œäº†captionsï¼š

```json
{
    "image_id": 179765,
    "id": 182,
    "caption": "A Honda motorcycle parked in a grass driveway"
}
```

è¿™é‡Œå¯ä»¥å‘ç°ï¼Œ<font color='red'><b>ä¸€å¼ å›¾ç‰‡ä¸ä¸€å®šåªæœ‰ä¸€ä¸ªcaptionï¼Œæœ‰å¯èƒ½ä¼šæœ‰å¤šä¸ªcaptions</b></font>ã€‚

### 5.4.4 MixedGroundingæ•°æ®é›†

å’Œä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ç›¸æ¯”ï¼ŒMixedGroundingæ•°æ®é›†å¤šäº†æ–‡å­—æè¿°ï¼Œå³<font color='red'><b>ä¸€å¼ å›¾ç‰‡æœ‰ä¸€ä¸ªcaption</b></font>ã€‚

# å‚è€ƒæ–‡çŒ®

1. [YOLO-World/docs](https://github.com/AILab-CVC/YOLO-World/tree/master/docs)
2. [Zero Shotã€One Shotã€Few Shotçš„é€šä¿—ç†è§£](https://blog.51cto.com/u_15408171/7004231)