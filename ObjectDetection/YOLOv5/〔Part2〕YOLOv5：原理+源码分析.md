# 5. YOLOv5 è®­ç»ƒæŠ€å·§

## 5.1 warm-up

åœ¨ YOLOv5 ä¸­ï¼Œwarm-upï¼ˆé¢„çƒ­ï¼‰æ˜¯æŒ‡åœ¨è®­ç»ƒåˆå§‹é˜¶æ®µä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œç„¶åé€æ¸å¢åŠ å­¦ä¹ ç‡ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°é€‚åº”æ•°æ®é›†ã€‚è¿™ä¸ªè¿‡ç¨‹æœ‰åŠ©äºé¿å…åœ¨åˆå§‹é˜¶æ®µå‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–ä¸ç¨³å®šçš„æƒ…å†µï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“æ”¶æ•›ã€‚

YOLOv5 ä¸­çš„ warm-up ä¸»è¦ä½“ç°åœ¨å­¦ä¹ ç‡çš„è°ƒæ•´ä¸Šã€‚å…·ä½“è€Œè¨€ï¼ŒYOLOv5 ä½¿ç”¨çº¿æ€§ warm-up ç­–ç•¥ï¼Œå³åœ¨åˆå§‹è®­ç»ƒé˜¶æ®µï¼Œå­¦ä¹ ç‡ä»ä¸€ä¸ªè¾ƒå°çš„åˆå§‹å€¼çº¿æ€§å¢åŠ åˆ°è®¾å®šçš„åˆå§‹å­¦ä¹ ç‡ã€‚è¿™æœ‰åŠ©äºå‡ç¼“æ¨¡å‹çš„å‚æ•°æ›´æ–°é€Ÿåº¦ï¼Œé˜²æ­¢åœ¨åˆå§‹æ—¶å‡ºç°è¿‡å¤§çš„æƒé‡æ›´æ–°ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

åœ¨ YOLOv5 çš„å®ç°ä¸­ï¼Œwarm-up é˜¶æ®µé€šå¸¸æŒç»­ä¸€å®šçš„è¿­ä»£æ¬¡æ•°ï¼Œè¿™ä¸ªæ¬¡æ•°æ˜¯åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾å®šçš„ã€‚ä¸€æ—¦ warm-up é˜¶æ®µç»“æŸï¼Œæ¨¡å‹å°†ä»¥è®¾å®šçš„åˆå§‹å­¦ä¹ ç‡è¿›è¡Œæ­£å¸¸çš„è®­ç»ƒã€‚

Warm-up çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå¯ä»¥åœ¨æ¨¡å‹å¼€å§‹å­¦ä¹ ä»»åŠ¡æ—¶æ›´å¥½åœ°æ§åˆ¶å­¦ä¹ çš„é€Ÿåº¦ï¼Œä»è€Œæœ‰åŠ©äºæ¨¡å‹æ›´å¿«åœ°é€‚åº”æ•°æ®åˆ†å¸ƒã€‚è¿™åœ¨å¤„ç†å¤æ‚çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡é€šå¸¸å…·æœ‰å¤§é‡çš„æ ·æœ¬å’Œå¤æ‚çš„èƒŒæ™¯ã€‚

æˆ‘ä»¬çœ‹ä¸€ä¸‹ç›¸å…³çš„æºç ï¼ˆ`train.py`ï¼‰ï¼š

```python
nb = len(train_loader)  # number of batches | ä¸€ä¸ªepochæ‹¥æœ‰çš„batchæ•°é‡
nw = max(round(hyp["warmup_epochs"] * nb), 100)  # number of warmup | çƒ­èº«çš„æ€»è¿­ä»£æ¬¡æ•°

pbar = enumerate(train_loader)  # éå†train_loader

# è®°å½•æ—¥å¿—
LOGGER.info(("\n" + "%11s" * 7) % ("Epoch", "GPU_mem", "box_loss", "obj_loss", "cls_loss", "Instances", "Size"))

# å¦‚æœåœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œé‚£ä¹ˆç»™enumberateåŠ ä¸Štqdmè¿›åº¦æ¡
if RANK in {-1, 0}:
    pbar = tqdm(pbar, total=nb, bar_format=TQDM_BAR_FORMAT)  # progress bar

# å¼€å§‹éå†train_loader
for i, (imgs, targets, paths, _) in pbar:  # batch 
    # imgs: ä¸€ä¸ªbatchçš„å›¾ç‰‡
    # targets: ä¸€ä¸ªbatchçš„æ ‡ç­¾
    # paths: ä¸€ä¸ªbatchçš„è·¯å¾„
    callbacks.run("on_train_batch_start")  # è®°å½•æ­¤æ—¶æ­£åœ¨å¹²ä»€ä¹ˆ

    # è®¡ç®—å½“å‰çš„è¿­ä»£æ¬¡æ•°
    ni = i + nb * epoch  # number integrated batches (since train start)
    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0

    # Warmup
    if ni <= nw:  # å¦‚æœå½“å‰çš„è¿­ä»£æ¬¡æ•°å°äºéœ€è¦çƒ­èº«çš„è¿­ä»£æ¬¡æ•°ï¼Œåˆ™å¼€å§‹çƒ­èº«
        xi = [0, nw]  # x interp

        # accumulateå˜é‡çš„ä½œç”¨æ˜¯åŠ¨æ€åœ°æ§åˆ¶ç´¯ç§¯çš„ Batch æ•°ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒå¼€å§‹æ—¶é€æ¸å¢åŠ ç´¯ç§¯çš„ Batch æ•°ï¼Œ
        # ä»è€Œå®ç°ä»è¾ƒå°çš„ç´¯ç§¯ Batch æ•°åˆ°è¾ƒå¤§çš„ç´¯ç§¯ Batch æ•°çš„å¹³æ»‘è¿‡æ¸¡
        # è¿™æœ‰åŠ©äºæ¨¡å‹åœ¨è®­ç»ƒåˆæœŸç¨³å®šåœ°å­¦ä¹ 
        accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())
        for j, x in enumerate(optimizer.param_groups):
            # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
            x["lr"] = np.interp(ni, xi, [hyp["warmup_bias_lr"] if j == 0 else 0.0, x["initial_lr"] * lf(epoch)])
            if "momentum" in x:
                x["momentum"] = np.interp(ni, xi, [hyp["warmup_momentum"], hyp["momentum"]])
```

åœ¨ [How suspend image mixing #931](https://github.com/ultralytics/yolov5/issues/931) ä¸­æœ‰ä½œè€…å…³äº warm-up çš„è¯´æ˜ï¼š

warmup æ…¢æ…¢åœ°å°†è®­ç»ƒå‚æ•°ä»å®ƒä»¬çš„åˆå§‹ï¼ˆæ›´ç¨³å®šï¼‰å€¼è°ƒæ•´åˆ°å®ƒä»¬çš„é»˜è®¤è®­ç»ƒå€¼ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸ä¼šåœ¨æœ€åˆçš„å‡ ä¸ª Epoch å†…å°†å­¦ä¹ ç‡ä» 0 è°ƒæ•´åˆ°æŸä¸ªåˆå§‹å€¼ï¼Œä»¥é¿å…æ—©æœŸè®­ç»ƒçš„ä¸ç¨³å®šã€nan ç­‰é—®é¢˜ã€‚

çƒ­èº«æ•ˆæœå¯ä»¥åœ¨ Tensorboard çš„å­¦ä¹ ç‡æ›²çº¿å›¾ä¸­è§‚å¯Ÿåˆ°ï¼Œè¿™äº›æ›²çº¿è‡ªä»æœ€è¿‘çš„æäº¤ä»¥æ¥å·²ç»è¢«è‡ªåŠ¨è·Ÿè¸ªã€‚ä¸‹é¢çš„ä¾‹å­æ˜¾ç¤ºäº†åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¤§çº¦ 30 ä¸ª Epoch çš„çƒ­èº«ï¼Œæ¯ä¸ªå‚æ•°ç»„æœ‰ä¸€ä¸ªæ›²çº¿å›¾ã€‚æœ€åä¸€ä¸ªæ›²çº¿å›¾å±•ç¤ºäº†ä¸åŒçš„çƒ­èº«ç­–ç•¥ï¼ˆå³ä¸åŒçš„è¶…å‚æ•°è®¾ç½®ï¼‰ã€‚

<div align=center>
    <img src=./imgs_markdown/2024-02-06-17-35-38.png
    width=100%>
    <center></center>
</div>

### 5.1.1 np.interp è¯­æ³•

`numpy.interp(x, xp, fp, left=None, right=None, period=None)` æ˜¯ NumPy ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºçº¿æ€§æ’å€¼ã€‚çº¿æ€§æ’å€¼æ˜¯ä¸€ç§ä¼°ç®—åœ¨ä¸¤ä¸ªå·²çŸ¥å€¼ä¹‹é—´çš„æœªçŸ¥å€¼çš„æ–¹æ³•ï¼Œå‡è®¾è¿™äº›å€¼ä¹‹é—´çš„å˜åŒ–æ˜¯çº¿æ€§çš„ã€‚

å…¶ä¸­ï¼š

- `x`: éœ€è¦è¿›è¡Œæ’å€¼çš„ä¸€ç»´æ•°ç»„ã€‚
- `xp`: å·²çŸ¥æ•°æ®ç‚¹çš„ x åæ ‡ï¼ˆä¸€ç»´æ•°ç»„ï¼‰-> x pointsã€‚
- `fp`: å·²çŸ¥æ•°æ®ç‚¹çš„ y åæ ‡ï¼ˆä¸€ç»´æ•°ç»„ï¼‰-> function pointsã€‚
- `left`: å½“ x å°äº xp çš„æœ€å°å€¼æ—¶ï¼Œè¿”å›çš„é»˜è®¤å€¼ï¼Œé»˜è®¤ä¸º fp[0]ã€‚
- `right`: å½“ x å¤§äº xp çš„æœ€å¤§å€¼æ—¶ï¼Œè¿”å›çš„é»˜è®¤å€¼ï¼Œé»˜è®¤ä¸º fp[-1]ã€‚
- `period`: å¦‚æœæä¾›äº† periodï¼Œè¡¨ç¤º xp æ˜¯å‘¨æœŸæ€§çš„ï¼Œæ­¤æ—¶æ’å€¼ä¼šè€ƒè™‘å‘¨æœŸæ€§ã€‚period æ˜¯å‘¨æœŸçš„é•¿åº¦ã€‚

**ç¤ºä¾‹**ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

# å·²çŸ¥æ•°æ®ç‚¹
x_known = np.array([1, 2, 3, 4, 5])
y_known = np.array([3, 5, 7, 9, 11])

# å¾…æ’å€¼çš„æ•°æ®ç‚¹
x_unknown = [0.0, 1.5, 3.0, 4.5, 6.0]

# ä½¿ç”¨np.interpè¿›è¡Œæ’å€¼
y_unknown = np.interp(x_unknown, x_known, y_known)
print(f"{y_unknown = }")  # [3, 4, 7, 10, 11]

# ç»˜åˆ¶å›¾å½¢
plt.figure(figsize=(10, 6), dpi=200)
plt.plot(x_known, y_known, 'o', label='Known points', color='green')  # å·²çŸ¥æ•°æ®ç‚¹
plt.plot(x_unknown, y_unknown, 'o', label='Unknown points', color='red')  # æ’å€¼ç»“æœ
plt.xlabel('x')
plt.ylabel('y')
plt.title(r'Example for $np.interp()$')
plt.legend()
plt.grid(True)
plt.savefig('Example4np.interp.jpg')
```

çœ‹ä¸æ‡‚æ²¡å…³ç³»ï¼Œæˆ‘ä»¬ä½œå›¾çœ‹ä¸€ä¸‹ï¼š

<div align=center>
    <img src=./imgs_markdown/2024-02-05-14-29-23.png
    width=100%>
    <center></center>
</div>

å¤–æ¨è§„åˆ™å¦‚ä¸‹ï¼š
- å¦‚æœ `x` çš„å€¼å°äº `xp` çš„æœ€å°å€¼ï¼Œåˆ™ `np.interp` è¿”å›ä¸ `xp` æœ€å°å€¼å¯¹åº”çš„ `fp` å€¼ã€‚
- å¦‚æœ `x` çš„å€¼å¤§äº `xp` çš„æœ€å¤§å€¼ï¼Œåˆ™ `np.interp` è¿”å›ä¸ `xp` æœ€å¤§å€¼å¯¹åº”çš„ `fp` å€¼ã€‚

åˆ†æå¦‚ä¸‹ï¼š

- x[0] = 0.0, å®ƒå°äº xp çš„æœ€å°å€¼1ï¼Œæ‰€ä»¥å¤–æ¨ï¼Œæ­¤æ—¶ x[0] å¯¹åº”çš„ y[0] = fp[0] -> 3
- x[1] = 1.5, å®ƒåœ¨ xp çš„ [1, 2] ä¹‹é—´ï¼Œæ‰€ä»¥å¯¹åº”çš„ y[1] åº”è¯¥ä¸º y[0] = (fp[1] + f[2]) / 2 --> (3 + 5) / 2 = 4
- x[2] = 3.0 == xp[2], æ‰€ä»¥å¯¹åº”çš„ y[2] == fp[2] --> 7
- x[3] = 4.5 âˆˆ [4, 5], y[3] == (fp[4] + fp[5]) / 2 --> (9 + 11) / 2 --> 10
- x[4] = 6.0ï¼Œå®ƒå¤§äº xp çš„æœ€å¤§å€¼ï¼Œæ‰€ä»¥å¤–æ¨ï¼Œæ­¤è‡´ x[4] å¯¹åº”çš„ y[4] == fp[5] --> 11

> âš ï¸ x å’Œ y å–çš„æ˜¯ç´¢å¼•ï¼Œè€Œ xp å’Œ fp è¿™é‡Œä¸æ˜¯å–ç´¢å¼•ï¼Œè€Œæ˜¯å–å€¼

## 5.2 Cosine Annealing Warm Restart

- è®ºæ–‡åœ°å€ï¼š[SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)
- ç¿»è¯‘ï¼š[Cosine Annealing Warm Restartè®ºæ–‡è®²è§£](https://blog.csdn.net/weixin_44878336/article/details/125016166)

Cosine Annealing Warm Restart æ˜¯ä¸€ç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼Œå®ƒæ˜¯åŸºäºä½™å¼¦é€€ç«å‘¨æœŸæ€§è°ƒæ•´å­¦ä¹ ç‡çš„ç®—æ³•ã€‚è¿™ç§ç­–ç•¥åœ¨å­¦ä¹ ç‡è°ƒæ•´ä¸Šå¼•å…¥äº†å‘¨æœŸæ€§çš„â€œé‡å¯â€ï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå‘¨æœŸæ€§åœ°è·³å‡ºå±€éƒ¨æœ€å°å€¼ï¼Œä»è€Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚

å…·ä½“æ¥è¯´ï¼ŒCosine Annealing Warm Restart ç­–ç•¥åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š

1. **ä½™å¼¦é€€ç«å‘¨æœŸ**ï¼šåœ¨æ¯ä¸ªå‘¨æœŸå†…ï¼Œå­¦ä¹ ç‡æŒ‰ç…§ä½™å¼¦å‡½æ•°çš„å˜åŒ–è§„å¾‹è¿›è¡Œè°ƒæ•´ã€‚ä½™å¼¦å‡½æ•°ä»æœ€å¤§å€¼å¼€å§‹ï¼Œé€æ¸å‡å°åˆ°æœ€å°å€¼ï¼Œå› æ­¤å­¦ä¹ ç‡ä¹Ÿä¼šä»åˆå§‹å€¼å¼€å§‹ï¼Œå…ˆå‡å°åˆ°ä¸€ä¸ªä½ç‚¹ï¼Œç„¶åå†å¢åŠ å›åˆ°åˆå§‹å€¼ã€‚
2. **å‘¨æœŸæ€§é‡å¯**ï¼šåœ¨æ¯ä¸ªå‘¨æœŸç»“æŸæ—¶ï¼Œå­¦ä¹ ç‡ä¼šè¢«é‡æ–°è®¾ç½®å›åˆå§‹å€¼ï¼Œå¹¶é‡æ–°å¼€å§‹ä¸€ä¸ªæ–°çš„å‘¨æœŸã€‚è¿™ç§é‡å¯æœ‰åŠ©äºæ¨¡å‹è·³å‡ºå½“å‰çš„ä¼˜åŒ–è·¯å¾„ï¼Œæ¢ç´¢æ–°çš„å‚æ•°ç©ºé—´ã€‚
3. **å‘¨æœŸé•¿åº¦è°ƒæ•´**ï¼šéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå‘¨æœŸé•¿åº¦ï¼ˆå³é€€ç«å‘¨æœŸï¼‰å’Œæœ€å°å­¦ä¹ ç‡å¯ä»¥é€æ¸è°ƒæ•´ã€‚é€šå¸¸ï¼Œæ¯ä¸ªå‘¨æœŸçš„é•¿åº¦ä¼šé€æ¸å‡å°ï¼Œè€Œæœ€å°å­¦ä¹ ç‡ä¼šé€æ¸å¢åŠ ï¼Œè¿™æ ·å¯ä»¥è®©æ¨¡å‹åœ¨è®­ç»ƒåæœŸæ›´åŠ ç»†è‡´åœ°æœç´¢æœ€ä¼˜è§£ã€‚
4. **å­¦ä¹ ç‡èŒƒå›´**ï¼šåœ¨æ¯ä¸ªå‘¨æœŸå†…ï¼Œå­¦ä¹ ç‡çš„å˜åŒ–èŒƒå›´æ˜¯ä»æœ€å¤§å€¼åˆ°æœ€å°å€¼ï¼Œè¿™ä¸¤ä¸ªå€¼éƒ½å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚

Cosine Annealing Warm Restart ç­–ç•¥çš„ä¼˜åŠ¿åœ¨äºå®ƒé€šè¿‡å‘¨æœŸæ€§é‡å¯å’Œè°ƒæ•´å‘¨æœŸé•¿åº¦ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­æ¢ç´¢æ–°çš„å‚æ•°ç©ºé—´ï¼Œä»è€Œæœ‰å¯èƒ½æ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€å°å€¼æˆ–å…¨å±€æœ€å°å€¼ã€‚è¿™ç§ç­–ç•¥ç‰¹åˆ«é€‚åˆäºé‚£äº›å®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼çš„å¤æ‚æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

åœ¨è®ºæ–‡ [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187) ä¸­æœ‰ä»‹ç»åˆ°ä½™å¼¦é€€ç«å’Œé˜¶æ®µä¸¤ç§å­¦ä¹ ç‡åœ¨ ImageNet æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆæ¨¡å‹ä¸º ResNet-50ï¼‰ï¼š

<div align=center>
    <img src=./imgs_markdown/2024-02-06-17-42-37.png
    width=70%>
    <center></center>
</div>

> å›¾ 3ï¼šå¸¦æœ‰çƒ­èº«é˜¶æ®µçš„å­¦ä¹ ç‡è®¡åˆ’çš„å¯è§†åŒ–ã€‚é¡¶éƒ¨ï¼šBatch size=1024 ä¸‹çš„ä½™å¼¦å’Œé˜¶è·ƒè°ƒåº¦ã€‚åº•éƒ¨ï¼šä¸¤ç§è°ƒåº¦ä¸‹çš„Top-1éªŒè¯å‡†ç¡®ç‡æ›²çº¿ã€‚

---

ä½™å¼¦é€€ç«çƒ­é‡å¯çš„è°ƒç”¨å¦‚ä¸‹ï¼š

```python
import torch.optim as optim


model = ...
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ä½¿ç”¨CosineAnnealingWarmRestartsè°ƒåº¦å™¨
# T_0æ˜¯åˆå§‹å‘¨æœŸçš„å¤§å°ï¼ŒT_multæ¯ä¸ªå‘¨æœŸç»“æŸåå‘¨æœŸå¤§å°ä¹˜ä»¥çš„å€æ•°
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)

for epoch in range(num_epochs):
    # è®­ç»ƒæ¨¡å‹çš„ä»£ç 
    train(...)
    # åœ¨æ¯ä¸ªepochåæ›´æ–°å­¦ä¹ ç‡
    scheduler.step(epoch)
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œ`T_0` å‚æ•°ä»£è¡¨åˆå§‹å‘¨æœŸçš„å¤§å°ï¼Œå³åœ¨ç¬¬ä¸€æ¬¡ä½™å¼¦é€€ç«å‘¨æœŸä¸­ï¼Œå­¦ä¹ ç‡å°†æŒ‰ç…§ä½™å¼¦è°ƒåº¦è¿›è¡Œè°ƒæ•´çš„ Epoch æ•°ã€‚`T_mult` å‚æ•°æŒ‡å®šäº†æ¯ä¸ªå‘¨æœŸç»“æŸåå‘¨æœŸå¤§å°å°†ä¹˜ä»¥çš„å€æ•°ã€‚`scheduler.step(epoch)` åº”è¯¥åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°ä¹‹åã€æ¯ä¸ªepochç»“æŸæ—¶è°ƒç”¨ã€‚
è¯·æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚è°ƒæ•´ `T_0` å’Œ `T_mult` çš„å€¼ï¼Œä»¥åŠ `num_epochs`ï¼Œå³ä½ çš„è®­ç»ƒå‘¨æœŸæ€»æ•°ã€‚

## 5.3 YOLOv5-v7.0 ä½¿ç”¨çš„ Scheduler

```python
# Scheduler
if opt.cos_lr:  # å¦‚æœä½¿ç”¨cosineå­¦ä¹ ç‡
    lf = one_cycle(1, hyp["lrf"], epochs)  # cosine 1->hyp['lrf']
else:
    lf = lambda x: (1 - x / epochs) * (1.0 - hyp["lrf"]) + hyp["lrf"]  # linear
scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)
```

æˆ‘ä»¬ç”»å›¾çœ‹ä¸€ä¸‹äºŒè€…çš„åŒºåˆ«ï¼š

```python
import matplotlib.pyplot as plt
import math


def one_cycle(y1=0.0, y2=1.0, steps=100):
    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf
    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1


# è®¾å®šè®­ç»ƒçš„æ€»epochæ•°
epochs = 100

# YOLOv5ä¸­çš„è¶…å‚æ•°
hyp = {
    "lr0": 0.01,  # åˆå§‹å­¦ä¹ ç‡
    "lrf": 0.1  # final OneCycleLR learning rate (lr0 * lrf)
}

# åˆ›å»ºä¸€ä¸ªnumpyæ•°ç»„ï¼Œè¡¨ç¤ºepochæ•°
epoch_lst = range(epochs)

# Cosineè°ƒåº¦å™¨çš„å­¦ä¹ ç‡å˜åŒ–
lf_cos = one_cycle(1, hyp["lrf"], epochs)
lr_cos = [lf_cos(epoch) for epoch in epoch_lst]

# Linearè°ƒåº¦å™¨çš„å­¦ä¹ ç‡å˜åŒ–
lf_lin = lambda x: (1 - x / epochs) * (1.0 - hyp["lrf"]) + hyp["lrf"]
lr_lin = [lf_lin(epoch) for epoch in epoch_lst]

# ç»˜åˆ¶å­¦ä¹ ç‡å˜åŒ–æ›²çº¿
plt.figure(figsize=(10, 6), dpi=200)

plt.plot(epoch_lst, lr_cos, '-', label='Cosine Scheduler', color='skyblue')
plt.plot(epoch_lst, lr_lin, '-.', label='Linear Scheduler', color='lightpink')

plt.xlabel('Epochs')
plt.ylabel('Learning Rate')
plt.title('Comparison of Cosine and Linear Learning Rate Schedulers')

plt.legend()
plt.grid(True)
plt.savefig('Le0v1n/results/Comparison-of-Cosine-and-Linear-Learning-Rate-Schedulers.jpg')
```

<div align=center>
    <img src=./imgs_markdown/Comparison-of-Cosine-and-Linear-Learning-Rate-Schedulers.jpg
    width=100%>
    <center></center>
</div>

## 5.3 AutoAnchor

### 5.3.1 ç›®çš„

AutoAnchor æ˜¯ YOLOv5 ä¸­çš„ä¸€ä¸ªåŠŸèƒ½ï¼Œç”¨äºè‡ªåŠ¨è°ƒæ•´ Anchorï¼ˆanchor boxesï¼‰çš„å¤§å°ä»¥æ›´å¥½åœ°é€‚åº”è®­ç»ƒæ•°æ®é›†ä¸­çš„å¯¹è±¡å½¢çŠ¶ã€‚

> Anchor æ˜¯åœ¨å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­ä½¿ç”¨çš„ä¸€ç§æŠ€æœ¯ï¼Œå®ƒä»¬ä»£è¡¨äº†ä¸åŒå¤§å°å’Œå®½é«˜æ¯”çš„é¢„å®šä¹‰è¾¹ç•Œæ¡†ï¼Œç”¨äºé¢„æµ‹çœŸå®å¯¹è±¡çš„ä½ç½®å’Œå¤§å°ã€‚

åœ¨ YOLOv5 ä¸­ï¼ŒAutoAnchor çš„ä¸»è¦ç›®çš„æ˜¯ä¼˜åŒ– Anchor çš„å¤§å°ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´æé«˜æ£€æµ‹ç²¾åº¦å’Œæ•ˆç‡ã€‚è¿™ä¸ªåŠŸèƒ½åœ¨è®­ç»ƒè¿‡ç¨‹å¼€å§‹æ—¶æ‰§è¡Œï¼Œæ ¹æ®è®­ç»ƒæ•°æ®é›†ä¸­çš„è¾¹ç•Œæ¡†è®¡ç®—æœ€ä½³ Anchor é…ç½®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒYOLOv5 å¯ä»¥è‡ªåŠ¨é€‚åº”æ–°çš„æ•°æ®é›†ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´ Anchorã€‚

### 5.3.2 AutoAnchor çš„æ­¥éª¤

1. **åˆ†ææ•°æ®é›†**ï¼šåˆ†ææ•°æ®é›†ä¸­çš„è¾¹ç•Œæ¡†ï¼Œäº†è§£å¯¹è±¡çš„å¤§å°å’Œå½¢çŠ¶åˆ†å¸ƒã€‚
2. **Anchorèšç±»**ï¼šä½¿ç”¨èšç±»ç®—æ³•ï¼ˆå¦‚ K-meansï¼‰å¯¹è¾¹ç•Œæ¡†è¿›è¡Œèšç±»ï¼Œä»¥ç¡®å®šæœ€ä½³çš„ Anchor æ•°é‡å’Œå¤§å°ã€‚
3. **æ›´æ–°é…ç½®**ï¼šæ ¹æ®èšç±»ç»“æœæ›´æ–° Anchor é…ç½®ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨è¿™äº›æ–° Anchorã€‚
4. **é‡æ–°è®­ç»ƒ**ï¼šä½¿ç”¨æ–°çš„ Anchor é…ç½®é‡æ–°å¼€å§‹è®­ç»ƒè¿‡ç¨‹ã€‚

### 5.3.3 ä½œç”¨

AutoAnchor çš„ä¼˜åŠ¿åœ¨äºå®ƒèƒ½å¤Ÿä¸ºç‰¹å®šçš„æ•°æ®é›†å®šåˆ¶ Anchorï¼Œè¿™æœ‰åŠ©äºæé«˜æ£€æµ‹ç²¾åº¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å…·æœ‰ä¸åŒå¯¹è±¡å¤§å°å’Œå½¢çŠ¶çš„å¤šæ ·åŒ–æ•°æ®é›†æ—¶ã€‚é€šè¿‡è‡ªåŠ¨è°ƒæ•´ Anchorï¼ŒYOLOv5 å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œå‡å°‘å¯¹è¶…å‚æ•°çš„æ‰‹åŠ¨è°ƒæ•´éœ€æ±‚ï¼Œä»è€Œç®€åŒ–äº†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚

### 5.3.4 æºç 

é¦–å…ˆéœ€è¦å…ˆè®¡ç®—å½“å‰çš„ Anchor ä¸æ•°æ®é›†çš„é€‚åº”ç¨‹åº¦ã€‚

```python
@TryExcept(f"{PREFIX}ERROR")
def check_anchors(dataset, model, thr=4.0, imgsz=640):
    # å‡½æ•°ä½œç”¨ï¼šæ£€æŸ¥anchoræ˜¯å¦é€‚åˆæ•°æ®ï¼Œå¦‚æœ‰å¿…è¦ï¼Œåˆ™é‡æ–°è®¡ç®—anchor

    # ä»æ¨¡å‹ä¸­è·å–æ£€æµ‹å±‚ï¼ˆDetect()ï¼‰
    m = model.module.model[-1] if hasattr(model, "module") else model.model[-1]
    
    # è®¡ç®—è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸ç›¸å¯¹äºæœ€å¤§å°ºå¯¸çš„æ¯”ä¾‹
    shapes = imgsz * dataset.shapes / dataset.shapes.max(1, keepdims=True)

    # ç”Ÿæˆä¸€ä¸ªéšæœºçš„æ¯”ä¾‹å› å­ï¼Œç”¨äºæ‰©å¤§æˆ–ç¼©å°å›¾ç‰‡å°ºå¯¸
    scale = np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1))

    # è®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„å®½é«˜ï¼ˆwhï¼‰
    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(shapes * scale, dataset.labels)]))

    def metric(k):  # è®¡ç®—åº¦é‡å€¼
        # è®¡ç®—æ¯ä¸ªanchorä¸gt boxesçš„å®½é«˜æ¯”
        r = wh[:, None] / k[None]

        # è®¡ç®—æœ€å°æ¯”ç‡å’Œæœ€å¤§æ¯”ç‡
        x = torch.min(r, 1 / r).min(2)[0]

        # æ‰¾åˆ°æœ€å¤§æ¯”ç‡çš„anchor
        best = x.max(1)[0]

        # è®¡ç®—è¶…è¿‡é˜ˆå€¼ï¼ˆthrï¼‰çš„anchoræ•°é‡å æ¯”
        aat = (x > 1 / thr).float().sum(1).mean()

        # è®¡ç®—BPRï¼ˆbest possible recallï¼‰
        bpr = (best > 1 / thr).float().mean()

        return bpr, aat

    # è·å–æ¨¡å‹çš„æ­¥é•¿ï¼ˆstrideï¼‰
    stride = m.stride.to(m.anchors.device).view(-1, 1, 1)

    # è®¡ç®—å½“å‰çš„anchor
    anchors = m.anchors.clone() * stride
    
    # è®¡ç®—å½“å‰anchorä¸gt boxesçš„æ¯”å€¼ï¼Œå¹¶æ‰¾åˆ°æœ€ä½³æ¯”å€¼å’Œè¶…è¿‡é˜ˆå€¼çš„anchorå æ¯”
    bpr, aat = metric(anchors.cpu().view(-1, 2))
    s = f"\n{PREFIX}{aat:.2f} anchors/target, {bpr:.3f} Best Possible Recall (BPR). "
    
    # å¦‚æœæœ€ä½³æ¯”å€¼å¬å›ç‡å¤§äº0.98ï¼Œè¯´æ˜å½“å‰anchoré€‚åˆæ•°æ®é›†
    if bpr > 0.98:
        LOGGER.info(f"{s}Current anchors are a good fit to dataset âœ…")
    else:  # è¯´æ˜anchorä¸é€‚åˆæ•°æ®é›†ï¼Œéœ€è¦å°è¯•æ”¹è¿›
        LOGGER.info(f"{s}Anchors are a poor fit to dataset âš ï¸, attempting to improve...")

        # è®¡ç®—anchoræ•°é‡
        na = m.anchors.numel() // 2

        # ä½¿ç”¨k-meansèšç±»ç®—æ³•é‡æ–°è®¡ç®—anchor
        anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)

        # è®¡ç®—æ–°anchorçš„æœ€ä½³æ¯”å€¼å¬å›ç‡
        new_bpr = metric(anchors)[0]

        # å¦‚æœæ–°anchorçš„å¬å›ç‡æ¯”åŸæ¥çš„é«˜ï¼Œåˆ™æ›¿æ¢anchor
        if new_bpr > bpr:
            anchors = torch.tensor(anchors, device=m.anchors.device).type_as(m.anchors)
            m.anchors[:] = anchors.clone().view_as(m.anchors)

            # æ£€æŸ¥anchoré¡ºåºæ˜¯å¦æ­£ç¡®ï¼ˆå¿…é¡»åœ¨åƒç´ ç©ºé—´ï¼Œä¸èƒ½åœ¨ç½‘æ ¼ç©ºé—´ï¼‰
            check_anchor_order(m)
            m.anchors /= stride
            s = f"{PREFIX}Done
```

## 5.4 Hyper-parameter Evolution è¶…å‚æ•°è¿›åŒ–

è¶…å‚æ•°è¿›åŒ–ï¼ˆHyperparameter Evolutionï¼‰æ˜¯ä¸€ç§æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯ï¼Œå®ƒæ¶‰åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€åœ°è°ƒæ•´æ¨¡å‹çš„è¶…å‚æ•°ï¼ˆhyperparametersï¼‰ï¼Œä»¥æ‰¾åˆ°åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šæ€§èƒ½æœ€ä½³çš„å‚æ•°è®¾ç½®ã€‚è¿™äº›è¶…å‚æ•°æ˜¯æ¨¡å‹è®¾è®¡ä¸­çš„é«˜çº§è®¾ç½®ï¼Œå®ƒä»¬æ§åˆ¶æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œä½†ä¸ç›´æ¥ä½œä¸ºæ¨¡å‹è¾“å…¥çš„ä¸€éƒ¨åˆ†ã€‚å¸¸è§çš„è¶…å‚æ•°åŒ…æ‹¬å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€è¿­ä»£æ¬¡æ•°ã€æ­£åˆ™åŒ–å‚æ•°ã€Anchor å¤§å°ç­‰ã€‚

è¶…å‚æ•°è¿›åŒ–çš„ç›®æ ‡æ˜¯å‡å°‘è¶…å‚æ•°è°ƒæ•´çš„è¯•é”™è¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡ã€‚ä¼ ç»Ÿçš„è¶…å‚æ•°è°ƒæ•´æ–¹æ³•é€šå¸¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´è¶…å‚æ•°æˆ–ä½¿ç”¨ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰ç­‰æ–¹æ³•è¿›è¡Œå¤§é‡çš„å®éªŒæ¥æ‰¾åˆ°æœ€ä½³è®¾ç½®ã€‚è¿™äº›æ–¹æ³•æ—¢è€—æ—¶åˆå¯èƒ½æ— æ³•æ‰¾åˆ°æœ€ä¼˜è§£ã€‚

åœ¨ [ã€Šè¶…å‚æ•°æ¼”å˜ã€‹](https://docs.ultralytics.com/zh/yolov5/tutorials/hyperparameter_evolution/) è¿™ä¸€å®˜æ–¹æ–‡æ¡£ä¸­å¯¹å…¶è¿›è¡Œäº†ä»‹ç»ï¼š

è¶…å‚æ•°æ¼”åŒ–æ˜¯ä¸€ç§ä½¿ç”¨é—ä¼ ç®—æ³•ï¼ˆGAï¼‰è¿›è¡Œä¼˜åŒ–çš„è¶…å‚æ•°ä¼˜åŒ–æ–¹æ³•ã€‚

ML ä¸­çš„è¶…å‚æ•°æ§åˆ¶ç€è®­ç»ƒçš„å„ä¸ªæ–¹é¢ï¼Œè€Œä¸ºè¶…å‚æ•°å¯»æ‰¾æœ€ä½³å€¼æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ç½‘æ ¼æœç´¢ç­‰ä¼ ç»Ÿæ–¹æ³•å¾ˆå¿«å°±ä¼šå˜å¾—éš¾ä»¥å¤„ç†ï¼ŒåŸå› åœ¨äºï¼š1ï¼‰æœç´¢ç©ºé—´ç»´åº¦é«˜ï¼›2ï¼‰ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§æœªçŸ¥ï¼›3ï¼‰è¯„ä¼°æ¯ä¸ªç‚¹çš„é€‚é…æ€§æˆæœ¬é«˜æ˜‚ï¼Œå› æ­¤ GA æ˜¯è¶…å‚æ•°æœç´¢çš„åˆé€‚å€™é€‰æ–¹æ³•ã€‚

GA çš„æµç¨‹å¦‚ä¸‹ï¼š

<div align=center>
    <img src=./imgs_markdown/plots-GA.jpg
    width=80%>
    <center></center>
</div>

æˆ‘ä»¬çœ‹ä¸€ä¸‹å®˜æ–¹çš„ä»‹ç»ï¼š

### 5.4.1 åˆå§‹åŒ–è¶…å‚æ•°

YOLOv5 æœ‰å¤§çº¦ 30 ä¸ªè¶…å‚æ•°ï¼Œç”¨äºä¸åŒçš„è®­ç»ƒè®¾ç½®ã€‚è¿™äº›å‚æ•°åœ¨ `*.yaml` æ–‡ä»¶ä¸­çš„ `/data/hyps` ç›®å½•ã€‚æ›´å¥½çš„åˆå§‹çŒœæµ‹å°†äº§ç”Ÿæ›´å¥½çš„æœ€ç»ˆç»“æœï¼Œå› æ­¤åœ¨æ¼”åŒ–ä¹‹å‰æ­£ç¡®åˆå§‹åŒ–è¿™äº›å€¼éå¸¸é‡è¦ã€‚å¦‚æœæœ‰ç–‘é—®ï¼Œåªéœ€ä½¿ç”¨é»˜è®¤å€¼å³å¯ï¼Œè¿™äº›å€¼å·²é’ˆå¯¹ YOLOv5 COCO ä»å¤´å¼€å§‹çš„è®­ç»ƒè¿›è¡Œäº†ä¼˜åŒ–ã€‚

```yaml
# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
# Hyperparameters for low-augmentation COCO training from scratch

lr0: 0.01 # initial learning rate (SGD=1E-2, Adam=1E-3)
lrf: 0.01 # final OneCycleLR learning rate (lr0 * lrf)
momentum: 0.937 # SGD momentum/Adam beta1
weight_decay: 0.0005 # optimizer weight decay 5e-4
warmup_epochs: 3.0 # warmup epochs (fractions ok)
warmup_momentum: 0.8 # warmup initial momentum
warmup_bias_lr: 0.1 # warmup initial bias lr
box: 0.05 # box loss gain
cls: 0.5 # cls loss gain
cls_pw: 1.0 # cls BCELoss positive_weight
obj: 1.0 # obj loss gain (scale with pixels)
obj_pw: 1.0 # obj BCELoss positive_weight
iou_t: 0.20 # IoU training threshold
anchor_t: 4.0 # anchor-multiple threshold
# anchors: 3  # anchors per output layer (0 to ignore)
fl_gamma: 0.0 # focal loss gamma (efficientDet default gamma=1.5)
hsv_h: 0.015 # image HSV-Hue augmentation (fraction)
hsv_s: 0.7 # image HSV-Saturation augmentation (fraction)
hsv_v: 0.4 # image HSV-Value augmentation (fraction)
degrees: 0.0 # image rotation (+/- deg)
translate: 0.1 # image translation (+/- fraction)
scale: 0.5 # image scale (+/- gain)
shear: 0.0 # image shear (+/- deg)
perspective: 0.0 # image perspective (+/- fraction), range 0-0.001
flipud: 0.0 # image flip up-down (probability)
fliplr: 0.5 # image flip left-right (probability)
mosaic: 1.0 # image mosaic (probability)
mixup: 0.0 # image mixup (probability)
copy_paste: 0.0 # segment copy-paste (probability)
```

### 5.4.2 å®šä¹‰é€‚åº”åº¦ï¼ˆfitnessï¼‰

é€‚åº”åº¦æ˜¯æˆ‘ä»¬è¯•å›¾æœ€å¤§åŒ–çš„å€¼ã€‚åœ¨ YOLOv5 ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªé»˜è®¤çš„é€‚åº”åº¦å‡½æ•°ï¼Œå®ƒæ˜¯ä»¥ä¸‹æŒ‡æ ‡çš„åŠ æƒç»„åˆï¼š`mAP@0.5` è´¡çŒ®äº† 10% çš„æƒé‡ï¼Œè€Œ `mAP@0.5:0.95` è´¡çŒ®äº†å‰©ä½™çš„ 90%ï¼Œå…¶ä¸­ä¸åŒ…æ‹¬ Precision `P` å’Œ Recall `R`ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™äº›æŒ‡æ ‡ï¼Œæˆ–è€…ä½¿ç”¨ `utils/metrics.py` ä¸­çš„é»˜è®¤é€‚åº”åº¦å®šä¹‰ï¼ˆå»ºè®®ä½¿ç”¨ï¼‰ã€‚

```python
def fitness(x):
    # Model fitness as a weighted combination of metrics
    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]
    return (x[:, :4] * w).sum(1)
```

ç®€å•æ¥è¯´ï¼š

$$
\mathrm{fitness} = 0.1 \times \mathrm{mAP^{0.5}} + 0.9 \times \mathrm{mAP^{0.5:0.95}}
$$

### 5.4.3 è¿›åŒ–ï¼ˆEvolveï¼‰

è¿›åŒ–æ˜¯åŸºäºæˆ‘ä»¬å¯»æ±‚æ”¹è¿›çš„åŸºç¡€æƒ…æ™¯è¿›è¡Œçš„ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒåŸºç¡€æƒ…æ™¯æ˜¯åœ¨ COCO128 ä¸Šå¯¹é¢„è®­ç»ƒçš„ YOLOv5s è¿›è¡Œ 10 ä¸ªå‘¨æœŸçš„å¾®è°ƒã€‚åŸºç¡€æƒ…æ™¯çš„è®­ç»ƒå‘½ä»¤æ˜¯ï¼š

```bash
python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache
```

ä¸ºäº†é’ˆå¯¹è¿™ä¸ªæƒ…æ™¯è¿›åŒ–ç‰¹å®šçš„è¶…å‚æ•°ï¼Œä»æˆ‘ä»¬åœ¨ 5.4.1 ä¸­å®šä¹‰çš„åˆå§‹å€¼å¼€å§‹ï¼Œå¹¶æœ€å¤§åŒ–æˆ‘ä»¬åœ¨ 5.4.2 ä¸­å®šä¹‰çš„é€‚åº”åº¦ï¼Œè¯·åœ¨å‘½ä»¤è¡Œä¸­æ·»åŠ  `--evolve` å‚æ•°ï¼š

```bash
# Single-GPU
python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve

# Multi-GPU
for i in 0 1 2 3 4 5 6 7; do
  sleep $(expr 30 \* $i) &&  # 30-second delay (optional)
  echo 'Starting GPU '$i'...' &&
  nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --device $i --evolve > evolve_gpu_$i.log &
done

# Multi-GPU bash-while (not recommended)
for i in 0 1 2 3 4 5 6 7; do
  sleep $(expr 30 \* $i) &&  # 30-second delay (optional)
  echo 'Starting GPU '$i'...' &&
  "$(while true; do nohup python train.py... --device $i --evolve 1 > evolve_gpu_$i.log; done)" &
done
```

> ğŸ’¡ nohup å‘½ä»¤ï¼š`nohup` æ˜¯ä¸€ä¸ªåœ¨ Unix-like ç³»ç»Ÿä¸­å¸¸ç”¨çš„å‘½ä»¤ï¼Œç”¨äºåœ¨ç”¨æˆ·é€€å‡ºç™»å½•ä¼šè¯åç»§ç»­è¿è¡Œå‘½ä»¤ã€‚è¿™ä¸ªåå­—æ˜¯ "no hang up" çš„ç¼©å†™ï¼Œæ„å‘³ç€å³ä½¿ä¼šè¯æŒ‚èµ·ï¼ˆå³ç”¨æˆ·é€€å‡ºç™»å½•ï¼‰ï¼Œå‘½ä»¤ä¹Ÿä¼šç»§ç»­æ‰§è¡Œã€‚

é»˜è®¤çš„è¶…å‚æ•°è¿›åŒ–è®¾ç½®å°†è¿è¡ŒåŸºç¡€æƒ…æ™¯ 300 æ¬¡ï¼Œå³è¿›è¡Œ 300 ä»£è¿›åŒ–ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ `--evolve` å‚æ•°ä¿®æ”¹ä»£æ•°ï¼Œä¾‹å¦‚ `python train. py --evolve 1000`ã€‚

ä¸»è¦çš„é—ä¼ è¿ç®—ç¬¦æ˜¯äº¤å‰ï¼ˆcrossoverï¼‰å’Œå˜å¼‚ï¼ˆmutationï¼‰ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œå˜å¼‚è¢«ä½¿ç”¨ï¼Œå˜å¼‚æ¦‚ç‡ä¸º 80%ï¼Œæ–¹å·®ä¸º 0.04ï¼ŒåŸºäºæ‰€æœ‰ä¹‹å‰ä»£ä¸­æœ€ä½³çˆ¶æ¯ç»„åˆåˆ›å»ºæ–°çš„åä»£ã€‚ç»“æœè¢«è®°å½•åˆ° `runs/evolve/exp/evolve.csv`ï¼Œå¹¶ä¸”æ¯ä¸€ä»£ä¸­é€‚åº”åº¦æœ€é«˜çš„åä»£éƒ½è¢«ä¿å­˜ä¸º `runs/evolve/hyp_evolved.yaml`ï¼š

```yaml
# YOLOv5 Hyperparameter Evolution Results
# Best generation: 287
# Last generation: 300
#    metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss
#              0.54634,              0.55625,              0.58201,              0.33665,             0.056451,             0.042892,             0.013441

lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)
lrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)
momentum: 0.937  # SGD momentum/Adam beta1
weight_decay: 0.0005  # optimizer weight decay 5e-4
warmup_epochs: 3.0  # warmup epochs (fractions ok)
warmup_momentum: 0.8  # warmup initial momentum
warmup_bias_lr: 0.1  # warmup initial bias lr
box: 0.05  # box loss gain
cls: 0.5  # cls loss gain
cls_pw: 1.0  # cls BCELoss positive_weight
obj: 1.0  # obj loss gain (scale with pixels)
obj_pw: 1.0  # obj BCELoss positive_weight
iou_t: 0.20  # IoU training threshold
anchor_t: 4.0  # anchor-multiple threshold
# anchors: 3  # anchors per output layer (0 to ignore)
fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)
hsv_h: 0.015  # image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # image HSV-Value augmentation (fraction)
degrees: 0.0  # image rotation (+/- deg)
translate: 0.1  # image translation (+/- fraction)
scale: 0.5  # image scale (+/- gain)
shear: 0.0  # image shear (+/- deg)
perspective: 0.0  # image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # image flip up-down (probability)
fliplr: 0.5  # image flip left-right (probability)
mosaic: 1.0  # image mosaic (probability)
mixup: 0.0  # image mixup (probability)
copy_paste: 0.0  # segment copy-paste (probability)
```

æˆ‘ä»¬å»ºè®®è‡³å°‘è¿›è¡Œ 300 ä»£çš„è¿›åŒ–ä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚âš ï¸ è¯·æ³¨æ„ï¼Œè¿›åŒ–é€šå¸¸æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œå› ä¸ºåŸºç¡€æƒ…æ™¯éœ€è¦è®­ç»ƒæ•°ç™¾æ¬¡ï¼Œå¯èƒ½éœ€è¦æ•°ç™¾æˆ–æ•°åƒå°æ—¶çš„ GPU æ—¶é—´ã€‚

### 5.4.4 å¯è§†åŒ–ï¼ˆVisualizeï¼‰

`evolve.csv` åœ¨è¿›åŒ–å®Œæˆä¹‹åï¼Œç”± `utils.plots.plot_evolve()` ç»˜åˆ¶ä¸º `evolve.png`ï¼Œæ¯ä¸ªè¶…å‚æ•°éƒ½æœ‰ä¸€ä¸ªå­å›¾ï¼Œæ˜¾ç¤ºé€‚åº”åº¦ï¼ˆyè½´ï¼‰ä¸è¶…å‚æ•°å€¼ï¼ˆxè½´ï¼‰çš„å…³ç³»ã€‚é»„è‰²è¡¨ç¤ºæ›´é«˜çš„æµ“åº¦ã€‚å‚ç›´åˆ†å¸ƒè¡¨æ˜ä¸€ä¸ªå‚æ•°å·²è¢«ç¦ç”¨ä¸”ä¸ä¼šå˜å¼‚ã€‚è¿™å¯ä»¥åœ¨ `train.py` ä¸­çš„å…ƒå­—å…¸ä¸­é€‰æ‹©ï¼Œå¯¹äºå›ºå®šå‚æ•°å¹¶é˜²æ­¢å®ƒä»¬è¿›åŒ–çš„åœºæ™¯éå¸¸æœ‰ç”¨ã€‚

<div align=center>
    <img src=./imgs_markdown/2024-02-07-14-31-43.png
    width=100%>
    <center></center>
</div>

### 5.4.5 æºç 

```python
# Hyperparameter evolution metadata (including this hyperparameter True-False, lower_limit, upper_limit)
# è¶…å‚æ•°è¿›åŒ–metadataï¼ˆåŒ…æ‹¬æ­¤è¶…å‚æ•°æ˜¯å¦å‚ä¸è¿›åŒ–ï¼Œä¸‹é™ï¼Œä¸Šé™ï¼‰
meta = {
    "lr0": (False, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)
    "lrf": (False, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)
    "momentum": (False, 0.6, 0.98),  # SGD momentum/Adam beta1
    "weight_decay": (False, 0.0, 0.001),  # optimizer weight decay
    "warmup_epochs": (False, 0.0, 5.0),  # warmup epochs (fractions ok)
    "warmup_momentum": (False, 0.0, 0.95),  # warmup initial momentum
    "warmup_bias_lr": (False, 0.0, 0.2),  # warmup initial bias lr
    "box": (False, 0.02, 0.2),  # box loss gain
    "cls": (False, 0.2, 4.0),  # cls loss gain
    "cls_pw": (False, 0.5, 2.0),  # cls BCELoss positive_weight
    "obj": (False, 0.2, 4.0),  # obj loss gain (scale with pixels)
    "obj_pw": (False, 0.5, 2.0),  # obj BCELoss positive_weight
    "iou_t": (False, 0.1, 0.7),  # IoU training threshold
    "anchor_t": (False, 2.0, 8.0),  # anchor-multiple threshold
    "anchors": (False, 2.0, 10.0),  # anchors per output grid (0 to ignore)
    "fl_gamma": (False, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)
    "hsv_h": (True, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)
    "hsv_s": (True, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)
    "hsv_v": (True, 0.0, 0.9),  # image HSV-Value augmentation (fraction)
    "degrees": (True, 0.0, 45.0),  # image rotation (+/- deg)
    "translate": (True, 0.0, 0.9),  # image translation (+/- fraction)
    "scale": (True, 0.0, 0.9),  # image scale (+/- gain)
    "shear": (True, 0.0, 10.0),  # image shear (+/- deg)
    "perspective": (True, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001
    "flipud": (True, 0.0, 1.0),  # image flip up-down (probability)
    "fliplr": (True, 0.0, 1.0),  # image flip left-right (probability)
    "mosaic": (True, 0.0, 1.0),  # image mixup (probability)
    "mixup": (True, 0.0, 1.0),  # image mixup (probability)
    "copy_paste": (True, 0.0, 1.0),
}  # segment copy-paste (probability)

# GA configs
# é—ä¼ ç®—æ³•çš„é…ç½®
pop_size = 50  # # ç§ç¾¤å¤§å°

# å˜å¼‚ç‡çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
mutation_rate_min = 0.01
mutation_rate_max = 0.5

# äº¤å‰ç‡çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
crossover_rate_min = 0.5
crossover_rate_max = 1

# ç²¾è‹±å¤§å°ï¼ˆä¿ç•™çš„æœ€å¥½ä¸ªä½“æ•°é‡ï¼‰çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
min_elite_size = 2
max_elite_size = 5

# é”¦æ ‡èµ›å¤§å°ï¼ˆç”¨äºé€‰æ‹©çˆ¶ä»£çš„é€‰æ‹©æ± å¤§å°ï¼‰çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
tournament_size_min = 2
tournament_size_max = 10

with open(opt.hyp, errors="ignore") as f:
    hyp = yaml.safe_load(f)  # load hyps dict

    # å¦‚æœåœ¨.yamlæ–‡ä»¶ä¸­æ²¡æœ‰ anchors è¿™ä¸ªè¶…å‚æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬åŠ ä¸Š
    if "anchors" not in hyp:  # anchors commented in hyp.yaml
        hyp["anchors"] = 3

# ä¸ä½¿ç”¨AutoAnchors
if opt.noautoanchor:
    del hyp["anchors"], meta["anchors"]  # ä»GAç§ç¾¤ä¸­åˆ å»

# ä¿®æ”¹éƒ¨åˆ†å‚æ•°å€¼
opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # only val/save final epoch

# æ‹¼æ¥ä¿å­˜è·¯å¾„
evolve_yaml, evolve_csv = save_dir / "hyp_evolve.yaml", save_dir / "evolve.csv"

# Delete the items in meta dictionary whose first value is False
# åˆ é™¤å…ƒå­—å…¸ä¸­å…¶ç¬¬ä¸€ä¸ªå€¼ä¸º False çš„é¡¹ --> ä¸å‚ä¸è¿›åŒ–çš„å‚æ•°éƒ½åˆ æ‰
del_ = [item for item, value_ in meta.items() if value_[0] is False]

# åœ¨åˆ é™¤ä¹‹å‰å¤‡ä»½ä¸€ä¸‹
hyp_GA = hyp.copy()  # Make a copy of hyp dictionary

# å¼€å§‹åˆ é™¤ä¸å‚ä¸è¿›åŒ–çš„è¶…å‚æ•°
for item in del_:
    del meta[item]  # Remove the item from meta dictionary
    del hyp_GA[item]  # Remove the item from hyp_GA dictionary

# Set lower_limit and upper_limit arrays to hold the search space boundaries
# è®¾ç½® lower_limit å’Œ upper_limit æ•°ç»„ä»¥ä¿æŒæœç´¢ç©ºé—´çš„è¾¹ç•Œ
lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])
upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])

# Create gene_ranges list to hold the range of values for each gene in the population
# åˆ›å»º gene_ranges åˆ—è¡¨ä»¥æŒæœ‰ç§ç¾¤ä¸­æ¯ä¸ªåŸºå› å€¼çš„èŒƒå›´
gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]

# Initialize the population with initial_values or random values
# åˆå§‹åŒ–ç§ç¾¤ï¼Œä½¿ç”¨åˆå§‹å€¼æˆ–éšæœºå€¼
initial_values = []

# If resuming evolution from a previous checkpoint
# æ ¹æ®ä¹‹å‰çš„ ckpt ç»§ç»­è¿›åŒ–
if opt.resume_evolve is not None:
    assert os.path.isfile(ROOT / opt.resume_evolve), "evolve population path is wrong!"
    with open(ROOT / opt.resume_evolve, errors="ignore") as f:
        evolve_population = yaml.safe_load(f)
        for value in evolve_population.values():
            value = np.array([value[k] for k in hyp_GA.keys()])
            initial_values.append(list(value))

# If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population
# å¦‚æœä¸æ˜¯ä»ä¹‹å‰çš„ckptæ¢å¤ï¼Œåˆ™ä» opt.evolve_population ä¸­çš„ .yaml æ–‡ä»¶ç”Ÿæˆåˆå§‹å€¼
else:
    yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(".yaml")]
    for file_name in yaml_files:
        with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:
            value = yaml.safe_load(yaml_file)
            value = np.array([value[k] for k in hyp_GA.keys()])
            initial_values.append(list(value))

# Generate random values within the search space for the rest of the population
# ä¸ºç§ç¾¤ä¸­å‰©ä½™çš„éƒ¨åˆ†åœ¨æœç´¢ç©ºé—´å†…ç”Ÿæˆéšæœºå€¼
if initial_values is None:
    population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]
elif pop_size > 1:
    population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]
    for initial_value in initial_values:
        population = [initial_value] + population

# Run the genetic algorithm for a fixed number of generations
# å¯¹å›ºå®šçš„ä¸€ä»£æ•°è¿è¡Œé—ä¼ ç®—æ³•
list_keys = list(hyp_GA.keys())
for generation in range(opt.evolve):
    if generation >= 1:
        save_dict = {}
        for i in range(len(population)):
            little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}
            save_dict[f"gen{str(generation)}number{str(i)}"] = little_dict

        with open(save_dir / "evolve_population.yaml", "w") as outfile:
            yaml.dump(save_dict, outfile, default_flow_style=False)

    # Adaptive elite size
    # è‡ªé€‚åº”ç²¾è‹±çš„å¤§å°
    elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))

    # Evaluate the fitness of each individual in the population
    # è¯„ä¼°ç§ç¾¤ä¸­æ¯ä¸ªä¸ªä½“çš„é€‚åº”åº¦
    fitness_scores = []
    for individual in population:
        for key, value in zip(hyp_GA.keys(), individual):
            hyp_GA[key] = value
        hyp.update(hyp_GA)
        results = train(hyp.copy(), opt, device, callbacks)
        callbacks = Callbacks()
        # Write mutation results
        # å†™å…¥å˜å¼‚ç»“æœ
        keys = (
            "metrics/precision",
            "metrics/recall",
            "metrics/mAP_0.5",
            "metrics/mAP_0.5:0.95",
            "val/box_loss",
            "val/obj_loss",
            "val/cls_loss",
        )
        print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)
        fitness_scores.append(results[2])

    # Select the fittest individuals for reproduction using adaptive tournament selection
    # ä½¿ç”¨â€œè‡ªé€‚åº”é”¦æ ‡èµ›é€‰æ‹©â€é€‰æ‹©é€‚åº”åº¦æœ€é«˜çš„è¿›è¡Œç¹æ®–
    selected_indices = []
    for _ in range(pop_size - elite_size):
        # Adaptive tournament size
        # è‡ªé€‚åº”
        tournament_size = max(
            max(2, tournament_size_min),
            int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),
        )
        # Perform tournament selection to choose the best individual
        # æ‰§è¡Œé”¦æ ‡èµ›é€‰æ‹©ä»è€ŒæŒ‘é€‰å‡ºæœ€ä½³çš„ä¸ªä½“
        tournament_indices = random.sample(range(pop_size), tournament_size)
        tournament_fitness = [fitness_scores[j] for j in tournament_indices]
        winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]
        selected_indices.append(winner_index)

    # Add the elite individuals to the selected indices
    # å°†ç²¾è‹±ä¸ªä½“æ·»åŠ åˆ°é€‰å®šçš„ç´¢å¼•ä¸­
    elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]
    selected_indices.extend(elite_indices)

    # Create the next generation through crossover and mutation
    # é€šè¿‡äº¤å‰å’Œå˜å¼‚åˆ›é€ ä¸‹ä¸€ä»£
    next_generation = []
    for _ in range(pop_size):
        parent1_index = selected_indices[random.randint(0, pop_size - 1)]
        parent2_index = selected_indices[random.randint(0, pop_size - 1)]

        # Adaptive crossover rate
        # è‡ªé€‚åº”äº¤å‰ï¼ˆäº¤é…ï¼‰æ¯”ä¾‹
        crossover_rate = max(
            crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))
        )
        if random.uniform(0, 1) < crossover_rate:
            crossover_point = random.randint(1, len(hyp_GA) - 1)
            child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]
        else:
            child = population[parent1_index]

        # Adaptive mutation rate
        # è‡ªé€‚åº”å˜å¼‚æ¯”ä¾‹
        mutation_rate = max(
            mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))
        )
        for j in range(len(hyp_GA)):
            if random.uniform(0, 1) < mutation_rate:
                child[j] += random.uniform(-0.1, 0.1)
                child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])
        next_generation.append(child)

    # Replace the old population with the new generation
    # ç”¨æ–°ä¸€ä»£æ›¿æ¢æ—§ç§ç¾¤
    population = next_generation

# Print the best solution found
# æ‰“å°æ‰¾åˆ°çš„æœ€ä½³è§£å†³æ–¹æ¡ˆ
best_index = fitness_scores.index(max(fitness_scores))
best_individual = population[best_index]
print("Best solution found:", best_individual)

# Plot results
# ç»˜åˆ¶ç»“æœ
plot_evolve(evolve_csv)
LOGGER.info(
    f'Hyperparameter evolution finished {opt.evolve} generations\n'
    f"Results saved to {colorstr('bold', save_dir)}\n"
    f'Usage example: $ python train.py --hyp {evolve_yaml}'
)
```

## 5.5 Automatic mixed precision (AMP) training

### 5.5.1 å®šä¹‰

è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAutomatic Mixed Precision, AMPï¼‰è®­ç»ƒæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€æœ¯ï¼Œå®ƒå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€åœ°é€‰æ‹©ä½¿ç”¨æµ®ç‚¹æ•°çš„ç²¾åº¦ã€‚

è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæ ¹æ®è®¡ç®—çš„éœ€æ±‚å’Œæˆæœ¬ï¼Œè‡ªåŠ¨åœ°åœ¨å•ç²¾åº¦ï¼ˆFP32ï¼‰å’ŒåŠç²¾åº¦ï¼ˆFP16ï¼‰ä¹‹é—´åˆ‡æ¢ã€‚å…·ä½“æ¥è¯´ï¼ŒAMP è®­ç»ƒä¼šè¯†åˆ«é‚£äº›å¯¹ç²¾åº¦è¦æ±‚ä¸é«˜çš„è®¡ç®—ï¼ˆä¾‹å¦‚ï¼Œæƒé‡çŸ©é˜µçš„ä¹˜æ³•ï¼‰ï¼Œå¹¶å°†è¿™äº›è®¡ç®—è½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆFP16ï¼‰è®¡ç®—ï¼Œä»¥å‡å°‘æ¢¯åº¦è®¡ç®—ä¸­çš„æ•°å€¼è¯¯å·®ã€‚è€Œå¯¹äºé‚£äº›å¯¹ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„è®¡ç®—ï¼ˆä¾‹å¦‚ï¼Œæ¿€æ´»å‡½æ•°çš„è®¡ç®—ï¼‰ï¼ŒAMP è®­ç»ƒä»ç„¶ä½¿ç”¨å•ç²¾åº¦ï¼ˆFP32ï¼‰è®¡ç®—ï¼Œä»¥ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§å’Œå“åº”æ€§ã€‚

> - float32: å•ç²¾åº¦æµ®ç‚¹æ•°
> - float16: åŠç²¾åº¦æµ®ç‚¹æ•°
> - float64: åŒç²¾åº¦æµ®ç‚¹æ•°

### 5.5.2 AMP è®­ç»ƒçš„ä¼˜ç‚¹

1. **æé«˜è®­ç»ƒé€Ÿåº¦**ï¼šä½¿ç”¨åŒç²¾åº¦è¿›è¡ŒæŸäº›è®¡ç®—å¯ä»¥å‡å°‘æµ®ç‚¹è¿ç®—çš„æ¬¡æ•°ï¼Œä»è€Œæé«˜è®­ç»ƒé€Ÿåº¦ã€‚
2. **å‡å°‘å†…å­˜ä½¿ç”¨**ï¼šåŒç²¾åº¦é€šå¸¸éœ€è¦æ¯”å•ç²¾åº¦æ›´å¤šçš„å†…å­˜ï¼Œä½†åªåœ¨å¿…è¦æ—¶ä½¿ç”¨åŒç²¾åº¦ï¼Œå¯ä»¥å‡å°‘æ€»ä½“å†…å­˜ä½¿ç”¨ã€‚
3. **æé«˜æ•°å€¼ç¨³å®šæ€§**ï¼šåœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŒç²¾åº¦å¯ä»¥å‡å°‘æ¢¯åº¦æ›´æ–°çš„æ•°å€¼è¯¯å·®ï¼Œæé«˜æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§ã€‚

### 5.5.3 `torch.FloatTensor` å’Œ `torch.HalfTensor`

åœ¨ PyTorch ä¸­ï¼Œ`torch.FloatTensor` å’Œ `torch.HalfTensor` æ˜¯ä¸¤ç§ä¸åŒç²¾åº¦çš„æµ®ç‚¹å¼ é‡ç±»å‹ï¼Œå®ƒä»¬åˆ†åˆ«å¯¹åº”äºå•ç²¾åº¦ï¼ˆFP32ï¼‰å’ŒåŠç²¾åº¦ï¼ˆFP16ï¼‰æµ®ç‚¹æ•°ã€‚

- **torch.FloatTensor**ï¼šè¿™æ˜¯ PyTorch ä¸­çš„å•ç²¾åº¦æµ®ç‚¹å¼ é‡ã€‚å®ƒä½¿ç”¨ 32 ä½ï¼ˆ4 å­—èŠ‚ï¼‰æ¥å­˜å‚¨æ¯ä¸ªæµ®ç‚¹æ•°ï¼Œæä¾›äº†è¾ƒé«˜çš„æ•°å€¼ç²¾åº¦å’Œè¾ƒå¤§çš„æ•°å€¼èŒƒå›´ã€‚è¿™æ˜¯å¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­é»˜è®¤ä½¿ç”¨çš„æµ®ç‚¹ç±»å‹ã€‚
- **torch.HalfTensor**ï¼šè¿™æ˜¯ PyTorch ä¸­çš„åŠç²¾åº¦æµ®ç‚¹å¼ é‡ã€‚å®ƒä½¿ç”¨ 16 ä½ï¼ˆ2 å­—èŠ‚ï¼‰æ¥å­˜å‚¨æ¯ä¸ªæµ®ç‚¹æ•°ï¼Œæ•°å€¼èŒƒå›´å’Œç²¾åº¦éƒ½æ¯”å•ç²¾åº¦æµ®ç‚¹æ•°ä½ã€‚ç„¶è€Œï¼Œç”±äºåŠç²¾åº¦æµ®ç‚¹æ•°å ç”¨çš„å†…å­˜è¾ƒå°‘ï¼Œå› æ­¤åœ¨æŸäº›æƒ…å†µä¸‹ï¼ˆå¦‚å†…å­˜å—é™çš„ç¯å¢ƒæˆ–éœ€è¦å¤§å¹…æé«˜è®¡ç®—é€Ÿåº¦æ—¶ï¼‰ä¼šä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ã€‚

### 5.5.4 ä½œè€…ç­”ç–‘

åœ¨ [Automatic mixed precision (AMP) training is now natively supported and a stable feature. #557](https://github.com/ultralytics/yolov5/issues/557) æœ‰æåˆ° AMPã€‚

<div align=center>
    <img src=./imgs_markdown/2024-02-07-15-05-30.png
    width=100%>
    <center></center>
</div>

ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒğŸ’¡ YOLOv5 é»˜è®¤å¼€å¯ AMP è®­ç»ƒï¼Œå¹¶ä¸”ä¿å­˜çš„æ¨¡å‹ä¹Ÿæ˜¯ FP16 è€Œéä¼ ç»Ÿçš„ FP32ã€‚

### 5.5.5 å¦‚ä½•åœ¨ PyTorch ä¸­ä½¿ç”¨ AMPï¼Ÿ

> ğŸ’¡ æˆ‘ä¹‹å‰å†™è¿‡ç›¸å…³åšå®¢ï¼š[PyTorchæ··åˆç²¾åº¦åŸç†åŠå¦‚ä½•å¼€å¯è¯¥æ–¹æ³•](https://blog.csdn.net/weixin_44878336/article/details/125433023)

```python
from torch.cuda.amp import Scaler, autocast
```

> âš ï¸ æ³¨æ„ï¼š
> 1. Scaler å¹¶ä¸æ˜¯ AMPï¼Œautocast ä¹Ÿä¸æ˜¯ AMPï¼Œåªæœ‰ AMP + Scaler æ‰æ˜¯ AMP
>
> 2. AMP å¹¶ä¸ç‰¹æŒ‡åŠç²¾åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šä»»æ„ç²¾åº¦ï¼

#### 5.5.5.1 autocast

- ã€”å®˜æ–¹æ–‡æ¡£ã€•[torch.cuda.amp.autocast](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast)

ä½¿ç”¨ `torch.cuda.amp` æ¨¡å—ä¸­çš„ `autocast` ç±»ã€‚å½“è¿›å…¥ `autocast` ä¸Šä¸‹æ–‡åï¼Œæ”¯æŒ AMP çš„ CUDA ç®—å­ä¼šæŠŠ Tensor çš„ `dtype` è½¬æ¢ä¸º FP16ï¼Œä»è€Œåœ¨ä¸æŸå¤±è®­ç»ƒç²¾åº¦çš„æƒ…å†µä¸‹åŠ å¿«è¿ç®—ã€‚åˆšè¿›å…¥ `autocast` çš„ä¸Šä¸‹æ–‡æ—¶ï¼ŒTensor å¯ä»¥æ˜¯ä»»ä½•ç±»å‹ï¼Œä¸éœ€è¦åœ¨ `model` æˆ– `input` ä¸Šæ‰‹å·¥è°ƒç”¨ `.half()`ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åšï¼Œè¿™å°±æ˜¯ AMP ä¸­çš„ Automaticã€‚

å¦å¤–éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`autocast` ä¸Šä¸‹æ–‡åº”è¯¥åªåŒ…å«ç½‘ç»œçš„å‰å‘æ¨ç†è¿‡ç¨‹ï¼ˆåŒ…æ‹¬ loss çš„è®¡ç®—ï¼‰ï¼Œâš ï¸ ä¸è¦åŒ…å«åå‘ä¼ æ’­ï¼Œå› ä¸º BP çš„ç®—å­ä¼šä½¿ç”¨å’Œå‰å‘ç®—å­ç›¸åŒçš„ç±»å‹ã€‚

---

æˆ‘ä»¬çœ‹ä¸€ä¸‹æºç ï¼š

```python
class torch.autocast(device_type, 
                     dtype=None, 
                     enabled=True, 
                     cache_enabled=None)
```

**å‚æ•°**ï¼š

- `device_type`ï¼ˆstrï¼Œå¿…éœ€ï¼‰ - è¦ä½¿ç”¨çš„è®¾å¤‡ç±»å‹ã€‚å¯èƒ½çš„å€¼æœ‰ï¼š'cuda'ï¼Œ'cpu'ï¼Œ'xpu' å’Œ 'hpu'ã€‚ç±»å‹ä¸ `torch.device` çš„ `type` å±æ€§ç›¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Tensor.device.type` è·å–å¼ é‡çš„è®¾å¤‡ç±»å‹ã€‚
- `enabled`ï¼ˆboolï¼Œå¯é€‰ï¼‰ - åŒºåŸŸå†…æ˜¯å¦åº”å¯ç”¨ autocastã€‚é»˜è®¤å€¼ï¼šTrue
- `dtype`ï¼ˆtorch_dtypeï¼Œå¯é€‰ï¼‰ - æ˜¯å¦ä½¿ç”¨ `torch.float16` æˆ– `torch.bfloat16`ã€‚
- `cache_enabled`ï¼ˆboolï¼Œå¯é€‰ï¼‰ - æ˜¯å¦åº”å¯ç”¨ autocast å†…éƒ¨çš„æƒé‡ç¼“å­˜ã€‚é»˜è®¤å€¼ï¼šTrue

> âš ï¸ autocast åªæ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä¼šæŠŠåœ¨å®ƒèŒƒå›´å†…çš„ Tensor çš„æ•°æ®èŒƒå›´éƒ½ç»Ÿä¸€ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¿®æ”¹ `dtype` å‚æ•°æ¥å®ç°ä¸åŒç²¾åº¦çš„è®¡ç®—ï¼Œæ¯”å¦‚ `dtype=torch.float32, int8, ...`


`autocast` çš„å®ä¾‹å¯ç”¨ä½œä¸Šä¸‹æ–‡ç®¡ç†å™¨æˆ–è£…é¥°å™¨ï¼Œå…è®¸è„šæœ¬çš„æŸäº›åŒºåŸŸä»¥æ··åˆç²¾åº¦è¿è¡Œã€‚

åœ¨è¿™äº›åŒºåŸŸä¸­ï¼Œæ“ä½œä»¥ `autocast` é€‰æ‹©çš„ä¸æ“ä½œç‰¹å®šçš„ `dtype` è¿è¡Œï¼Œä»¥æé«˜æ€§èƒ½åŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚

åœ¨è¿›å…¥å¯ç”¨ `autocast` çš„åŒºåŸŸæ—¶ï¼Œå¼ é‡å¯ä»¥æ˜¯ä»»ä½•ç±»å‹ã€‚åœ¨ä½¿ç”¨ autocasting æ—¶ï¼Œä¸åº”åœ¨æ¨¡å‹æˆ–è¾“å…¥ä¸Šè°ƒç”¨ `half()` æˆ– `bfloat16()`ã€‚

`autocast` åº”è¯¥ä»…åŒ…è£…ç½‘ç»œçš„å‰å‘æ¨ç†ï¼ŒåŒ…æ‹¬æŸå¤±è®¡ç®—ã€‚âš ï¸ ä¸å»ºè®®åœ¨ autocast ä¸‹æ‰§è¡Œåå‘ä¼ é€’ã€‚åå‘æ“ä½œåœ¨ä¸ autocast ç”¨äºç›¸åº”å‰å‘æ¨ç†çš„ç›¸åŒç±»å‹ä¸­è¿è¡Œã€‚

---

**CUDA è®¾å¤‡çš„ç¤ºä¾‹**ï¼š

```python
# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

for input, target in data:
    optimizer.zero_grad()

    # Enables autocasting for the forward pass (model + loss)
    # å¯ç”¨å‰å‘æ¨ç†ï¼ˆæ¨¡å‹ + æŸå¤±ï¼‰çš„ autocastã€‚
    with torch.autocast(device_type="cuda"):
        output = model(input)
        loss = loss_fn(output, target)

    # Exits the context manager before backward()
    # åœ¨è°ƒç”¨backward()ä¹‹å‰é€€å‡ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚
    loss.backward()
    optimizer.step()
```

`autocast` ä¹Ÿå¯ä»¥ä½œä¸ºè£…é¥°å™¨ä½¿ç”¨ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨åœ¨æ¨¡å‹çš„ `forward` æ–¹æ³•ä¸Šï¼š

```python
class AutocastModel(nn.Module):
    ...
    @torch.autocast(device_type="cuda")
    def forward(self, input):
        ...
```

åœ¨å¯ç”¨äº† `autocast` çš„åŒºåŸŸä¸­äº§ç”Ÿçš„æµ®ç‚¹å¼ é‡å¯èƒ½æ˜¯ `float16`ï¼ˆé»˜è®¤å°±æ˜¯ FP16ï¼‰ã€‚åœ¨è¿”å›åˆ°ç¦ç”¨ `autocast` çš„åŒºåŸŸåï¼Œå°†å…¶ä¸ä¸åŒ `dtype` çš„æµ®ç‚¹å¼ é‡ä¸€èµ·ä½¿ç”¨å¯èƒ½å¯¼è‡´ç±»å‹ä¸åŒ¹é…é”™è¯¯ã€‚å¦‚æœå‡ºç°æ­¤æƒ…å†µï¼Œè¯·å°†åœ¨ `autocast` åŒºåŸŸä¸­ç”Ÿæˆçš„å¼ é‡è½¬å›ä¸º `float32`ï¼ˆæˆ–å…¶ä»–æ‰€éœ€çš„ `dtype`ï¼‰ã€‚å¦‚æœ `autocast` åŒºåŸŸçš„å¼ é‡å·²ç»æ˜¯ `float32`ï¼Œåˆ™è½¬æ¢æ˜¯ä¸€ä¸ªæ— æ“ä½œï¼Œå¹¶ä¸”ä¸ä¼šäº§ç”Ÿé¢å¤–å¼€é”€ã€‚

---

**CUDA ç¤ºä¾‹**ï¼š

```python
# Creates some tensors in default dtype (here assumed to be float32)
a_float32 = torch.rand((8, 8), device="cuda")
b_float32 = torch.rand((8, 8), device="cuda")
c_float32 = torch.rand((8, 8), device="cuda")
d_float32 = torch.rand((8, 8), device="cuda")

with torch.autocast(device_type="cuda"):
    # torch.mm is on autocast's list of ops that should run in float16.
    # torch.mm åœ¨ autocast çš„æ“ä½œåˆ—è¡¨ä¸­ï¼Œåº”è¯¥åœ¨ float16 ä¸­è¿è¡Œ
    # Inputs are float32, but the op runs in float16 and produces float16 output.
    # è¾“å…¥æ˜¯ float32ï¼Œä½†æ“ä½œåœ¨ float16 ä¸­è¿è¡Œï¼Œå¹¶ç”Ÿæˆ float16 çš„è¾“å‡º
    # No manual casts are required.
    # æ— éœ€æ‰‹åŠ¨è¿›è¡Œç±»å‹è½¬æ¢ã€‚
    e_float16 = torch.mm(a_float32, b_float32)

    # Also handles mixed input types
    # è¿˜å¤„ç†æ··åˆè¾“å…¥ç±»å‹
    f_float16 = torch.mm(d_float32, e_float16)

# After exiting autocast, calls f_float16.float() to use with d_float32
# åœ¨é€€å‡º autocast åï¼Œè°ƒç”¨ f_float16.float() ä»¥ä¸ d_float32 ä¸€èµ·ä½¿ç”¨
g_float32 = torch.mm(d_float32, f_float16.float())  # é€šè¿‡ .float() å°† FP16 è½¬æ¢ä¸ºäº† FP32
```

---

**CPU è®­ç»ƒç¤ºä¾‹**ï¼š

```python
# Creates model and optimizer in default precision
model = Net()
optimizer = optim.SGD(model.parameters(), ...)

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()

        # Runs the forward pass with autocasting.
        with torch.autocast(device_type="cpu", dtype=torch.bfloat16):
            output = model(input)
            loss = loss_fn(output, target)

        loss.backward()
        optimizer.step()
```

#### 5.5.5.2 GradScaler

- ã€”å®˜æ–¹æ–‡æ¡£ã€•[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)

ä½¿ç”¨ `torch.cuda.amp.GradScaler`ï¼Œéœ€è¦åœ¨è®­ç»ƒæœ€å¼€å§‹ä¹‹å‰å®ä¾‹åŒ–ä¸€ä¸ª `GradScaler` å¯¹è±¡ã€‚é€šè¿‡æ”¾å¤§ Loss çš„å€¼ï¼Œä»è€Œé˜²æ­¢æ¢¯åº¦çš„ underflowï¼ˆâš ï¸ è¿™åªæ˜¯ BP çš„æ—¶å€™ä¼ é€’æ¢¯åº¦ä¿¡æ¯ä½¿ç”¨ï¼ŒçœŸæ­£æ›´æ–°æƒé‡çš„æ—¶å€™è¿˜æ˜¯è¦æŠŠæ”¾å¤§çš„æ¢¯åº¦å† unscale å›å»ï¼‰

---

æˆ‘ä»¬çœ‹ä¸€ä¸‹å®ƒçš„æºç ï¼š

```python
class torch.cuda.amp.GradScaler(init_scale=65536.0, 
                                growth_factor=2.0, 
                                backoff_factor=0.5, 
                                growth_interval=2000, 
                                enabled=True)
```

**å‚æ•°**ï¼š

- `init_scale`ï¼ˆfloatï¼Œå¯é€‰ï¼Œé»˜è®¤ä¸º 2.0**16ï¼‰ - åˆå§‹ç¼©æ”¾å› å­ã€‚
- `growth_factor`ï¼ˆfloatï¼Œå¯é€‰ï¼Œé»˜è®¤ä¸º 2.0ï¼‰ - å¦‚æœåœ¨ `growth_interval` è¿ç»­çš„è¿­ä»£ä¸­æ²¡æœ‰å‡ºç° inf/NaN æ¢¯åº¦ï¼Œåˆ™åœ¨ `update()` æœŸé—´å°†ç¼©æ”¾ä¹˜ä»¥æ­¤å› å­ â€”â€” **ç›®çš„æ˜¯å°½æœ€å¤§å¯èƒ½å°†ç¼©æ”¾å› å­å˜å¤§**ã€‚
- `backoff_factor`ï¼ˆfloatï¼Œå¯é€‰ï¼Œé»˜è®¤ä¸º 0.5ï¼‰ - å¦‚æœåœ¨è¿­ä»£ä¸­å‡ºç° inf/NaN æ¢¯åº¦ï¼Œåˆ™åœ¨ `update()` æœŸé—´å°†ç¼©æ”¾ä¹˜ä»¥æ­¤å› å­ â€”â€” å‡å°ç¼©æ”¾å› å­é¿å…æ¨¡å‹æ— æ³•è®­ç»ƒã€‚
- `growth_interval`ï¼ˆintï¼Œå¯é€‰ï¼Œé»˜è®¤ä¸º 2000ï¼‰ - å¿…é¡»åœ¨æ²¡æœ‰ inf/NaN æ¢¯åº¦çš„è¿ç»­è¿­ä»£ä¸­å‘ç”Ÿçš„æ¬¡æ•°ï¼Œä»¥ä¾¿é€šè¿‡ `growth_factor` å°†ç¼©æ”¾ä¹˜ä»¥æ­¤å› å­ â€”â€” åœ¨ `growth_interval` æ¬¡è¿­ä»£ä¸­éƒ½æ²¡æœ‰å‡ºç° inf/NaN ç°è±¡ï¼Œå°±è¦æ”¾å¤§ç¼©æ”¾å› å­äº†ã€‚
- `enabled`ï¼ˆboolï¼Œå¯é€‰ï¼‰ - å¦‚æœä¸º Falseï¼Œåˆ™ç¦ç”¨æ¢¯åº¦ç¼©æ”¾ã€‚`step()` ç®€å•åœ°è°ƒç”¨åº•å±‚çš„ `optimizer.step()`ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™æˆä¸ºæ— æ“ä½œã€‚é»˜è®¤å€¼ï¼šTrue â€”â€” æé«˜å…¼å®¹æ€§ç”¨çš„

**æ–¹æ³•**ï¼š

- `scaler.scale(loss)` å°†ç»™å®šçš„æŸå¤±ä¹˜ä»¥ç¼©æ”¾å™¨å½“å‰çš„ç¼©æ”¾å› å­ã€‚
- `scaler.step(optimizer)` å®‰å…¨åœ°å–æ¶ˆç¼©æ”¾æ¢¯åº¦å¹¶è°ƒç”¨ `optimizer.step()`ã€‚
- `scaler.update()` æ›´æ–°ç¼©æ”¾å™¨çš„ç¼©æ”¾å› å­ã€‚

âš ï¸ ç¼©æ”¾å› å­é€šå¸¸ä¼šå¯¼è‡´åœ¨å‰å‡ æ¬¡è¿­ä»£ä¸­æ¢¯åº¦ä¸­å‡ºç° infs/NaNsï¼Œå› ä¸ºå…¶å€¼è¿›è¡Œæ ¡å‡†ã€‚å¯¹äºè¿™äº›è¿­ä»£ï¼Œ`scaler.step` å°†è·³è¿‡åº•å±‚çš„ `optimizer.step()`ã€‚ä¹‹åï¼Œè·³è¿‡æ­¥éª¤åº”è¯¥å¾ˆå°‘å‘ç”Ÿï¼ˆæ¯å‡ ç™¾æˆ–å‡ åƒæ¬¡è¿­ä»£ä¸€æ¬¡ï¼‰ã€‚

#### 5.5.5.3 ç¤ºä¾‹

##### 1. å…¸å‹çš„æ··åˆç²¾åº¦è®­ç»ƒ

```python
# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

# åœ¨è®­ç»ƒå¼€å§‹æ—¶åˆ›å»ºä¸€ä¸ª GradScaler å®ä¾‹
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()  # æ¸…ç©ºå†å²æ¢¯åº¦

        # ä½¿ç”¨ autocast è¿è¡Œå‰å‘æ¨ç†
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # ç¼©æ”¾æŸå¤±ã€‚å¯¹ç¼©æ”¾åçš„æŸå¤±è°ƒç”¨ backward() ä»¥åˆ›å»ºç¼©æ”¾åçš„æ¢¯åº¦ã€‚
        # âš ï¸ åœ¨ autocast ä¸‹æ‰§è¡Œåå‘ä¼ é€’æ˜¯ä¸æ¨èçš„
        # åœ¨ autocast é€‰æ‹©çš„ç›¸åº”å‰å‘æ¨ç†çš„ dtype ä¸­è¿è¡Œåå‘æ“ä½œ
        scaler.scale(loss).backward()

        # scaler.step() é¦–å…ˆå–æ¶ˆä¼˜åŒ–å™¨çš„åˆ†é…å‚æ•°çš„æ¢¯åº¦çš„ç¼©æ”¾ï¼ˆä»F32å˜ä¸ºF16ï¼‰
        # å¦‚æœè¿™äº›æ¢¯åº¦ä¸åŒ…å«æ— ç©·å¤§æˆ– NaNï¼Œç„¶åè°ƒç”¨ optimizer.step()
        # å¦åˆ™ï¼Œè·³è¿‡ optimizer.step()
        scaler.step(optimizer)

        # æ›´æ–°ä¸‹ä¸€æ¬¡è¿­ä»£çš„ç¼©æ”¾å› å­
        scaler.update()
```

##### 2. æ¢¯åº¦ç´¯ç§¯

æ¢¯åº¦ç´¯ç§¯ä¼šå°†ä¸€ä¸ªæœ‰æ•ˆ Batch å¤§å°ï¼ˆ`batch_per_iter * iters_to_accumulate` * `num_procs`ï¼‰å†…çš„æ¢¯åº¦ç›¸åŠ ã€‚ç¼©æ”¾åº”è¯¥æ ¹æ®æœ‰æ•ˆ Batch è¿›è¡Œæ ¡å‡†ï¼Œè¿™æ„å‘³ç€åœ¨æœ‰æ•ˆ Batch ç²’åº¦ä¸Šè¿›è¡Œ inf/NaN æ£€æŸ¥ã€å¦‚æœå‘ç° inf/NaN æ¢¯åº¦åˆ™è·³è¿‡æ­¥éª¤ï¼Œä»¥åŠåœ¨æœ‰æ•ˆ Batch ä¸Šæ›´æ–°ç¼©æ”¾å› å­ã€‚è€Œåœ¨ç»™å®šæœ‰æ•ˆ Batch ç´¯ç§¯æ¢¯åº¦æœŸé—´ï¼Œæ¢¯åº¦åº”è¯¥ä¿æŒç¼©æ”¾ï¼Œç¼©æ”¾å› å­åº”è¯¥ä¿æŒä¸å˜ã€‚å¦‚æœåœ¨ç´¯ç§¯å®Œæˆä¹‹å‰æ¢¯åº¦è¢«å–æ¶ˆç¼©æ”¾ï¼ˆæˆ–ç¼©æ”¾å› å­å‘ç”Ÿå˜åŒ–ï¼‰ï¼Œé‚£ä¹ˆä¸‹ä¸€æ¬¡åå‘ä¼ é€’å°†ä¼šå°†ç¼©æ”¾æ¢¯åº¦æ·»åŠ åˆ°æœªç¼©æ”¾æ¢¯åº¦ä¸­ï¼ˆæˆ–ç”¨ä¸åŒå› å­ç¼©æ”¾çš„æ¢¯åº¦ï¼‰ï¼Œä¹‹åå°±æ— æ³•æ¢å¤ç´¯ç§¯çš„æœªç¼©æ”¾æ¢¯åº¦ï¼Œæ­¥éª¤å¿…é¡»åº”ç”¨ã€‚

å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦å–æ¶ˆç¼©æ”¾æ¢¯åº¦ï¼ˆä¾‹å¦‚ï¼Œå…è®¸å‰ªåˆ‡æœªç¼©æ”¾æ¢¯åº¦ï¼‰ï¼Œè¯·åœ¨æ‰§è¡Œæ­¥éª¤ä¹‹å‰ï¼Œåœ¨å³å°†åˆ°æ¥çš„æ­¥éª¤çš„æ‰€æœ‰ï¼ˆç¼©æ”¾çš„ï¼‰æ¢¯åº¦è¢«ç´¯ç§¯åè°ƒç”¨ `unscale_`ã€‚å¹¶ä¸”**åªæœ‰åœ¨ä¸ºä¸€ä¸ªå®Œæ•´çš„æœ‰æ•ˆ Batch è°ƒç”¨äº†æ­¥éª¤çš„è¿­ä»£ç»“æŸæ—¶**æ‰è°ƒç”¨ `update`ï¼š

```python
scaler = GradScaler()

for epoch in epochs:
    for i, (input, target) in enumerate(data):
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)
            loss = loss / iters_to_accumulate

        # ç´¯ç§¯ç¼©æ”¾çš„æ¢¯åº¦
        scaler.scale(loss).backward()

        if (i + 1) % iters_to_accumulate == 0:
            # åœ¨è¿™é‡Œå¯ä»¥ä½¿ç”¨ unscale_ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼Œä¾‹å¦‚ï¼Œå…è®¸å‰ªåˆ‡æœªç¼©æ”¾çš„æ¢¯åº¦
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶éœ€è¦æ”¾åœ¨æœ€åäº†ï¼Œä¸ç„¶æ¢¯åº¦æ²¡æ³•ç´¯ç§¯çš„
```

##### 3. å¤„ç†å¤šä¸ªæ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨

å¦‚æœæˆ‘ä»¬çš„ç½‘ç»œæœ‰å¤šä¸ªæŸå¤±ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹æ¯ä¸ªæŸå¤±åˆ†åˆ«è°ƒç”¨ `scaler.scale`ã€‚å¦‚æœæˆ‘ä»¬çš„ç½‘ç»œæœ‰å¤šä¸ªä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«å¯¹æ¯ä¸ªä¼˜åŒ–å™¨è°ƒç”¨ `scaler.unscale_`ï¼Œå¹¶ä¸”å¿…é¡»å¯¹æ¯ä¸ªä¼˜åŒ–å™¨åˆ†åˆ«è°ƒç”¨ `scaler.step`ã€‚

ç„¶è€Œï¼Œâš ï¸ `scaler.update` åªåº”åœ¨æ­¤è¿­ä»£ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä¼˜åŒ–å™¨éƒ½å·²æ‰§è¡Œæ­¥éª¤ä¹‹åè°ƒç”¨ä¸€æ¬¡ï¼š

```python
scaler = torch.cuda.amp.GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer0.zero_grad()
        optimizer1.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output0 = model0(input)  # ç¬¬ä¸€ä¸ªæ¨¡å‹
            output1 = model1(input)  # ç¬¬äºŒä¸ªæ¨¡å‹
            loss0 = loss_fn(2 * output0 + 3 * output1, target)  # æ··åˆæŸå¤±1
            loss1 = loss_fn(3 * output0 - 5 * output1, target)  # æ··åˆæŸå¤±2

        # è¿™é‡Œçš„ retain_graph ä¸ amp æ— å…³ï¼Œå®ƒå­˜åœ¨æ˜¯å› ä¸ºåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œ
        # ä¸¤ä¸ª backward() è°ƒç”¨å…±äº«äº†ä¸€äº›å›¾çš„éƒ¨åˆ†
        scaler.scale(loss0).backward(retain_graph=True)
        scaler.scale(loss1).backward()

        # æˆ‘ä»¬å¯ä»¥é€‰æ‹©å“ªäº›ä¼˜åŒ–å™¨æ¥æ”¶æ˜¾å¼å–æ¶ˆç¼©æ”¾ï¼Œ
        # ä»¥ä¾¿æ£€æŸ¥æˆ–ä¿®æ”¹å®ƒä»¬æ‹¥æœ‰çš„å‚æ•°çš„æ¢¯åº¦ã€‚
        scaler.unscale_(optimizer0)

        scaler.step(optimizer0)
        scaler.step(optimizer1)

        scaler.update()
```

> âš ï¸ æ¯ä¸ªä¼˜åŒ–å™¨éƒ½ä¼šæ£€æŸ¥å…¶æ¢¯åº¦ä¸­æ˜¯å¦åŒ…å« inf/NaNï¼Œå¹¶ç‹¬ç«‹å†³å®šæ˜¯å¦è·³è¿‡è¯¥æ­¥éª¤ã€‚è¿™å¯èƒ½å¯¼è‡´ä¸€ä¸ªä¼˜åŒ–å™¨è·³è¿‡è¯¥æ­¥éª¤ï¼Œè€Œå¦ä¸€ä¸ªä¸è·³è¿‡ã€‚ç”±äºæ­¥éª¤è·³è¿‡å¾ˆå°‘å‘ç”Ÿï¼ˆæ¯å‡ ç™¾æ¬¡è¿­ä»£ä¸€æ¬¡ï¼‰ï¼Œè¿™ä¸åº”å½±å“æ”¶æ•›æ€§ã€‚

##### 4. DataParallel (DP) in a single process

å³ä½¿ `torch.nn.DataParallel` ç”Ÿæˆçº¿ç¨‹æ¥åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šè¿è¡Œå‰å‘æ¨ç†ï¼Œautocast çŠ¶æ€ä¹Ÿä¼šåœ¨æ¯ä¸ªçº¿ç¨‹ä¸­ä¼ æ’­ï¼Œä»¥ä¸‹æ“ä½œå°†èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼š

```python
model = MyModel()
dp_model = nn.DataParallel(model)

# åœ¨ä¸»çº¿ç¨‹ä¸­è®¾ç½® autocast
with autocast(device_type='cuda', dtype=torch.float16):
    # dp_model å†…éƒ¨çš„çº¿ç¨‹å°†ä½¿ç”¨ autocastã€‚
    output = dp_model(input)
    # loss_fn ä¹Ÿä½¿ç”¨ autocast
    loss = loss_fn(output)
```

##### 5. DistributedDataParallel (DDP), å•å¡å•çº¿ç¨‹

`torch.nn.parallel.DistributedDataParallel` çš„æ–‡æ¡£å»ºè®®æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ä¸€ä¸ª GPU ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`DistributedDataParallel` ä¸ä¼šåœ¨å†…éƒ¨ç”Ÿæˆçº¿ç¨‹ï¼Œå› æ­¤å¯¹ autocast å’Œ GradScaler çš„ä½¿ç”¨ä¸å—å½±å“ã€‚

##### 6. DistributedDataParallel (DDP), å¤šå¡å¤šçº¿ç¨‹

åœ¨è¿™é‡Œï¼Œ`torch.nn.parallel.DistributedDataParallel` å¯èƒ½ä¼šç”Ÿæˆä¸€ä¸ªè¾…åŠ©çº¿ç¨‹æ¥åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šè¿è¡Œå‰å‘æ¨ç†ï¼Œç±»ä¼¼äº `torch.nn.DataParallel`ã€‚

è§£å†³æ–¹æ³•æ˜¯ç›¸åŒçš„ï¼šåœ¨æ¨¡å‹çš„å‰å‘æ–¹æ³•ä¸­åº”ç”¨ autocastï¼Œä»¥ç¡®ä¿å®ƒåœ¨è¾…åŠ©çº¿ç¨‹ä¸­å¯ç”¨ã€‚

## 5.6 æ–­ç‚¹ç»­è®­

æ–­ç‚¹ç»­è®­ï¼ˆResume Trainingï¼‰æ˜¯æœºå™¨å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªåŠŸèƒ½ï¼Œå®ƒå…è®¸æ¨¡å‹è®­ç»ƒåœ¨ä¹‹å‰åœæ­¢çš„åœ°æ–¹ç»§ç»­è¿›è¡Œã€‚è¿™å¯¹äºå¤„ç†å¤§å‹æ•°æ®é›†æˆ–éœ€è¦é•¿æ—¶é—´è®­ç»ƒçš„æ¨¡å‹å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šç”±äºå„ç§åŸå› ï¼ˆå¦‚ç¡¬ä»¶æ•…éšœã€ç”µåŠ›ä¸­æ–­ç­‰ï¼‰å¯¼è‡´è®­ç»ƒè¿‡ç¨‹æ„å¤–åœæ­¢ã€‚

åœ¨ YOLOv5 ä¸­å®ç°æ–­ç‚¹ç»­è®­é€šå¸¸æ¶‰åŠä»¥ä¸‹æ­¥éª¤ï¼š

1. **ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šå®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œè¿™äº›æ£€æŸ¥ç‚¹åŒ…å«äº†æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠå½“å‰çš„è®­ç»ƒè½®æ¬¡ç­‰ä¿¡æ¯ã€‚
2. **ä¸­æ–­è®­ç»ƒ**ï¼šå¦‚æœè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°äº†ä¸­æ–­ï¼Œç³»ç»Ÿä¼šåœæ­¢æ›´æ–°è¿™äº›æ£€æŸ¥ç‚¹ã€‚
3. **æ¢å¤è®­ç»ƒ**ï¼šè¦æ¢å¤è®­ç»ƒï¼Œç”¨æˆ·éœ€è¦æŒ‡å®šä¸Šæ¬¡ä¿å­˜çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ã€‚YOLOv5 è®­ç»ƒè„šæœ¬é€šå¸¸ä¼šæœ‰ä¸€ä¸ª `--resume` å‚æ•°ï¼Œé€šè¿‡è®¾ç½®è¿™ä¸ªå‚æ•°ï¼Œå¯ä»¥ä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹å¼€å§‹ç»§ç»­è®­ç»ƒã€‚
4. **è®¾ç½®**ï¼šåœ¨æ¢å¤è®­ç»ƒä¹‹å‰ï¼Œç¡®ä¿è®­ç»ƒçš„è®¾ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€æ•°æ®é›†ç­‰ï¼‰ä¸ä¹‹å‰è®­ç»ƒçš„è®¾ç½®ä¿æŒä¸€è‡´ï¼Œä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„è¿ç»­æ€§å’Œç¨³å®šæ€§ã€‚
5. **ç»§ç»­è®­ç»ƒ**ï¼šå¯åŠ¨è®­ç»ƒè„šæœ¬ï¼Œç¨‹åºä¼šåŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¹¶ä»åœæ­¢çš„åœ°æ–¹å¼€å§‹ç»§ç»­è®­ç»ƒæ¨¡å‹ã€‚

æ–­ç‚¹ç»­è®­ä¸ä»…èƒ½å¤Ÿå¸®åŠ©èŠ‚çœæ—¶é—´ï¼Œé¿å…ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè¿˜èƒ½å¤Ÿç¡®ä¿æ¨¡å‹è®­ç»ƒçš„è¿è´¯æ€§å’Œæœ€ç»ˆæ•ˆæœã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„åŠŸèƒ½ï¼Œå¯ä»¥æé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡ã€‚

æˆ‘ä»¬çœ‹ä¸€ä¸‹ `--resume` åœ¨æºç ä¸­çš„ä½¿ç”¨ï¼š

```python
parser.add_argument("--resume", nargs="?", const=True, default=False, help="resume most recent training")

...

def main(opt, callbacks=Callbacks()):
    # Checks
    if RANK in {-1, 0}:
        print_args(vars(opt))
        check_git_status()
        check_requirements(ROOT / "requirements.txt")

    # Resume (from specified or most recent last.pt)
    if opt.resume and not check_comet_resume(opt) and not opt.evolve:
        # å…ˆåˆ¤æ–­ opt.resume æ˜¯ä¸æ˜¯ä¸€ä¸ªstrï¼Œå¦‚æœæ˜¯ï¼Œè¯´æ˜æˆ‘ä»¬æŒ‡å®šäº†å…·ä½“çš„last.pt
        last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())
        opt_yaml = last.parent.parent / "opt.yaml"  # train options yaml
        opt_data = opt.data  # original dataset
        if opt_yaml.is_file():
            with open(opt_yaml, errors="ignore") as f:
                d = yaml.safe_load(f)
        else:
            d = torch.load(last, map_location="cpu")["opt"]
        opt = argparse.Namespace(**d)  # replace
        opt.cfg, opt.weights, opt.resume = "", str(last), True  # reinstate
        if is_url(opt_data):
            opt.data = check_file(opt_data)  # avoid HUB resume auth timeout
```

å› ä¸º `--resume` æ˜¯ `nargs="?"`ï¼Œæ‰€ä»¥å®ƒå¯ä»¥æœ‰ 0 ä¸ªå‚æ•°æˆ–è€… 1 ä¸ªå‚æ•°ï¼Œå³æˆ‘ä»¬å¯ä»¥ç»™å®ƒä¼ å‚ä¹Ÿå¯ä»¥ä¸ç»™å®ƒä¼ å‚ï¼Œé‚£ä¹ˆå®ƒæœ‰å¦‚ä¸‹ä¸¤ç§ç”¨æ³•ï¼š

```bash
# ç”¨æ³•1: ç›´æ¥ä½¿ç”¨ last.pt è¿›è¡Œæ–­ç‚¹ç»­è®­
python train.py --resume

# ç”¨æ³•2: ä½¿ç”¨æŒ‡å®šçš„æƒé‡è¿›è¡Œæ–­ç‚¹ç»­è®­
python train.py --resume runs/exp/weights/example_weights.pt
```

## 5.7 Multi-GPU Trainingï¼Œå¤š GPU è®­ç»ƒ











# å‚è€ƒ

1. ã€”è§†é¢‘æ•™ç¨‹ã€•[YOLOv5å…¥é—¨åˆ°ç²¾é€šï¼ä¸æ„§æ˜¯å…¬è®¤çš„è®²çš„æœ€å¥½çš„ã€ç›®æ ‡æ£€æµ‹å…¨å¥—æ•™ç¨‹ã€‘åŒæµå¤§ä½¬12å°æ—¶å¸¦ä½ ä»å…¥é—¨åˆ°è¿›é˜¶ï¼ˆYOLO/ç›®æ ‡æ£€æµ‹/ç¯å¢ƒéƒ¨ç½²+é¡¹ç›®å®æˆ˜/Python/ï¼‰](https://www.bilibili.com/video/BV1YG411876u?p=13)
2. ã€”PyTorch å®˜æ–¹æ–‡æ¡£ã€•[torch.cuda.amp.autocast](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast)
3. ã€”PyTorch å®˜æ–¹æ–‡æ¡£ã€•[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)