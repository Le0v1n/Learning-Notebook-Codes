<center><b><font size=12>å›¾åƒåˆ†ç±»æ¨¡å‹éƒ¨ç½²: PyTorch -> ONNX</font></b></center>

# 1. æ¨¡å‹éƒ¨ç½²ä»‹ç»

## 1.1 äººå·¥æ™ºèƒ½å¼€å‘éƒ¨ç½²å…¨æµç¨‹

<div align=center>

```mermaid
graph LR

style step1 fill:transparent,stroke:#FF0F50,stroke-width:2px;
style æ•°æ® fill:transparent,stroke:#4CAF50,stroke-width:2px;
style æ•°æ®é‡‡é›† fill:transparent,stroke:#4CAF50,stroke-width:2px;
style å®šä¹‰ç±»åˆ« fill:transparent,stroke:#4CAF50,stroke-width:2px;
style æ ‡æ³¨ fill:transparent,stroke:#4CAF50,stroke-width:2px;
style æ•°æ®é›† fill:transparent,stroke:#4CAF50,stroke-width:2px;

style step2 fill:transparent,stroke:#FF0F50,stroke-width:2px;
style æ¨¡å‹ fill:transparent,stroke:#2196F3,stroke-width:2px;
style è®­ç»ƒæ¨¡å‹ fill:transparent,stroke:#2196F3,stroke-width:2px;
style æµ‹è¯•é›†è¯„ä¼° fill:transparent,stroke:#2196F3,stroke-width:2px;
style è°ƒå‚ä¼˜åŒ– fill:transparent,stroke:#2196F3,stroke-width:2px;
style å¯è§£é‡Šåˆ†æ fill:transparent,stroke:#2196F3,stroke-width:2px;

style step3 fill:transparent,stroke:#FF0F50,stroke-width:2px;
style éƒ¨ç½² fill:transparent,stroke:#BA55D30,stroke-width:2px;
style æ‰‹æœº/å¹³æ¿ fill:transparent,stroke:#BA55D30,stroke-width:2px;
style æœåŠ¡å™¨ fill:transparent,stroke:#BA55D30,stroke-width:2px;
style PC/æµè§ˆå™¨ fill:transparent,stroke:#BA55D30,stroke-width:2px;
style åµŒå…¥å¼å¼€å‘æ¿ fill:transparent,stroke:#BA55D30,stroke-width:2px;

step1 --> æ•°æ® --> æ•°æ®é‡‡é›† --> å®šä¹‰ç±»åˆ« --> æ ‡æ³¨ --> æ•°æ®é›†

step2 --> æ¨¡å‹ --> è®­ç»ƒæ¨¡å‹ --> æµ‹è¯•é›†è¯„ä¼° --> è°ƒå‚ä¼˜åŒ– --> å¯è§£é‡Šåˆ†æ

step3 --> éƒ¨ç½² --> æ‰‹æœº/å¹³æ¿
éƒ¨ç½² --> æœåŠ¡å™¨
éƒ¨ç½² --> PC/æµè§ˆå™¨
éƒ¨ç½² --> åµŒå…¥å¼å¼€å‘æ¿
```

</div>

## 1.2 æ¨¡å‹éƒ¨ç½²å¹³å°å’ŒèŠ¯ç‰‡ä»‹ç»

- **è®¾å¤‡**ï¼šPCã€æµè§ˆå™¨ã€APPã€å°ç¨‹åºã€æœåŠ¡å™¨ã€åµŒå…¥å¼å¼€å‘æ¿ã€æ— äººè½¦ã€æ— äººæœºã€Jetson Nanoã€æ ‘è“æ´¾ã€æœºæ¢°è‡‚ã€ç‰©è”ç½‘è®¾å¤‡
- **å‚å•†**ï¼š
  - è‹±ç‰¹å°”ï¼ˆIntelï¼‰ï¼šä¸»è¦ç”Ÿäº§ CPUï¼ˆä¸­å¤®å¤„ç†å™¨ï¼‰å’Œä¸€äº› FPGAï¼ˆç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼‰èŠ¯ç‰‡ã€‚ä»£è¡¨ä½œå“åŒ…æ‹¬ Intel Core ç³»åˆ— CPU å’Œ Xeon ç³»åˆ—æœåŠ¡å™¨ CPUï¼Œä»¥åŠ FPGA äº§å“å¦‚ Intel Stratix ç³»åˆ—ã€‚
  - è‹±ä¼Ÿè¾¾ï¼ˆNVIDIAï¼‰ï¼šä»¥ GPUï¼ˆå›¾å½¢å¤„ç†å™¨ï¼‰ä¸ºä¸»æ‰“äº§å“ï¼Œå¹¿æ³›åº”ç”¨äºå›¾å½¢æ¸²æŸ“ã€æ·±åº¦å­¦ä¹ ç­‰é¢†åŸŸã€‚ä»£è¡¨ä½œå“åŒ…æ‹¬ NVIDIA GeForce ç³»åˆ—ç”¨äºæ¸¸æˆå›¾å½¢å¤„ç†ï¼ŒNVIDIA Tesla å’Œ NVIDIA A100 ç”¨äºæ·±åº¦å­¦ä¹ åŠ é€Ÿã€‚
  - AMDï¼šä¸»è¦ç”Ÿäº§ CPU å’Œ GPUã€‚ä»£è¡¨ä½œå“åŒ…æ‹¬ AMD Ryzen ç³»åˆ— CPU å’Œ AMD EPYC ç³»åˆ—æœåŠ¡å™¨ CPUï¼Œä»¥åŠ AMD Radeon ç³»åˆ— GPU ç”¨äºæ¸¸æˆå’Œä¸“ä¸šå›¾å½¢å¤„ç†ã€‚
  - è‹¹æœï¼ˆAppleï¼‰ï¼šç”Ÿäº§è‡ªå®¶è®¾è®¡çš„èŠ¯ç‰‡ï¼Œä¸»è¦åŒ…æ‹¬è‹¹æœ M ç³»åˆ—èŠ¯ç‰‡ã€‚ä»£è¡¨ä½œå“æœ‰ M1 èŠ¯ç‰‡ï¼Œå¹¿æ³›åº”ç”¨äºè‹¹æœçš„ Mac ç”µè„‘ã€iPad å’Œä¸€äº›å…¶ä»–è®¾å¤‡ã€‚
  - é«˜é€šï¼ˆQualcommï¼‰ï¼šä¸»è¦ç”Ÿäº§ç§»åŠ¨å¹³å°èŠ¯ç‰‡ï¼ŒåŒ…æ‹¬ç§»åŠ¨å¤„ç†å™¨å’Œè°ƒåˆ¶è§£è°ƒå™¨ã€‚ä»£è¡¨ä½œå“åŒ…æ‹¬ Snapdragon ç³»åˆ—èŠ¯ç‰‡ï¼Œç”¨äºæ™ºèƒ½æ‰‹æœºå’Œç§»åŠ¨è®¾å¤‡ã€‚
  - æ˜‡è…¾ï¼ˆAscendï¼‰ï¼šç”±åä¸ºç”Ÿäº§ï¼Œä¸»è¦ç”Ÿäº§ NPUï¼ˆç¥ç»ç½‘ç»œå¤„ç†å™¨ï¼‰ï¼Œç”¨äºæ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚ä»£è¡¨ä½œå“åŒ…æ‹¬æ˜‡è…¾ 910 å’Œæ˜‡è…¾ 310ã€‚
  - éº’éºŸï¼ˆKirinï¼‰ï¼šåŒæ ·ç”±åä¸ºç”Ÿäº§ï¼Œä¸»è¦ç”Ÿäº§æ‰‹æœºèŠ¯ç‰‡ï¼ŒåŒ…æ‹¬ CPU å’Œ GPUã€‚ä»£è¡¨ä½œå“åŒ…æ‹¬éº’éºŸ 9000 ç³»åˆ—ï¼Œç”¨äºåä¸ºæ——èˆ°æ‰‹æœºã€‚
  - ç‘èŠ¯å¾®ï¼ˆRockchipï¼‰ï¼šä¸»è¦ç”Ÿäº§ VPUï¼ˆè§†è§‰å¤„ç†å™¨ï¼‰å’Œä¸€äº›ç§»åŠ¨å¹³å°èŠ¯ç‰‡ã€‚ä»£è¡¨ä½œå“åŒ…æ‹¬ RK3288 å’Œ RK3399ï¼Œå¹¿æ³›åº”ç”¨äºæ™ºèƒ½æ˜¾ç¤ºã€æœºå™¨äººç­‰é¢†åŸŸã€‚

<div align=center>

|èŠ¯ç‰‡å|è‹±æ–‡å|ä¸­æ–‡å|å‚å•†|ä¸»è¦ä»»åŠ¡|æ˜¯å¦è®­ç»ƒ|æ˜¯å¦æ¨ç†|ç®—åŠ›|é€Ÿåº¦|
|:-|:-|:-|:-|:-|:-|:-|:-|:-|
|CPU|Central Processing Unit(CPU)|ä¸­å¤®å¤„ç†å™¨|å„å¤§å‚å•†|é€šç”¨è®¡ç®—|æ˜¯|æ˜¯|é«˜|ä¸­ç­‰|
|GPU|Graphics Processing Unit(GPU)|å›¾å½¢å¤„ç†å™¨|NVIDIAã€AMDç­‰|å›¾å½¢æ¸²æŸ“ã€æ·±åº¦å­¦ä¹ åŠ é€Ÿ|æ˜¯|æ˜¯|é«˜|é«˜|
|TPU|Tensor Processing Unit(TPU)|å¼ é‡å¤„ç†å™¨|è°·æ­Œ|æœºå™¨å­¦ä¹ ä¸­çš„å¼ é‡è¿ç®—|æ˜¯|æ˜¯|é«˜|é«˜|
|NPU|Neural Processing Unit(NPU)|ç¥ç»ç½‘ç»œå¤„ç†å™¨|åä¸ºã€è”å‘ç§‘ç­‰|æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½æå‡|æ˜¯|æ˜¯|é«˜|ä¸­ç­‰|
|VPU|Vision Processing Unit(VPU)|è§†è§‰å¤„ç†å™¨|è‹±ç‰¹å°”ã€åšé€šç­‰|å›¾åƒå’Œè§†é¢‘å¤„ç†|å¦|æ˜¯|ä¸­ç­‰|ä¸­ç­‰|
|DSP|Digital Signal Processor(DSP)|æ•°å­—ä¿¡å·å¤„ç†å™¨|å¾·å·ä»ªå™¨ã€é«˜é€šç­‰|æ•°å­—ä¿¡å·å¤„ç†ã€éŸ³é¢‘ä¿¡å·å¤„ç†|å¦|æ˜¯|ä¸­ç­‰|ä¸­ç­‰|
|FPGA|Field-Programmable Gate Array(FPGA)|ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—|è‹±ç‰¹å°”ã€èµ›çµæ€ç­‰|å¯ç¼–ç¨‹ç¡¬ä»¶åŠ é€Ÿå™¨|æ˜¯|æ˜¯|é«˜|ä¸­ç­‰|

</div>

## 1.3 æ¨¡å‹éƒ¨ç½²çš„é€šç”¨æµç¨‹

<div align=center>

```mermaid
graph LR

style PyTorch fill:transparent,stroke:#2196F3,stroke-width:2px;
style TensorFlow fill:transparent,stroke:#2196F3,stroke-width:2px;
style Caffe fill:transparent,stroke:#2196F3,stroke-width:2px;
style PaddlePaddle fill:transparent,stroke:#2196F3,stroke-width:2px;
style è®­ç»ƒæ¡†æ¶ fill:transparent,stroke:#2196F3,stroke-width:2px;
style ONNX/ä¸­é—´è¡¨ç¤º fill:transparent,stroke:#4CAF50,stroke-width:2px;
style æ¨ç†æ¡†æ¶/å¼•æ“/åç«¯ fill:transparent,stroke:#2196F3,stroke-width:2px;
style TensorRT fill:transparent,stroke:#2196F3,stroke-width:2px;
style ONNXRuntime fill:transparent,stroke:#2196F3,stroke-width:2px;
style OpenVINO fill:transparent,stroke:#2196F3,stroke-width:2px;
style NCNN/TNN fill:transparent,stroke:#2196F3,stroke-width:2px;
style PPL fill:transparent,stroke:#2196F3,stroke-width:2px;

PyTorch --> è®­ç»ƒæ¡†æ¶
TensorFlow --> è®­ç»ƒæ¡†æ¶
Caffe --> è®­ç»ƒæ¡†æ¶
PaddlePaddle --> è®­ç»ƒæ¡†æ¶
è®­ç»ƒæ¡†æ¶ -->|è½¬æ¢| ONNX/ä¸­é—´è¡¨ç¤º -->|è¿è¡Œ| æ¨ç†æ¡†æ¶/å¼•æ“/åç«¯
æ¨ç†æ¡†æ¶/å¼•æ“/åç«¯ --> TensorRT
æ¨ç†æ¡†æ¶/å¼•æ“/åç«¯ --> ONNXRuntime
æ¨ç†æ¡†æ¶/å¼•æ“/åç«¯ --> OpenVINO
æ¨ç†æ¡†æ¶/å¼•æ“/åç«¯ --> NCNN/TNN
æ¨ç†æ¡†æ¶/å¼•æ“/åç«¯ --> PPL
```

</div>

# 2. ä½¿ç”¨ ONNX çš„æ„ä¹‰

<div align=center>
    <img src=./imgs_markdown/2024-01-25-10-49-51.png
    width=100%>
</div>

<div align=center>
    <img src=./imgs_markdown/2024-01-25-10-50-17.png
    width=100%>
</div>

ä»è¿™ä¸¤å¼ å›¾å¯ä»¥å¾ˆæ˜æ˜¾çš„çœ‹åˆ°ï¼Œå½“æœ‰äº†ä¸­é—´è¡¨ç¤º ONNX åï¼Œä»åŸæ¥çš„ $M \times N$ å˜ä¸ºäº† $M + N$ï¼Œè®©æ¨¡å‹éƒ¨ç½²çš„æµç¨‹å˜å¾—ç®€å•ã€‚

# 3. ONNX çš„ä»‹ç»

å¼€æºæœºå™¨å­¦ä¹ <font color='blue'>é€šç”¨ä¸­é—´æ ¼å¼</font>ï¼Œç”±å¾®è½¯ã€Facebookï¼ˆMetaï¼‰ã€äºšé©¬é€Šã€IBM å…±åŒå‘èµ·çš„ã€‚<font color='green'>å®ƒå¯ä»¥å…¼å®¹å„ç§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¹Ÿå¯ä»¥å…¼å®¹å„ç§æ¨ç†å¼•æ“å’Œç»ˆç«¯ç¡¬ä»¶ã€æ“ä½œç³»ç»Ÿ</font>ã€‚

# 4. ONNX ç¯å¢ƒå®‰è£…

```bash
pip install onnx -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple
```

# 5. PyTorch â†’ ONNX
## 5.1 å°†ä¸€ä¸ªåˆ†ç±»æ¨¡å‹è½¬æ¢ä¸º ONNX

```python
import torch
from torchvision import models


device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: {device}")

# åˆ›å»ºä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹
model = models.resnet18(pretrained=True)  # ImageNet é¢„è®­ç»ƒæƒé‡
model = model.eval().to(device)

# æ„å»ºä¸€ä¸ªè¾“å…¥
dummy_input = torch.randn(size=[1, 3, 256, 256]).to(device)  # [N, B, H, W]

# è®©æ¨¡å‹æ¨ç†
output = model(dummy_input)
print(f"output.shape: {output.shape}")

# ä½¿ç”¨ PyTorch è‡ªå¸¦çš„å‡½æ•°å°†æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼
onnx_save_path = 'ONNX/saves/resnet18_imagenet.onnx'  # å¯¼å‡ºçš„ONNXæ¨¡å‹è·¯å¾„ 
with torch.no_grad():
    torch.onnx.export(
        model=model,                            # è¦è½¬æ¢çš„æ¨¡å‹
        args=dummy_input,                       # æ¨¡å‹çš„è¾“å…¥
        f=onnx_save_path,                       # å¯¼å‡ºçš„ONNXæ¨¡å‹è·¯å¾„ 
        input_names=['input'],                  # ONNXæ¨¡å‹è¾“å…¥çš„åå­—(è‡ªå®šä¹‰)
        output_names=['output'],                # ONNXæ¨¡å‹è¾“å‡ºçš„åå­—(è‡ªå®šä¹‰)
        opset_version=11,                       # Opsetç®—å­é›†åˆçš„ç‰ˆæœ¬ï¼ˆé»˜è®¤ä¸º17ï¼‰
    )
    
print(f"ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸï¼Œè·¯å¾„ä¸ºï¼š{onnx_save_path}")
```

```
æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: cpu
/home/leovin/anaconda3/envs/wsl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/leovin/anaconda3/envs/wsl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /home/leovin/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:03<00:00, 13.9MB/s]
output.shape: torch.Size([1, 1000])
ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸï¼Œè·¯å¾„ä¸ºï¼šONNX/saves/resnet18_imagenet.onnx
```

ğŸ’¡ <kbd>Tips</kbd>:
1. opset ç®—å­é›†ä¸åŒç‰ˆæœ¬åŒºåˆ«: [Operators.md](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
2. è™½ç„¶è¯´ PyTorch åœ¨æé†’ `pretrained=True` å°†ä¼šè¢«å¼ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨ `weights=weights=ResNet18_Weights.DEFAULT` æˆ– `weights=ResNet18_Weights.IMAGENET1K_V1` æ¥ä»£æ›¿ã€‚ä½†å¾ˆæ˜æ˜¾å‰è€…æ¯”è¾ƒæ–¹ä¾¿ï¼Œåè€…è¿˜éœ€è¦æŸ¥çœ‹å¯¹åº”çš„ç‰ˆæœ¬å·ï¼Œæ¯”è¾ƒéº»çƒ¦ :joy:

æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨ Netron æŸ¥çœ‹ä¸€ä¸‹è¿™ä¸ªæ¨¡å‹ï¼š

<div align=center>
    <img src=./imgs_markdown/2024-01-25-11-35-29.png
    width=100%>
</div>

> 1. åŸå›¾é“¾æ¥ä¸ºï¼š[resnet18_imagenet.png](https://github.com/Le0v1n/Learning-Notebook-Codes/blob/main/ONNX/imgs_markdown/resnet18_imagenet.png)
> 
> 2. ImageNet æ•°æ®é›†æœ‰ 1000 ä¸ªç±»åˆ«

## 5.2 æ£€æŸ¥ä¸€ä¸ªæ¨¡å‹å¯¼å‡ºæ˜¯å¦æ­£ç¡®

```python
import onnx


# è¯»å–å¯¼å‡ºçš„æ¨¡å‹
onnx_path = 'ONNX/saves/resnet18_imagenet.onnx'  # å¯¼å‡ºçš„ONNXæ¨¡å‹è·¯å¾„
onnx_model = onnx.load(onnx_path)

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£å¸¸
onnx.checker.check_model(onnx_model)

print(f"æ¨¡å‹å¯¼å‡ºæ­£å¸¸!")
```

```
æ¨¡å‹å¯¼å‡ºæ­£å¸¸!
```

> æˆ‘ä»¬åœ¨ã€Š[onnxåŸºç¡€](https://blog.csdn.net/weixin_44878336/article/details/135820896)ã€‹ä¸­å·²ç»è®²è¿‡ `check_model()` è¿™ä¸ªå‡½æ•°ï¼Œå®ƒå¯ä»¥æ£€æŸ¥ ONNX æ¨¡å‹ï¼Œå¦‚æœè¯¥å‡½æ•°å‘ç°æ¨¡å‹é”™è¯¯ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œ

## 5.3 ä¿®æ”¹åŠ¨æ€ç»´åº¦

å‰é¢æˆ‘ä»¬å¯¼å‡ºçš„ ONNX æ¨¡å‹ä¸­ï¼Œè¾“å…¥çš„ç»´åº¦æ˜¯å›ºå®šçš„ï¼š`[1, 3, 256, 256]`ï¼Œé‚£ä¹ˆæ­¤æ—¶è¿™ä¸ª ONNX çš„è¾“å…¥å°±è¢«é™åˆ¶äº†ï¼š
- å¦‚æœæˆ‘ä»¬æƒ³è¦å¤š Batch çš„è¾“å…¥ â†’ ä¸è¡Œ
- å¦‚æœæˆ‘ä»¬è¾“å…¥çš„å›¾ç‰‡æ˜¯ç°åº¦å›¾ â†’ ä¸è¡Œ
- å¦‚æœæˆ‘ä»¬è¾“å…¥çš„å›¾ç‰‡å°ºå¯¸ä¸æ˜¯ 256Ã—256 â†’ ä¸è¡Œ

è€Œ `torch.onnx.export()` è¿™ä¸ªå‡½æ•°ä¹Ÿå¸®æˆ‘è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œå®ƒæœ‰ä¸€ä¸ªåä¸º `dynamic_axis` çš„å‚æ•°ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹å®˜ç½‘å¯¹è¯¥å‚æ•°çš„æè¿°ï¼š

> dynamic_axes (*dict[string, dict[int, string]] or dict[string, list(int)], default empty dict*) â€“
> 
> By default the exported model will have the shapes of all input and output tensors set to exactly match those given in `args`. To specify axes of tensors as dynamic (i.e. known only at run-time), set `dynamic_axes` to a dict with schema:
> - **KEY (str)**: an input or output name. Each name must also be provided in input_names or output_names.
> - **VALUE (dict or list)**: If a dict, keys are axis indices and values are axis names. If a list, each element is an axis index.

> dynamic_axesï¼ˆ*dict[string, dict[int, string]]æˆ–dict[string, list(int)]ï¼Œé»˜è®¤ä¸ºç©ºå­—å…¸*ï¼‰â€“
> 
> é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯¼å‡ºçš„æ¨¡å‹å°†ä½¿æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶å®Œå…¨åŒ¹é…`args`ä¸­ç»™å®šçš„å½¢çŠ¶ã€‚è¦å°†å¼ é‡çš„è½´æŒ‡å®šä¸ºåŠ¨æ€ï¼ˆ<font color='green'>å³ä»…åœ¨è¿è¡Œæ—¶çŸ¥é“</font>ï¼‰ï¼Œè¯·å°†`dynamic_axes`è®¾ç½®ä¸ºä¸€ä¸ªå…·æœ‰ä»¥ä¸‹ç»“æ„çš„å­—å…¸ï¼š
> - **KEYï¼ˆstrï¼‰**ï¼šè¾“å…¥æˆ–è¾“å‡ºçš„åç§°ã€‚æ¯ä¸ªåç§°è¿˜å¿…é¡»åœ¨ `input_names` æˆ– `output_names` ä¸­æä¾›ã€‚
> - **VALUEï¼ˆdictæˆ–listï¼‰**ï¼šå¦‚æœæ˜¯å­—å…¸ï¼Œåˆ™é”®æ˜¯è½´ç´¢å¼•ï¼Œå€¼æ˜¯è½´åç§°ã€‚å¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆ™æ¯ä¸ªå…ƒç´ æ˜¯è½´ç´¢å¼•ã€‚

ä¸‹é¢æˆ‘ä»¬ç”¨ä¸€ä¸‹è¿™ä¸ªå‚æ•°ï¼š

```python
import torch
from torchvision import models
import onnx


device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: {device}")

# åˆ›å»ºä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹
model = models.resnet18(pretrained=True)  # ImageNet é¢„è®­ç»ƒæƒé‡
model = model.eval().to(device)

# æ„å»ºä¸€ä¸ªè¾“å…¥
dummy_input = torch.randn(size=[1, 3, 256, 256]).to(device)  # [N, B, H, W]

# è®©æ¨¡å‹æ¨ç†
output = model(dummy_input)
print(f"output.shape: {output.shape}\n")

# ------ ä½¿ç”¨ PyTorch è‡ªå¸¦çš„å‡½æ•°å°†æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼
onnx_save_path = 'ONNX/saves/resnet18_imagenet-with_dynamic_axis.onnx'  # å¯¼å‡ºçš„ONNXæ¨¡å‹è·¯å¾„ 
with torch.no_grad():
    torch.onnx.export(
        model=model,                            # è¦è½¬æ¢çš„æ¨¡å‹
        args=dummy_input,                       # æ¨¡å‹çš„è¾“å…¥
        f=onnx_save_path,                       # å¯¼å‡ºçš„ONNXæ¨¡å‹è·¯å¾„ 
        input_names=['input'],                  # ONNXæ¨¡å‹è¾“å…¥çš„åå­—(è‡ªå®šä¹‰)
        output_names=['output'],                # ONNXæ¨¡å‹è¾“å‡ºçš„åå­—(è‡ªå®šä¹‰)
        opset_version=11,                       # Opsetç®—å­é›†åˆçš„ç‰ˆæœ¬ï¼ˆé»˜è®¤ä¸º17ï¼‰
        dynamic_axes={                          # ä¿®æ”¹æŸä¸€ä¸ªç»´åº¦ä¸ºåŠ¨æ€
            'input': {0: 'B', 2: 'H', 3: 'W'}   # å°†åŸæœ¬çš„ [1, 3, 256, 256] ä¿®æ”¹ä¸º [B, 3, H, W]
        }
    )
    
print(f"ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸï¼Œè·¯å¾„ä¸ºï¼š{onnx_save_path}\n")

# ------ éªŒè¯å¯¼å‡ºçš„æ¨¡å‹æ˜¯å¦æ­£ç¡®
# è¯»å–å¯¼å‡ºçš„æ¨¡å‹
onnx_model = onnx.load(onnx_save_path)

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£å¸¸
onnx.checker.check_model(onnx_model)

print(f"æ¨¡å‹å¯¼å‡ºæ­£å¸¸!")
```

```
æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: cpu
output.shape: torch.Size([1, 1000])

ONNX æ¨¡å‹å¯¼å‡ºæˆåŠŸï¼Œè·¯å¾„ä¸ºï¼šONNX/saves/resnet18_imagenet-with_dynamic_axis.onnx

æ¨¡å‹å¯¼å‡ºæ­£å¸¸!
```

æ­¤æ—¶æˆ‘ä»¬å†ç”¨ Netron çœ‹ä¸€ä¸‹è¿™ä¸ªæ¨¡å‹ï¼š

<div align=center>
    <img src=./imgs_markdown/2024-01-25-11-55-28.png
    width=80%>
</div>

å¯ä»¥çœ‹åˆ°ï¼Œè¾“å…¥çš„ Batchã€Heightã€Width å‡å˜ä¸ºäº†åŠ¨æ€ç»´åº¦ï¼Œ<font color='green'>å³åªæœ‰å½“æ¨¡å‹è¿è¡Œçš„æ—¶å€™æ‰çŸ¥é“è¾“å…¥çš„è¿™ä¸‰ä¸ªç»´åº¦å…·ä½“çš„å€¼</font>ã€‚

# 6. ONNX Runtime éƒ¨ç½²ï¼šæ¨ç†å•å¼ å›¾ç‰‡

```python
import os
import random
import numpy as np
from PIL import Image
import onnxruntime
from torchvision import transforms
import torch
import torch.nn.functional as F
import pandas as pd


# ==================================== åŠ è½½ ONNX æ¨¡å‹ï¼Œåˆ›å»ºæ¨ç†ä¼šè¯ ==================================== 
ort_session = onnxruntime.InferenceSession(path_or_bytes='ONNX/saves/resnet18_imagenet-fix_axis.onnx')  # ort -> onnxruntime

# ==================================== æ¨¡å‹å†·å¯åŠ¨ ==================================== 
dummy_input = np.random.randn(1, 3, 256, 256).astype(np.float32)
ort_inputs = {'input': dummy_input}
ort_output = ort_session.run(output_names=['output'], input_feed=ort_inputs)[0]  # è¾“å‡ºè¢«[]åŒ…å›´äº†ï¼Œæ‰€ä»¥éœ€è¦å–å‡ºæ¥
print(f"æ¨¡å‹å†·å¯åŠ¨å®Œæ¯•! å…¶æ¨ç†ç»“æœçš„shapeä¸º: {ort_output.shape}")

# ==================================== åŠ è½½çœŸæ­£çš„å›¾åƒ ==================================== 
images_folder = 'Datasets/Web/images'
images_list = [os.path.join(images_folder, img) for img in os.listdir(images_folder) if img.lower().endswith(('.jpg', '.png', '.webp'))]

img_path = images_list[random.randint(0, len(images_list)-1)]
img = Image.open(fp=img_path)

# ==================================== å›¾åƒé¢„å¤„ç† ==================================== 
# å®šä¹‰é¢„å¤„ç†å‡½æ•°
img_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # imagenetä¸“ç”¨
        std=[0.229, 0.224, 0.225]),  # imagenetä¸“ç”¨
])

# å¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†
input_img = img_transform(img)
print(f"input_img.type: {type(input_img)}")
print(f"input_img.shape: {input_img.shape}")

# ä¸ºå›¾ç‰‡æ·»åŠ batchç»´åº¦
input_img = torch.unsqueeze(input_img, dim=0)

# ==================================== ONNXæ¨¡å‹æ¨ç† ==================================== 
# å› ä¸ºONNXRuntimeéœ€è¦çš„æ˜¯numpyè€Œétorchçš„tensor, æ‰€ä»¥å°†å…¶è½¬æ¢ä¸ºnumpy
input_img = input_img.numpy()
print(f"input_img.type: {type(input_img)}")
print(f"input_img.shape: {input_img.shape}")

# æ¨¡å‹æ¨ç†å›¾ç‰‡
ort_inputs = {'input': input_img, }
ort_results = ort_session.run(output_names=['output'], input_feed=ort_inputs)[0]  # å¾—åˆ° 1000 ä¸ªç±»åˆ«çš„åˆ†æ•°
print(f"æ¨¡å‹æ¨ç†å®Œæ¯•! æ­¤æ—¶ç»“æœçš„shapeä¸ºï¼š{ort_results.shape}")

# ==================================== åå¤„ç† ==================================== 
# ä½¿ç”¨ softmax å‡½æ•°å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡
ort_results_softmax = F.softmax(input=torch.from_numpy(ort_results), dim=1)
print(f"ç»è¿‡softmaxåçš„è¾“å‡ºçš„shapeä¸ºï¼š{ort_results_softmax.shape}")

# å–æ¦‚ç‡æœ€å¤§çš„å‰ n ä¸ªç»“æœ
n = 3
top_n = torch.topk(input=ort_results_softmax, k=n)

probs = top_n.values.numpy()[0]
indices = top_n.indices.numpy()[0]

print(f"ç½®ä¿¡åº¦æœ€é«˜çš„å‰{n}ä¸ªç»“æœä¸ºï¼š\t{probs}\n"
      f"å¯¹åº”çš„ç±»åˆ«ç´¢å¼•ä¸ºï¼š\t\t{indices}")

# ==================================== æ˜¾ç¤ºç±»åˆ« ==================================== 
df = pd.read_csv('Datasets/imagenet_classes_indices.csv')

idx2labels = {}
for idx, row in df.iterrows():
    # idx2labels[row['ID']] = row['class']  # è‹±æ–‡æ ‡ç­¾
    idx2labels[row['ID']] = row['Chinese']  # ä¸­æ–‡æ ‡ç­¾

print(f"=============== æ¨ç†ç»“æœ ===============\n"
      f"å›¾ç‰‡è·¯å¾„: {img_path}")
for i, (class_prob, idx) in enumerate(zip(probs, indices)):
    class_name = idx2labels[idx]
    text = f"\tNo.{i}: {class_name:<30} --> {class_prob:>.4f}"
    print(text)
```

```
æ¨¡å‹å†·å¯åŠ¨å®Œæ¯•! å…¶æ¨ç†ç»“æœçš„shapeä¸º: (1, 1000)
input_img.type: <class 'torch.Tensor'>
input_img.shape: torch.Size([3, 256, 256])
input_img.type: <class 'numpy.ndarray'>
input_img.shape: (1, 3, 256, 256)
æ¨¡å‹æ¨ç†å®Œæ¯•! æ­¤æ—¶ç»“æœçš„shapeä¸ºï¼š(1, 1000)
ç»è¿‡softmaxåçš„è¾“å‡ºçš„shapeä¸ºï¼štorch.Size([1, 1000])
ç½®ä¿¡åº¦æœ€é«˜çš„å‰3ä¸ªç»“æœä¸ºï¼š       [9.9472505e-01 7.4335985e-04 5.2123831e-04]
å¯¹åº”çš„ç±»åˆ«ç´¢å¼•ä¸ºï¼š              [673 662 487]
=============== æ¨ç†ç»“æœ ===============
å›¾ç‰‡è·¯å¾„: Datasets/Web/images/mouse.jpg
        No.0: é¼ æ ‡,ç”µè„‘é¼ æ ‡                        --> 0.9947
        No.1: è°ƒåˆ¶è§£è°ƒå™¨                          --> 0.0007
        No.2: ç§»åŠ¨ç”µè¯,æ‰‹æœº                        --> 0.0005
```

> ğŸ’¡ å›¾ç‰‡é“¾æ¥ï¼š[Web/images](https://github.com/Le0v1n/Learning-Notebook-Codes/tree/main/Datasets/Web/images)
> 
> ğŸ’¡ ImageNet ç±»åˆ«æ–‡ä»¶é“¾æ¥ï¼š[imagenet_classes_indices.csv](https://github.com/Le0v1n/Learning-Notebook-Codes/tree/main/Datasets/imagenet_classes_indices.csv)

# 7. ONNX Runtime å’Œ PyTorch é€Ÿåº¦å¯¹æ¯”

1. ä¸åŒå°ºåº¦ä¸‹å•å¼ å›¾ç‰‡æ¨ç† --> [å¯¹æ¯”ä»£ç é“¾æ¥](https://github.com/Le0v1n/Learning-Notebook-Codes/tree/main/ONNX/codes/onnx%E5%AE%9E%E6%93%8D/%E9%80%9F%E5%BA%A6%E5%AF%B9%E6%AF%94/No1-%E4%B8%8D%E5%90%8C%E5%B0%BA%E5%BA%A6%E4%B8%8B%E5%8D%95%E5%BC%A0%E5%9B%BE%E7%89%87%E6%8E%A8%E7%90%86)
2. ä¸åŒå°ºåº¦ä¸‹å¤šå¼ å›¾ç‰‡æ¨ç† --> [å¯¹æ¯”ä»£ç é“¾æ¥](https://github.com/Le0v1n/Learning-Notebook-Codes/tree/main/ONNX/codes/onnx%E5%AE%9E%E6%93%8D/%E9%80%9F%E5%BA%A6%E5%AF%B9%E6%AF%94/No2-%E4%B8%8D%E5%90%8C%E5%B0%BA%E5%BA%A6%E4%B8%8B%E5%A4%9A%E5%BC%A0%E5%9B%BE%E7%89%87%E6%8E%A8%E7%90%86)

**å®éªŒç¯å¢ƒ**ï¼š
- CPUï¼šIntel i5-10400F @ 2.90   GHz
- Memory: 8 x 2 = 16GB
- Disk: SSD
- GPU: RTX 3070 O8G
- OS: Windows 10 (WSL)
- Device: CPU
- æ¨¡å‹æ¨ç†æ¬¡æ•°: 50

## 7.1 ResNet-18

**å®éªŒç»“æœ**

<div align=center>

|Input Shape|ONNX(fix)|ONNX(fix+sim)|ONNX(dyn)|ONNX(dyn+sim)|PyTorch(CPU)|PyTorch(GPU)| 
|:-|:-:|:-:|:-:|:-:|:-:|:-:|
|[1, 3, 32, 32]      |0.0577s|0.0597s|0.0592s|0.0585s|0.0688s|0.0787s|
|[1, 3, 64, 64]      |0.0605s|0.0593s|0.0588s|0.0621s|0.0700s|0.0723s|
|[1, 3, 128, 128]    |0.0705s|0.0686s|0.0699s|0.0694s|0.0762s|0.0760s|
|[1, 3, 256, 256]    |0.0784s|0.0811s|0.0797s|0.0789s|0.0949s|0.0813s|
|[1, 3, 512, 512]    |0.1249s|0.1241s|0.1251s|0.1256s|0.1686s|0.0996s|
|[1, 3, 640, 640]    |0.1569s|0.1525s|0.1572s|0.1579s|0.2242s|0.0863s|
|[1, 3, 768, 768]    |0.1986s|0.1946s|0.1985s|0.2038s|0.2933s|0.0956s|
|[1, 3, 1024, 1024]  |0.2954s|0.2957s|0.3094s|0.3045s|0.4871s|0.1047s|
|[16, 3, 32, 32]     |0.2540s|0.2545s|0.2558s|0.2498s|0.2570s|0.2473s|
|[16, 3, 64, 64]     |0.2811s|0.2745s|0.2696s|0.2655s|0.2824s|0.2553s|
|[16, 3, 128, 128]   |0.3595s|0.3181s|0.3143s|0.3544s|0.3817s|0.3518s|
|[16, 3, 256, 256]   |0.7315s|0.7112s|0.6767s|0.6122s|0.7169s|0.3469s|
|[16, 3, 512, 512]   |1.3042s|1.2586s|1.1813s|1.1949s|1.6609s|0.4270s|
|[16, 3, 640, 640]   |1.6340s|1.6429s|1.6659s|1.6693s|2.3923s|0.5292s|
|[16, 3, 768, 768]   |2.2843s|2.2830s|2.3325s|2.3303s|3.9278s|1.7851s|
|[16, 3, 1024, 1024] |3.9132s|3.9742s|3.9668s|3.9104s|6.7532s|3.6507s|

</div>

**ç”»å›¾ç»“æœ**

> âš ï¸ åœ¨ `[18, 3, 768, 768]`ã€ æ—¶ï¼ŒPyTorch(CPU) å› ä¸ºå†…å­˜ä¸è¶³å¯¼è‡´åªèƒ½æ¨ç† 1 æ¬¡è€Œé 50 æ¬¡
> 
> âš ï¸ åœ¨ `[18, 3, 1024, 1024]`ã€ æ—¶ï¼ŒPyTorch(CPU) å’Œ PyTorch(GPU) å› ä¸ºå†…å­˜ä¸è¶³å¯¼è‡´åªèƒ½æ¨ç† 1 æ¬¡è€Œé 50 æ¬¡

<div align=center>
    <img src=./imgs_markdown/ResNet18-Speed-Comparison-of-Different-Models-Full-Data.jpg
    width=100%>
</div>

<div align=center>
    <img src=./imgs_markdown/ResNet18-Speed-Comparison-of-Different-Models-Single-Batch.jpg
    width=100%>
</div>

<div align=center>
    <img src=./imgs_markdown/ResNet18-Speed-Comparison-of-Different-Models-Multi-Batch.jpg
    width=100%>
</div>

<div align=center>
    <img src=./imgs_markdown/ResNet18-Speed-Comparison-of-Different-Models-Sim.jpg
    width=100%>
</div>

<kbd>ç»“è®º</kbd>ï¼š
- ã€”<font color='green'><b>å• Batch</b></font>ã€•
    - é™æ€ç»´åº¦å’ŒåŠ¨æ€ç»´åº¦ç›¸å·®ä¸å¤§
    - å½“å›¾ç‰‡å°ºå¯¸åœ¨ [32, 32] ~ [256, 256] ä¹‹é—´æ—¶ï¼ŒONNX é€Ÿåº¦æ¯” PyTorch-GPU é€Ÿåº¦è¦å¿«ï¼›å½“å›¾ç‰‡å°ºå¯¸å¤§äº [256, 256] æ—¶ï¼ŒPyTorch-GPU æ‹¥æœ‰ç»å¯¹çš„ä¼˜åŠ¿
    - å½“å›¾ç‰‡å°ºå¯¸å°äº [64, 64] æ—¶ï¼ŒPyTorch-CPU é€Ÿåº¦å¿«äº PyTorch-GPUï¼›å½“å›¾ç‰‡å°ºå¯¸å¤§äº [64, 64] æ—¶ï¼ŒPyTorch-GPU é€Ÿåº¦å¿«äº PyTorch-CPU
    - æ— è®ºåœ¨ä»€ä¹ˆæ—¶å€™ï¼ŒONNX é€Ÿåº¦å‡å¿«äº PyTorch-CPU
- ã€”<font color='blue'><b>å¤š Batch</b></font>ã€•
    - é™æ€ç»´åº¦å’ŒåŠ¨æ€ç»´åº¦ç›¸å·®ä¸å¤§
    - å½“å›¾ç‰‡å°ºå¯¸å°äº [128, 128] æ—¶ï¼ŒONNXã€PyTorch-CPUã€PyTorch-GPU ä¸‰è€…å¾ˆéš¾æœ‰åŒºåˆ«ï¼ˆå®é™…ä¸Š PyTorch-GPU é€Ÿåº¦è¦æ…¢ä¸€äº›ï¼Œå› ä¸ºè¦å°†æ¨¡å‹å’Œè¾“å…¥æ”¾åˆ° GPU ä¸­ï¼Œè¿™éƒ¨åˆ†ä¼šåˆ’åˆ†å‡ ç§’é’Ÿçš„æ—¶é—´ï¼‰
    - å½“å›¾ç‰‡å°ºå¯¸å¤§äº [128, 128] æ—¶ï¼ŒGPU é€æ¸æ‰©å¤§ä¼˜åŠ¿ï¼ˆç”±äº OOM çš„åŸå› ï¼Œ[18, 3, 1024, 1024] ä¸‹ PyTorch-GPU åªæ¨ç†äº†ä¸€æ¬¡ï¼Œå› æ­¤é€Ÿåº¦è¢«æ‹‰å¹³äº†å¾ˆå¤šã€‚åœ¨æ˜¾å­˜è¶³å¤Ÿå……è£•çš„æƒ…å†µä¸‹ï¼ŒPyTorch-GPU çš„é€Ÿåº¦æ˜¯ç¢¾å‹å…¶ä»–æ–¹æ³•çš„ï¼‰
    - å½“å›¾ç‰‡å°ºå¯¸å¤§äº [256, 256] æ—¶ï¼ŒPyTorch-CPU çš„é€Ÿåº¦è¿œè¿œæ…¢äº ONNX
- ã€”<font color='purple'><b>Sim å‰å</b></font>ã€•
    - å¯ä»¥å‘ç°ï¼Œåœ¨ä½¿ç”¨ `python -m onnxsim` å‰åå·®è·ä¸å¤§
- ã€”<font color='red'><b>æ€»ç»“</b></font>ã€•
    - åœ¨ä½¿ç”¨ CPU è¿›è¡Œæ¨ç†æ—¶ï¼Œå»ºè®®ä½¿ç”¨ ONNX è¿›è¡Œï¼Œå› ä¸ºä¸å…‰é€Ÿåº¦æœ‰ä¼˜åŠ¿ï¼Œè€Œä¸”å¯¹å†…å­˜çš„å ç”¨ä¹Ÿæ¯” PyTorch-CPU è¦å°çš„å¤š
    - åœ¨è¿›è¡Œå¤š Batch æ¨ç†æ—¶ï¼Œå¦‚æœæœ‰ GPU è¿˜æ˜¯ä½¿ç”¨ PyTorch-GPUï¼Œè¿™æ ·ä¼šç¼©å‡å¤§é‡çš„æ—¶é—´ï¼ˆâš ï¸ GPU åœ¨åŠ è½½æ¨¡å‹å’Œè¾“å…¥æ—¶å¯èƒ½ä¼šæ¯”è¾ƒè€—æ—¶ï¼‰
    - âš ï¸ åœ¨ä½¿ç”¨ `python -m onnxsim` å‰åå·®è·ä¸å¤§

## 7.2 MobileNetV3-Small

æ¥ä¸‹æ¥æˆ‘ä»¬åœ¨ MobileNetV3-Small ä¸Šä¹Ÿè¿›è¡Œç›¸åŒçš„å®éªŒã€‚

> âš ï¸ å› ä¸º `opset=11` ä¸æ”¯æŒ `hardsigmoid` ç®—å­ï¼Œåœ¨å®˜ç½‘ä¸ŠæŸ¥è¯¢åï¼Œæˆ‘ä»¬ä½¿ç”¨ `opset=17`
> 
> âš ï¸ åœ¨ä½¿ç”¨ `opset=17` æ—¶å¯èƒ½ä¼šæŠ¥é”™ï¼ŒæŠ¥é”™åŸå› ä¸€èˆ¬æ˜¯å½“å‰ PyTorch ç‰ˆæœ¬ä½å¯¼è‡´çš„ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„ç¯å¢ƒï¼Œä½¿ç”¨æœ€æ–°çš„ PyTorchï¼ˆä¹Ÿå¯ä»¥ä¸å®éªŒï¼Œç›´æ¥çœ‹æˆ‘å¾—ç»“è®ºå°±è¡Œ :joy:ï¼‰

<div align=center>

|Input Shape|ONNX(fix)|ONNX(dyn)|PyTorch(CPU)|PyTorch(GPU)|
|:-|:-:|:-:|:-:|:-:|
|[1, 3, 32, 32]     |0.0575s|0.0619s|0.0636s|0.0731s|
|[1, 3, 64, 64]     |0.0585s|0.0591s|0.0643s|0.0701s|
|[1, 3, 128, 128]   |0.0611s|0.0597s|0.0629s|0.0700s|
|[1, 3, 256, 256]   |0.0627s|0.0622s|0.0690s|0.0731s|
|[1, 3, 512, 512]   |0.0714s|0.0703s|0.0841s|0.0765s|
|[1, 3, 640, 640]   |0.0776s|0.0785s|0.0975s|0.0823s|
|[1, 3, 768, 768]   |0.0867s|0.0861s|0.1138s|0.0851s|
|[1, 3, 1024, 1024] |0.1103s|0.1126s|0.1630s|0.0958s|
|[16, 3, 32, 32]    |0.2410s|0.2295s|0.2538s|0.2446s|
|[16, 3, 64, 64]    |0.2443s|0.2421s|0.2576s|0.2481s|
|[16, 3, 128, 128]  |0.2618s|0.2576s|0.2804s|0.2727s|
|[16, 3, 256, 256]  |0.3097s|0.3131s|0.3502s|0.3043s|
|[16, 3, 512, 512]  |0.5556s|0.5873s|0.7655s|0.3970s|
|[16, 3, 640, 640]  |0.7191s|0.7130s|0.8988s|0.4877s|
|[16, 3, 768, 768]  |0.9293s|0.9285s|1.5091s|0.5754s|
|[16, 3, 1024, 1024]|1.4768s|1.4945s|3.3530s|1.1316s|

</div>

**ç”»å›¾ç»“æœ**

> âš ï¸ åœ¨ `[18, 3, 1024, 1024]`ã€ æ—¶ï¼ŒPyTorch(CPU) å› ä¸ºå†…å­˜ä¸è¶³å¯¼è‡´åªèƒ½æ¨ç† 1 æ¬¡è€Œé 50 æ¬¡

<div align=center>
    <img src=./imgs_markdown/MobileNetV3-Small-Speed-Comparison-of-Different-Models-Full-Data.jpg
    width=100%>
</div>

<div align=center>
    <img src=./imgs_markdown/MobileNetV3-Small-Speed-Comparison-of-Different-Models-Single-Batch.jpg
    width=100%>
</div>

<div align=center>
    <img src=./imgs_markdown/MobileNetV3-Small-Speed-Comparison-of-Different-Models-Multi-Batch.jpg
    width=100%>
</div>

å…¶å®å¯ä»¥å‘ç°ï¼Œä¸ ResNet18 çš„ç»“è®ºæ˜¯ä¸€è‡´çš„ã€‚

## 7.3 ä¸ºä»€ä¹ˆ `python -m onnxsim` æ²¡æœ‰æ•ˆæœ

æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ï¼š

<font color='green'> <b> -------------- ResNet-18 -------------- </b></font>

```bash
python -m onnxsim ONNX/saves/resnet18-dynamic_dims.onnx ONNX/saves/resnet18-dynamic_dims-sim.onnx
```

```
Simplifying...
Finish! Here is the difference:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                   â”ƒ Original Model â”ƒ Simplified Model â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Add               â”‚ 8              â”‚ 8                â”‚
â”‚ Constant          â”‚ 42             â”‚ 42               â”‚
â”‚ Conv              â”‚ 20             â”‚ 20               â”‚
â”‚ Flatten           â”‚ 1              â”‚ 1                â”‚
â”‚ Gemm              â”‚ 1              â”‚ 1                â”‚
â”‚ GlobalAveragePool â”‚ 1              â”‚ 1                â”‚
â”‚ MaxPool           â”‚ 1              â”‚ 1                â”‚
â”‚ Relu              â”‚ 17             â”‚ 17               â”‚
â”‚ Model Size        â”‚ 44.6MiB        â”‚ 44.6MiB          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<font color='green'> <b> -------------- MobileNetV3-Small -------------- </b></font>

```bash
python -m onnxsim ONNX/saves/mobilenetv3small-dynamic_dims.onnx ONNX/saves/mobilenetv3small-dynamic_dims-sim.onnx
```

```
Simplifying...
Finish! Here is the difference:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                   â”ƒ Original Model â”ƒ Simplified Model â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Add               â”‚ 6              â”‚ 6                â”‚
â”‚ Constant          â”‚ 108            â”‚ 108              â”‚
â”‚ Conv              â”‚ 52             â”‚ 52               â”‚
â”‚ Flatten           â”‚ 1              â”‚ 1                â”‚
â”‚ Gemm              â”‚ 2              â”‚ 2                â”‚
â”‚ GlobalAveragePool â”‚ 10             â”‚ 10               â”‚
â”‚ HardSigmoid       â”‚ 9              â”‚ 9                â”‚
â”‚ HardSwish         â”‚ 19             â”‚ 19               â”‚
â”‚ Mul               â”‚ 9              â”‚ 9                â”‚
â”‚ Relu              â”‚ 14             â”‚ 14               â”‚
â”‚ Model Size        â”‚ 9.7MiB         â”‚ 9.7MiB           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

å¯ä»¥çœ‹åˆ°ï¼Œå…¶å®æ ¹æœ¬æ²¡æœ‰å˜åŒ–ï¼Œæ‰€ä»¥é€Ÿåº¦ä¹Ÿæ²¡æœ‰æå‡ã€‚

> âš ï¸ ONNX æ–‡ä»¶å˜å¤§äº†å¯èƒ½æ˜¯å› ä¸º `onnxsim` æ”¾äº†ä¸€äº›ä¸œè¥¿åœ¨æ¨¡å‹ä¸­ï¼Œä½†å¯¹æ¨¡å‹æ€§èƒ½æ²¡æœ‰å½±å“ã€‚

# 8. ONNX ä¸ PyTorch ç²¾åº¦å¯¹æ¯”

æˆ‘ä»¬ç°åœ¨æœ‰å¦‚ä¸‹çš„æ¨¡å‹ï¼š

- `weights.pth`: PyTorch æƒé‡
- `weights.onnx`: ONNX æƒé‡
- `weights-sim.onnx`: ONNX ç²¾ç®€åçš„æƒé‡

æ¨¡å‹çš„å…³ç³»å¦‚ä¸‹ï¼š

```mermaid
graph LR

style weights.pth fill:transparent,stroke:#FF0F50,stroke-width:2px;
style weights-sim.onnx fill:transparent,stroke:#4CAF50,stroke-width:2px;
style weights.onnx fill:transparent,stroke:#2196F3,stroke-width:2px;

weights.pth --> |torch.onnx.export| weights.onnx --> |python -m onnxsim| weights-sim.onnx
```

ç°åœ¨æˆ‘ä»¬æƒ³è¦ææ¸…æ¥šï¼Œè¿™æ ·è½¬æ¢åçš„æ¨¡å‹ç²¾åº¦æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ

```python
import os
import argparse
import numpy as np
import pandas as pd
from PIL import Image
import onnxruntime
import torch
import torch.nn.functional as F
from torchvision import transforms, models
from rich.progress import track


# ==================================== å‚æ•° ==================================== 
parser = argparse.ArgumentParser()
parser.add_argument('--image_folder_path', type=str, default='Datasets/Web/images', help='å›¾ç‰‡è·¯å¾„')
parser.add_argument('--input-shape', type=int, nargs=2, default=[256, 256])
parser.add_argument('--verbose', action='store_true', help='')
args = parser.parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°

onnx_weights = 'ONNX/saves/model-dynamic_dims.onnx'
onnx_weights_sim = 'ONNX/saves/model-dynamic_dims-sim.onnx'
# ==============================================================================

# å®šä¹‰æ¨¡å‹
onnx_model = onnxruntime.InferenceSession(path_or_bytes=onnx_weights)
onnx_model_sim = onnxruntime.InferenceSession(path_or_bytes=onnx_weights_sim)
pytorch_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).eval()  # âš ï¸ ä¸€å®šè¦ .eval
# pytorch_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

# å®šä¹‰é¢„å¤„ç†å‡½æ•°
img_transform = transforms.Compose([
    transforms.Resize(args.input_shape[-1]),
    transforms.CenterCrop(args.input_shape[-1]),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # imagenetä¸“ç”¨
        std=[0.229, 0.224, 0.225]),  # imagenetä¸“ç”¨
])

image_list = [os.path.join(args.image_folder_path, img) for img in os.listdir(args.image_folder_path) \
               if img.lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))]

for img_idx, image_path in track(enumerate(image_list), description='Precision Comparison'):
    # è¯»å–å›¾ç‰‡
    img = Image.open(fp=image_path)  # è¯»å–å›¾ç‰‡
    input_img = img_transform(img)
    input_img = input_img.unsqueeze(0)
    print(f"inputs.type: {type(input_img)}") if args.verbose else ...
    print(f"inputs.shape: {input_img.shape}") if args.verbose else ...

    model_ls = ['pt', 'onnx', 'onnx-sim']
    for model_name in model_ls:
        if model_name != 'pt':
            if not isinstance(input_img, np.ndarray):
                input_img = input_img.numpy()
            model_input = {'input': input_img, }
            model_result = onnx_model.run(output_names=['output'], input_feed=model_input)[0]
        else:
            model_result = pytorch_model(input_img)
        
        if not isinstance(model_result, torch.Tensor):
            model_result = torch.from_numpy(model_result)
        
        model_result_softmax = F.softmax(input=model_result, dim=1)  # [1, 1000]

        # å–æ¦‚ç‡æœ€å¤§çš„å‰ n ä¸ªç»“æœ
        n = 3
        top_n = torch.topk(input=model_result_softmax, k=n, dim=1)

        probs = top_n.values.detach().numpy()[0]  # torch.Size([18, 3])
        indices = top_n.indices.detach().numpy()[0]  # torch.Size([18, 3])
        print(f"probs: {probs}") if args.verbose else ...
        print(f"indices: {indices}") if args.verbose else ...

        df = pd.read_csv('Datasets/imagenet_classes_indices.csv')

        idx2labels = {}
        for _, row in df.iterrows():
            idx2labels[row['ID']] = row['Chinese']  # ä¸­æ–‡æ ‡ç­¾

        print(f"============================== æ¨ç†ç»“æœ-{model_name} ==============================")  if args.verbose else ...
        
        _results = []
        for i, (prob, idx) in enumerate(zip(probs, indices)):
            class_name = idx2labels[idx]
            text = f"No.{i}: {class_name:<30} --> {prob:>.5f}"  if args.verbose else ...
            _results.append(prob)
            print(text)
        print(f"=====================================================================")  if args.verbose else ...

        with open("ONNX/saves/Precision-comparison.txt", 'a') as f:
            if model_name == 'pt':
                f.write(f"|[{img_idx+1}] {os.path.basename(image_path)}"
                        f"|{_results[0]:>.5f}</br>{_results[1]:>.5f}</br>{_results[2]:>.5f}")
            elif model_name == 'onnx':
                f.write(f"|{_results[0]:>.5f}</br>{_results[1]:>.5f}</br>{_results[2]:>.5f}")
            else:
                f.write(f"|{_results[0]:>.5f}</br>{_results[1]:>.5f}</br>{_results[2]:>.5f}|\n")
```

**å®éªŒç»“æœ**ï¼š

<div align=center>

|å›¾ç‰‡åç§°|PyTorch|ONNX|ONNX-sim|
|:-|:-:|:-:|:-:|
|[1] book.jpg|0.73973</br>0.05049</br>0.02358|0.73973</br>0.05049</br>0.02358|0.73973</br>0.05049</br>0.02358|
|[2] butterfly.jpg|0.89704</br>0.04772</br>0.01542|0.89704</br>0.04772</br>0.01542|0.89704</br>0.04772</br>0.01542|
|[3] camera.jpg|0.27658</br>0.17709</br>0.10925|0.27658</br>0.17709</br>0.10925|0.27658</br>0.17709</br>0.10925|
|[4] cat.jpg|0.27773</br>0.18393</br>0.17254|0.27773</br>0.18393</br>0.17254|0.27773</br>0.18393</br>0.17254|
|[5] dog.jpg|0.51787</br>0.25384</br>0.05929|0.51787</br>0.25384</br>0.05929|0.51787</br>0.25384</br>0.05929|
|[6] dogs_orange.jpg|0.35289</br>0.30114</br>0.07791|0.35289</br>0.30114</br>0.07791|0.35289</br>0.30114</br>0.07791|
|[7] female.jpg|0.15600</br>0.08031</br>0.04808|0.15600</br>0.08031</br>0.04808|0.15600</br>0.08031</br>0.04808|
|[8] free-images.jpg|0.45595</br>0.17626</br>0.08414|0.45595</br>0.17626</br>0.08414|0.45595</br>0.17626</br>0.08414|
|[9] gull.jpg|0.64711</br>0.23324</br>0.04430|0.64711</br>0.23324</br>0.04430|0.64711</br>0.23324</br>0.04430|
|[10] laptop-phone.jpg|0.49379</br>0.35405</br>0.06063|0.49379</br>0.35405</br>0.06063|0.49379</br>0.35405</br>0.06063|
|[11] monitor.jpg|0.51678</br>0.44193</br>0.02232|0.51678</br>0.44193</br>0.02232|0.51678</br>0.44193</br>0.02232|
|[12] motorcycle.jpg|0.31712</br>0.22435</br>0.15631|0.31712</br>0.22435</br>0.15631|0.31712</br>0.22435</br>0.15631|
|[13] mouse.jpg|0.99473</br>0.00074</br>0.00052|0.99473</br>0.00074</br>0.00052|0.99473</br>0.00074</br>0.00052|
|[14] panda.jpg|0.94559</br>0.03199</br>0.00561|0.94559</br>0.03199</br>0.00561|0.94559</br>0.03199</br>0.00561|
|[15] share_flower_fullsize.jpg|0.78806</br>0.05691</br>0.02483|0.78806</br>0.05691</br>0.02483|0.78806</br>0.05691</br>0.02483|
|[16] tiger.jpeg|0.61749</br>0.38001</br>0.00052|0.61749</br>0.38001</br>0.00052|0.61749</br>0.38001</br>0.00052|

</div>

å¯ä»¥çœ‹åˆ°ï¼Œè½¬æ¢å‰åæ¨¡å‹å¹¶æ²¡æœ‰ç²¾åº¦çš„ä¸¢å¤±ã€‚

# 9. ã€”æ‹“å±•çŸ¥è¯†ã€•ä¸ºä»€ä¹ˆ `.pt` æ¨¡å‹åœ¨æ¨ç†æ—¶ä¸€å®šè¦ `.eval()`ï¼Ÿ

åœ¨PyTorchä¸­ï¼Œ`.eval()` æ˜¯ä¸€ä¸ªç”¨äºå°†æ¨¡å‹åˆ‡æ¢åˆ°è¯„ä¼°ï¼ˆinferenceï¼‰æ¨¡å¼çš„æ–¹æ³•ã€‚åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹çš„è¡Œä¸ºä¼šæœ‰æ‰€å˜åŒ–ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼š**Dropout** å’Œ **Batch Normalization**ã€‚

1. **Dropoutï¼š**
   - åœ¨è®­ç»ƒé˜¶æ®µï¼Œä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€šå¸¸ä¼šä½¿ç”¨ dropout ç­–ç•¥ï¼Œå³åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œä»¥ä¸€å®šçš„æ¦‚ç‡éšæœºä¸¢å¼ƒæŸäº›ç¥ç»å…ƒçš„è¾“å‡ºã€‚
   - åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å¸Œæœ›è·å¾—æ¨¡å‹çš„ç¡®å®šæ€§è¾“å‡ºï¼Œè€Œä¸æ˜¯åœ¨æ¯æ¬¡æ¨ç†æ—¶éƒ½ä¸¢å¼ƒä¸åŒçš„ç¥ç»å…ƒã€‚å› æ­¤ï¼Œåœ¨æ¨ç†æ—¶åº”è¯¥å…³é—­ dropoutã€‚é€šè¿‡è°ƒç”¨ `.eval()`ï¼ŒPyTorch ä¼šå°†æ‰€æœ‰ dropout å±‚è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå³ä¸è¿›è¡Œéšæœºä¸¢å¼ƒã€‚

2. **Batch Normalizationï¼š**
   - Batch Normalizationï¼ˆæ‰¹æ ‡å‡†åŒ–ï¼‰åœ¨è®­ç»ƒæ—¶é€šè¿‡å¯¹æ¯ä¸ª mini-batch è¿›è¡Œæ ‡å‡†åŒ–æ¥åŠ é€Ÿè®­ç»ƒï¼Œä½†åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¸æ˜¯åŸºäº mini-batch è¿›è¡Œé¢„æµ‹ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œæ ‡å‡†åŒ–ã€‚
   - åœ¨ `.eval()` æ¨¡å¼ä¸‹ï¼ŒBatch Normalization ä¼šä½¿ç”¨è®­ç»ƒæ—¶è®¡ç®—çš„ç§»åŠ¨å¹³å‡å’Œæ–¹å·®ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å½“å‰ mini-batch çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

å› æ­¤ï¼Œä¸ºäº†ç¡®ä¿åœ¨æ¨ç†æ—¶å¾—åˆ°ä¸€è‡´å’Œå¯é çš„ç»“æœï¼Œéœ€è¦åœ¨æ¨ç†ä¹‹å‰è°ƒç”¨ `.eval()` æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼ï¼Œå…³é—­äº† dropoutï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„ Batch Normalization ç»Ÿè®¡ä¿¡æ¯ã€‚

---

ä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äºä¸€å¼ çŒ«å’ªå›¾ç‰‡è€Œè¨€ï¼Œå¦‚æœæˆ‘ä»¬çš„ `.pt` æ¨¡å‹æ²¡æœ‰å¼€å¯ `.eval()` å°±è¿›è¡Œæ¨ç†ï¼Œé‚£ä¹ˆå¾—åˆ°çš„ç»“æœå¦‚ä¸‹ï¼š

```
============================== æ¨ç†ç»“æœ-pt ==========================
No.0: æ¡¶                                --> 0.00780
No.1: æ‰‹å‹çš®ç¢—æ³µ                        --> 0.00680
No.2: é’©çˆª                              --> 0.00601
====================================================================
probs: [0.27773306 0.18392678 0.17254312]
indices: [281 285 287]
============================== æ¨ç†ç»“æœ-onnx ========================
No.0: è™æ–‘çŒ«                            --> 0.27773
No.1: åŸƒåŠçŒ«                            --> 0.18393
No.2: çŒçŒ,å±±çŒ«                         --> 0.17254
====================================================================
probs: [0.27773306 0.18392678 0.17254312]
indices: [281 285 287]
============================== æ¨ç†ç»“æœ-onnx-sim ====================
No.0: è™æ–‘çŒ«                            --> 0.27773
No.1: åŸƒåŠçŒ«                            --> 0.18393
No.2: çŒçŒ,å±±çŒ«                         --> 0.17254
====================================================================
```

å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äº ONNX æ¨¡å‹è€Œè¨€ï¼Œæ¨ç†ç›¸å¯¹æ¥è¯´æ˜¯æ¯”è¾ƒæ­£ç¡®çš„ã€‚ä½†å¯¹äº PyTorch æ¨¡å‹ï¼Œæ¨ç†ä¸çŒ«æ— å…³äº†ï¼Œæ‰€ä»¥ âš ï¸ åœ¨æ¨ç†æ—¶å¼€å¯ `.eval()` æ˜¯éå¸¸é‡è¦çš„ï¼

# å‚è€ƒ

1. [å›¾åƒåˆ†ç±»æ¨¡å‹éƒ¨ç½²-Pytorchè½¬ONNX](https://www.bilibili.com/video/BV1cM4y187Xc)
2. [Pytorchå›¾åƒåˆ†ç±»æ¨¡å‹éƒ¨ç½²-ONNX Runtimeæœ¬åœ°ç»ˆç«¯æ¨ç†](https://www.bilibili.com/video/BV1AM4y187yR)