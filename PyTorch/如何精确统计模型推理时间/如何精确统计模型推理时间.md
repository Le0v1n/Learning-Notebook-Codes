> æœ¬æ–‡ä¸­çš„çŸ¥è¯†æ¥æºä¸ºBç«™upä¸»[éœ¹é›³å§å•¦Wz](https://space.bilibili.com/18161609)çš„è§†é¢‘[å¦‚ä½•ç²¾ç¡®ç»Ÿè®¡æ¨¡å‹æ¨ç†æ—¶é—´](https://www.bilibili.com/video/BV1km4SeuEim)ã€‚

# 1. å¼•è¨€

ä¸‹é¢è¿™æ®µä»£ç è®¡ç®—æ¨¡å‹æ¨ç†ç”¨æ—¶æ˜¯å¦æ­£ç¡®ï¼Ÿ

```python
import time
import torchvision
import torch


if __name__ == "__main__":
    # Create a model
    mobilenetv3 = torchvision.models.mobilenet_v3_large(weights=None, num_classes=1000).eval().cuda()

    # Create a dummy input
    dummpy_input = torch.ones(size=[4, 3, 224, 224]).cuda()

    begin = time.perf_counter()

    result = mobilenetv3(dummpy_input)

    end = time.perf_counter()

    # Statistics
    print(f"The inference time: {end - begin:.4f}s")
```

```
The inference time: 0.3876s
```

åœ¨åˆ†ææ¨¡å‹æ€§èƒ½æ—¶éœ€è¦ç²¾ç¡®åœ°ç»Ÿè®¡å‡ºæ¨¡å‹çš„æ¨ç†æ—¶é—´ï¼Œä½†ä»…ä»…é€šè¿‡åœ¨æ¨¡å‹æ¨ç†å‰åæ‰“æ—¶é—´æˆ³ç„¶åç›¸å‡å¾—åˆ°çš„æ—¶é—´å…¶å®æ˜¯Hostä¾§å‘Deviceä¾§ä¸‹å‘æŒ‡ä»¤çš„æ—¶é—´ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-15-45-44.png
    width=70%></br><center></center>
</div>

å…¶ä¸­ï¼š

- Hostä¾§æ˜¯CPU
- è€ŒDeviceä¾§æ˜¯GPUã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼šHostä¸Deviceå…¶å®å¼‚æ­¥çš„ï¼Œæ„æ€æ˜¯CPUä¸‹å‘å®ŒæŒ‡ä»¤ä¹‹åå°±ç»“æŸäº†ï¼Œä¸ä¼šç­‰å¾…GPUæ˜¯å¦å®Œæˆï¼›GPUä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œå¹¶ä¸ä¼šç­‰CPUã€‚å› æ­¤æ¨¡å‹ç†è®ºä¸Šæ¨¡å‹çš„æ¨ç†æ—¶é—´åº”è¯¥ä¸ºï¼š

$$
æ¨¡å‹å®é™…æ¨ç†æ—¶é—´ = \mathrm{Hostä¸‹å‘æŒ‡ä»¤æ€»æ—¶é—´} \cup \mathrm{Deviceæ€»æ—¶é—´}
$$

æ‰€ä»¥å¦‚ä¸Šå›¾çš„å·¦ä¾§ï¼Œå› ä¸ºHostä¾§çš„æ—¶é—´æ¯”Deviceä¾§çš„æ—¶é—´çŸ­ï¼Œæ‰€ä»¥ä¸Šé¢çš„ä»£ç æ˜¯ä¸è¡Œçš„ï¼›å¯¹äºä¸Šå›¾çš„å³ä¾§ï¼Œå› ä¸ºHostä¾§çš„æ—¶é—´æ¯”Deviceä¾§çš„æ—¶é—´é•¿ï¼Œæ‰€ä»¥ä¸Šé¢çš„ä»£ç æ˜¯å¯ä»¥çš„ã€‚

# 2. ä¸¤ç§å¸¸è§çš„ç»Ÿè®¡æ–¹å¼

ğŸ¤” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ï¼šæˆ‘æµ‹ï¼Œè¿™æ ·ä¹Ÿå¤ªéº»çƒ¦äº†ï¼Œéš¾é“æˆ‘è¿˜éœ€è¦æŠŠæ¡Hostä¾§å’ŒDeviceä¾§çš„æ—¶é—´å—ï¼Ÿ
ğŸ¥³ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“ï¼šPyTorchå®˜æ–¹ä¹Ÿè€ƒè™‘åˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œå› æ­¤ç»™å‡ºäº†ä¸¤ä¸ªå¸¸è§çš„ç»Ÿè®¡æ–¹æ³•ï¼š

1. é€šè¿‡æ‰‹åŠ¨è°ƒç”¨åŒæ­¥å‡½æ•°ç¡®ä¿Deviceè®¡ç®—å®Œæˆï¼ˆHostä¾§éœ€è¦ç­‰å¾…Deviceä¾§ï¼‰
2. é€šè¿‡Eventæ–¹æ³•ç»Ÿè®¡æ—¶é—´

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-15-57-29.png
    width=70%></br><center></center>
</div>

## 2.1 æ–¹æ³•1ï¼šé€šè¿‡æ‰‹åŠ¨è°ƒç”¨åŒæ­¥å‡½æ•°ç¡®ä¿Deviceè®¡ç®—å®Œæˆ

ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
torch.cuda.synchronize(device)  # Hostä¾§ç­‰å¾…Deviceä¾§è®¡ç®—å®Œæˆ
begin = time.perf_counter()  # Hostä¾§
results = model(x)  # Hostä¾§+Deviceä¾§
torch.cuda.synchronize(device)  # Hostä¾§ç­‰å¾…Deviceä¾§è®¡ç®—å®Œæˆ
end = time.perf_counter()  # Hostä¾§
```

è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-16-03-37.png
    width=60%></br><center></center>
</div>

## 2.2 æ–¹æ³•2ï¼šé€šè¿‡Eventæ–¹æ³•ç»Ÿè®¡æ—¶é—´

ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
# åˆ›å»ºä¸¤ä¸ªEventå¯¹è±¡ï¼Œç”¨äºè®°å½•å’Œæµ‹é‡CUDAæ“ä½œçš„æ—¶é—´
start_event = torch.cuda.Event(enable_timing=True)  # å¼€å¯è®¡æ—¶åŠŸèƒ½
end_event = torch.cuda.Event(enable_timing=True)    # å¼€å¯è®¡æ—¶åŠŸèƒ½

# è®°å½•å½“å‰æ—¶é—´ç‚¹ï¼Œä½œä¸ºåç»­è®¡ç®—æ—¶é—´å·®çš„èµ·å§‹ç‚¹
start_event.record()

results = model(x)

# è®°å½•å½“å‰æ—¶é—´ç‚¹ï¼Œä½œä¸ºåç»­è®¡ç®—æ—¶é—´å·®çš„ç»“æŸç‚¹
end_event.record()

# ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œå®Œæˆï¼Œè¿™æ ·æµ‹é‡çš„æ—¶é—´æ‰å‡†ç¡®
end_event.synchronize()

# è®¡ç®—ä»start_eventåˆ°end_eventçš„æ—¶é—´å·®ï¼Œå•ä½æ˜¯æ¯«ç§’
elapsed_time = start_event.elapsed_time(end_event)
```

è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-16-03-47.png
    width=60%></br><center></center>
</div>

# 3. ä»£ç å®æµ‹

ä¸‹é¢ç¤ºä¾‹ä»£ç ä¸­åˆ†åˆ«ç»™å‡ºäº†ä¸‰ç§æ–¹å¼çš„ç»“æœï¼Œæ¯ç§æ–¹æ³•éƒ½é‡å¤50æ¬¡ï¼Œå¿½ç•¥å‰5æ¬¡æ¨ç†ï¼Œå–å45æ¬¡çš„å¹³å‡å€¼ã€‚

1. ç›´æ¥æ‰“æ—¶é—´æˆ³è®¡ç®—
2. æ‰‹åŠ¨è°ƒç”¨åŒæ­¥å‡½æ•°
3. ä½¿ç”¨Eventæ–¹æ³•

```python
import time
import torchvision
import torch
from tqdm import tqdm as TQDM
import numpy as np


def calc_inference_time_method_1(model: torch.nn.Module, input: torch.Tensor, times: int=50) -> float:
    with torch.inference_mode():
        time_list: list = []

        for _ in TQDM(range(times), desc='Method 1'):
            begin = time.perf_counter()
            results = model(input)
            end = time.perf_counter()
            time_list.append(end - begin)
            
    return np.average(time_list[5: ])


def calc_inference_time_method_2(model: torch.nn.Module, input: torch.Tensor, times: int=50) -> float:
    device = input.device
    with torch.inference_mode():
        time_list: list = []

        for _ in TQDM(range(times), desc='Method 2'):
            torch.cuda.synchronize(device=device)
            begin = time.perf_counter()
            results = model(input)
            torch.cuda.synchronize(device=device)
            end = time.perf_counter()
            time_list.append(end - begin)

    return np.average(time_list[5: ])


def calc_inference_time_method_3(model: torch.nn.Module, input: torch.Tensor, times: int=50) -> float:
    with torch.inference_mode():
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        time_list: list = []

        for _ in TQDM(range(times), desc='Method 3'):
            start_event.record()
            results = model(input)
            end_event.record()
            end_event.synchronize()
            time_list.append(start_event.elapsed_time(end_event) / 1000)

    return np.average(time_list[5: ])


if __name__ == "__main__":
    # Create a model
    mobilenetv3 = torchvision.models.mobilenet_v3_large(weights=None, num_classes=10, width_mult=2.0).cuda()

    # Create a dummy input
    dummpy_input = torch.randn(size=[32, 3, 224, 224]).cuda()

    time_1 = calc_inference_time_method_1(model=mobilenetv3, input=dummpy_input, times=50)
    print(f"The inference time of method 1: {time_1:.4f}s")

    time_2 = calc_inference_time_method_2(model=mobilenetv3, input=dummpy_input, times=50)
    print(f"The inference time of method 2: {time_2:.4f}s")

    time_3 = calc_inference_time_method_3(model=mobilenetv3, input=dummpy_input, times=50)
    print(f"The inference time of method 3: {time_3:.4f}s")
```

```
The inference time of method 1: 0.0269s
The inference time of method 2: 0.0331s
The inference time of method 3: 0.0324s
```

é€šè¿‡ç»ˆç«¯è¾“å‡ºå¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœç›´æ¥åœ¨æ¨¡å‹æ¨ç†å‰åæ‰“æ—¶é—´æˆ³ç›¸å‡å¾—åˆ°çš„æ—¶é—´ä¼šçŸ­ä¸€äº›ï¼ˆå› ä¸ºå¹¶æ²¡æœ‰ç­‰å¾…Deviceä¾§è®¡ç®—å®Œæˆï¼‰ã€‚è€Œä½¿ç”¨åŒæ­¥å‡½æ•°ï¼ˆ`torch.cuda.synchronize()`ï¼‰æˆ–è€…`Event`æ–¹æ³•ç»Ÿè®¡çš„æ—¶é—´æ˜æ˜¾è¦é•¿å¾ˆå¤šã€‚

# 4. ä¸‰ç§æ–¹æ³•å¯¹æ¯”

é€šè¿‡ä¸Šé¢çš„ä»£ç ç¤ºä¾‹å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡åŒæ­¥å‡½æ•°ï¼ˆ`torch.cuda.synchronize()`ï¼‰ç»Ÿè®¡çš„æ—¶é—´å’Œ`Event`æ–¹æ³•ç»Ÿè®¡çš„æ—¶é—´åŸºæœ¬ä¸€è‡´ã€‚é‚£ä¸¤è€…æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿå¦‚æœåªæ˜¯ç®€å•ç»Ÿè®¡ä¸€ä¸ªæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¡®å®çœ‹ä¸å‡ºä»€ä¹ˆå·®å¼‚ã€‚ä½†å¦‚æœè¦ç»Ÿè®¡ä¸€ä¸ªå®Œæ•´AIåº”ç”¨piplineï¼ˆå…¶ä¸­å¯èƒ½åŒ…å«å¤šä¸ªæ¨¡å‹ä»¥åŠå„ç§CPUè®¡ç®—ï¼‰ä¸­ä¸åŒæ¨¡å‹çš„è€—æ—¶ï¼Œè€Œåˆä¸æƒ³å½±å“åˆ°æ•´ä¸ªpiplineçš„æ€§èƒ½ï¼Œé‚£ä¹ˆå»ºè®®ä½¿ç”¨`Event`æ–¹æ³•ã€‚å› ä¸ºä½¿ç”¨åŒæ­¥å‡½æ•°å¯èƒ½ä¼šè®©Hosté•¿æœŸå¤„äºç­‰å¾…çŠ¶æ€ï¼Œç­‰å¾…è¿‡ç¨‹ä¸­ä¹Ÿæ— æ³•å¹²å…¶ä»–çš„äº‹æƒ…ï¼Œä»è€Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹ã€‚å¯ä»¥çœ‹çœ‹ä¸‹é¢è¿™ä¸ªç¤ºä¾‹ï¼Œæ•´ä¸ªpiplineç”±MobileNetV3-Largeæ¨ç†+ä¸€æ®µçº¯CPUè®¡ç®—+MobileNetV3-Smallæ¨ç†ä¸²è¡Œæ„æˆï¼Œå‡è®¾æƒ³ç»Ÿè®¡ä¸€ä¸‹MobileNetV3-Largeã€MobileNetV3-Smallæ¨ç†åˆ†åˆ«ç”¨äº†å¤šé•¿æ—¶é—´ï¼š

```python
import time
import torchvision
import torch
from tqdm import tqdm as TQDM
import numpy as np


def cpu_task() -> None:
    x = np.random.randn(1, 3, 512, 512)
    x = x.astype(np.float32)
    x = x * 1024 ** 0.5


def calc_inference_time_method_1(model_1: torch.nn.Module, model_2: torch.nn.Module, input: torch.Tensor, times: int=50) -> tuple:
    with torch.inference_mode():
        time_list_1: list = []
        time_list_2: list = []

        for _ in TQDM(range(times), desc='Method 1'):
            # step.1: model 1
            begin_1 = time.perf_counter()
            results_1 = model_1(input)
            end_1 = time.perf_counter()

            # step.2: CPU
            cpu_task()

            # step.3: model 2
            begin_2 = time.perf_counter()
            results_2 = model_2(input)
            end_2 = time.perf_counter()

            time_list_1.append(end_1 - begin_1)
            time_list_2.append(end_2 - begin_2)
            
    return np.average(time_list_1[5: ]), np.average(time_list_2[5: ])


def calc_inference_time_method_2(model_1: torch.nn.Module, model_2: torch.nn.Module, input: torch.Tensor, times: int=50) -> tuple:
    device = input.device
    with torch.inference_mode():
        time_list_1: list = []
        time_list_2: list = []

        for _ in TQDM(range(times), desc='Method 2'):
            # step.1: model 1
            torch.cuda.synchronize(device=device)
            begin_1 = time.perf_counter()
            results_1 = model_1(input)
            torch.cuda.synchronize(device=device)
            end_1 = time.perf_counter()

            # step.2: CPU
            cpu_task()

            # step.3: model 2
            torch.cuda.synchronize(device=device)
            begin_2 = time.perf_counter()
            results_2 = model_2(input)
            torch.cuda.synchronize(device=device)
            end_2 = time.perf_counter()

            time_list_1.append(end_1 - begin_1)
            time_list_2.append(end_2 - begin_2)

    return np.average(time_list_1[5: ]), np.average(time_list_2[5: ])


def calc_inference_time_method_3(model_1: torch.nn.Module, model_2: torch.nn.Module, input: torch.Tensor, times: int=50) -> tuple:
    with torch.inference_mode():
        start_event_1 = torch.cuda.Event(enable_timing=True)
        end_event_1 = torch.cuda.Event(enable_timing=True)
        time_list_1: list = []

        start_event_2 = torch.cuda.Event(enable_timing=True)
        end_event_2 = torch.cuda.Event(enable_timing=True)
        time_list_2: list = []

        for _ in TQDM(range(times), desc='Method 3'):
            # Step.1 model 1
            start_event_1.record()
            results_1 = model_1(input)
            end_event_1.record()
            end_event_1.synchronize()

            # Step.2 CPU
            cpu_task()

            # Step.3 model 2
            start_event_2.record()
            results_2 = model_1(input)
            end_event_2.record()
            end_event_2.synchronize()

            time_list_1.append(start_event_1.elapsed_time(end_event_1) / 1000)
            time_list_2.append(start_event_2.elapsed_time(end_event_2) / 1000)

    return np.average(time_list_1[5: ]), np.average(time_list_2[5: ])


if __name__ == "__main__":
    # Create a model
    mobilenetv3_large = torchvision.models.mobilenet_v3_large(weights=None, num_classes=1000, width_mult=1.0).cuda()
    mobilenetv3_small = torchvision.models.mobilenet_v3_small(weights=None, num_classes=1000, width_mult=1.0).cuda()

    # Create a dummy input
    dummpy_input = torch.randn(size=[32, 3, 224, 224]).cuda()

    method_1_time_1, method_1_time_2 = calc_inference_time_method_1(model_1=mobilenetv3_large, model_2=mobilenetv3_small, input=dummpy_input, times=50)
    print(
        f"[Method 1]\n"
        f"\tThe inference time of model 1: {method_1_time_1:.4f}s\n"
        f"\tThe inference time of model 2: {method_1_time_2:.4f}s\n"
        f"\tThe average time: {np.average([method_1_time_1, method_1_time_2]):.4f}s\n"
    )

    method_2_time_1, method_2_time_2 = calc_inference_time_method_2(model_1=mobilenetv3_large, model_2=mobilenetv3_small, input=dummpy_input, times=50)
    print(
        f"[Method 2]\n"
        f"\tThe inference time of model 1: {method_2_time_1:.4f}s\n"
        f"\tThe inference time of model 2: {method_2_time_2:.4f}s\n"
        f"\tThe average time: {np.average([method_2_time_1, method_1_time_2]):.4f}s\n"
    )

    method_3_time_1, method_3_time_2 = calc_inference_time_method_3(model_1=mobilenetv3_large, model_2=mobilenetv3_small, input=dummpy_input, times=50)
    print(
        f"[Method 3]\n"
        f"\tThe inference time of model 1: {method_3_time_1:.4f}s\n"
        f"\tThe inference time of model 2: {method_3_time_2:.4f}s\n"
        f"\tThe average time: {np.average([method_3_time_1, method_1_time_2]):.4f}s\n"
    )
```

```
[Method 1]
        The inference time of model 1: 0.0037s
        The inference time of model 2: 0.0031s
        The average time: 0.0034s

[Method 2]
        The inference time of model 1: 0.0134s
        The inference time of model 2: 0.0044s
        The average time: 0.0082s

[Method 3]
        The inference time of model 1: 0.0129s
        The inference time of model 2: 0.0128s
        The average time: 0.0080s
```

é€šè¿‡ç»ˆç«¯æ‰“å°çš„ç»“æœå¯ä»¥çœ‹åˆ°æ— è®ºæ˜¯ä½¿ç”¨åŒæ­¥å‡½æ•°è¿˜æ˜¯Eventæ–¹æ³•ç»Ÿè®¡çš„MobileNetV3-Largeã€MobileNetV3-Smallçš„æ¨ç†æ—¶é—´åŸºæœ¬æ˜¯ä¸€è‡´çš„ã€‚ä½†å¯¹äºæ•´ä¸ªpiplineè€Œè¨€ä½¿ç”¨åŒæ­¥å‡½æ•°æ—¶æ€»æ—¶é—´æ˜æ˜¾å˜é•¿äº†ã€‚ä¸‹å›¾å¤§è‡´è§£é‡Šäº†ä¸ºä»€ä¹ˆä½¿ç”¨åŒæ­¥å‡½æ•°æ—¶å¯¼è‡´æ•´ä¸ªpiplineå˜é•¿çš„åŸå› ï¼Œä¸»è¦æ˜¯åœ¨MobileNetV3-Largeå‘é€å®ŒæŒ‡ä»¤åä½¿ç”¨åŒæ­¥å‡½æ•°æ—¶ä¼šä¸€ç›´ç­‰å¾…Deviceä¾§è®¡ç®—ç»“æŸï¼ŒæœŸé—´å•¥ä¹Ÿä¸èƒ½å¹²ã€‚è€Œä½¿ç”¨Eventæ–¹æ³•æ—¶åœ¨MobileNetV3-Largeå‘é€å®ŒæŒ‡ä»¤åä¸ä¼šé˜»å¡Hostï¼Œå¯ä»¥ç«‹é©¬å»è¿›è¡Œåé¢çš„CPUè®¡ç®—ä»»åŠ¡ã€‚å…·ä½“çš„æ—¶åºå›¾å¦‚ä¸‹æ‰€ç¤ºï¼š

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-17-28-42.png
    width=100%></br><center></center>
</div>
